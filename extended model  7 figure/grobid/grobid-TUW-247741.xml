<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Users\Angela\git\grobid\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.4.5-dummy" ident="GROBID" when="2017-12-29T00:16+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A FORMAL METHOD FOR SELECTING EVALUATION METRICS FOR IMAGE SEGMENTATION</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdel</forename><forename type="middle">Aziz</forename><surname>Taha</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Vienna University of Technology Univ. of Applied Sciences Western Swizerland</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Allan</forename><surname>Hanbury</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Vienna University of Technology Univ. of Applied Sciences Western Swizerland</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><forename type="middle">A</forename><surname>Jimenez Del Toro</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Vienna University of Technology Univ. of Applied Sciences Western Swizerland</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">A FORMAL METHOD FOR SELECTING EVALUATION METRICS FOR IMAGE SEGMENTATION</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-image segmentation</term>
					<term>evaluation met-rics</term>
					<term>selection</term>
				</keywords>
			</textClass>
			<abstract>
				<p>Evaluating the quality of segmentations is an important process in image processing, especially in the medical domain. Many evaluation metrics have been used in evaluating segmentation. There exists no formal way to choose the most suitable metric(s) for a particular segmentation task and/or particular data. In this paper we propose a formal method for choosing the most suitable metrics for evaluating the quality of segmenta-tions with respect to ground truth segmentations. The proposed method depends on measuring the bias of metrics towards/against the properties of the the seg-mentations being evaluated. We firstly demonstrate how metrics can have bias towards/against particular properties and then we propose a general method for ranking metrics according to their overall bias. We finally demonstrate for 3D medical image segmentations that ranking produced using metrics with low overall bias strongly correlate with manual rankings done by an expert.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>1.1 The need to understand metrics: Many evaluation metrics for image segmentation have been introduced; most researchers choose the evaluation metrics arbitrar- ily or according to their popularity. Investigating met- rics would help researchers to better understand them and help companies and stakeholders to save effort and time reaching optimal systems <ref type="bibr" target="#b0">[1]</ref>. A poorly defined metric may lead to inaccurate conclusions like selecting suboptimal models when comparing the performance of classifiers <ref type="bibr" target="#b3">[2]</ref>.</p><p>Many researchers have investigated the drawbacks of particular metrics given particular properties of the data being classified. As a special case of classification, image segmentation is also affected by these draw- backs. The following are some examples: Hausdorff distance is very sensitive to noise and least squares based evaluation methods are very sensitive to out- liers <ref type="bibr" target="#b5">[3]</ref>. Mutual information doesn't utilize spatial information inherited in images because only voxel relationships are considered but not the neighbor- hoods <ref type="bibr" target="#b7">[4]</ref>. Information theoretical measures have a non-convergent baseline which depends on the ratio between the number of data points and the number of classes. Therefore this class of measure needs chance correction <ref type="bibr" target="#b9">[5]</ref>. Commonly used measures (precision, recall and F-measures) are biased and don't consider the level of chance <ref type="bibr" target="#b12">[6]</ref>. Choosing evaluation metrics is very important and application-dependent; when evaluating imbalanced datasets, the metric choice is not obvious <ref type="bibr" target="#b3">[2]</ref>. Metrics have different properties with respect to their correlation with user satisfaction crite- ria and their ease of interpretation <ref type="bibr" target="#b13">[7]</ref>. Benhabiles et al. <ref type="bibr" target="#b16">[8]</ref> validated 250 automatic segmentations against their corresponding ground truth segmentations using four different evaluation metrics. The results were then compared with manual ratings from 40 human ob- servers. They found that the correlations between the ranking based on the manual ratings and the rankings based on the evaluation metrics vary between 30% and 80% depending on the used metric.</p><p>Research in the last decades generally results in the relative system improvement achieved becoming smaller and smaller. As a result, sensitivity and fi- delity of evaluation metrics become increasingly criti- cal. When improvements are small, metrics with high sensitivity are needed to measure small but real im- provements and also with high fidelity to distinguish between improvements based on user preferences and improvements resulting from biased relevance judg- ments <ref type="bibr" target="#b18">[9]</ref> [10].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Problem definition and notations:</head><p>In this paper, we propose a formal method for selecting the most suit- able metrics to evaluate image segmentation depend- ing on the data being segmented and the goal of the segmentation task. The method is primarily based on two facts: the first is that effectiveness metrics can be biased towards or against properties of the images be- ing segmented, meaning that particular metrics over- 978-1-4799-5751-4/14/$31.00 Â©2014 IEEE penalize or over-reward segmentations given particu- lar properties <ref type="bibr" target="#b7">[4]</ref> [6] <ref type="bibr" target="#b2">[11]</ref> [2] <ref type="bibr" target="#b5">[3]</ref>. The second fact is that selecting the best evaluation metrics can be sub- ject to the segmentation goal which means that the bias towards/against a particular property of the data can be differently important depending on the segmenta- tion goal <ref type="bibr" target="#b16">[8]</ref>  <ref type="bibr" target="#b13">[7]</ref>. To meet the context dependency, the proposed method allows individual weighting of the influence of each property according to its importance in case this is known, which increases the effectiveness of the method.</p><p>The problem to be solved in this paper can be formulated as follows: given a set of metrics M = {M 1 , M2, ..., M r }, a set of image segmentations C = {C 1 , C 2 , ...., C k }, then the task is to rank the metrics in M according to their suitability for evaluating the qual- ity of the segmentations in C provided that for each segmentation there exists a ground truth segmentation.</p><p>The proposed method is general and can be applied to select evaluation metrics for all types of segmenta- tions. However, for simplicity, we will consider only the crisp segmentation task in this paper to present and for- mulate the method. In particular, we will be analyzing and testing the method using a special type of segmen- tation, namely medical volume segmentation e.g. mag- netic resonance images (MRI) where voxels (3D pixels) are either assigned or not to a given class (segment) e.g. an organ or a tumor. paring seven evaluation metrics. They negate the belief that commonly used evaluation measures are equally reliable. Fatourechi et al. <ref type="bibr" target="#b3">[2]</ref> proposed a framework based on Desired Region of Operation (DROP) for se- lecting the best evaluation metric for evaluating imbal- anced classifications. Sakai <ref type="bibr" target="#b11">[16]</ref> provided comparisons between metrics depending on the sensitivity and sta- bility using the Voorhees/Buckley swap method <ref type="bibr" target="#b14">[17]</ref>. All these papers lack generality because they are meth- ods designed either for specific metrics or for specific metric properties.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Proposed Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Jin et al. <ref type="bibr" target="#b4">[12]</ref> established a formal method for compar- ing two different measures and introduced two criteria for formal comparison of the goodness of evaluation metrics, namely the degree of consistency (DoC) and degree of discriminancy (DoD). Applying these criteria, they showed theoretically and empirically that AUC is a better measure than accuracy in evaluating the perfor- mance of classifiers. <ref type="bibr" target="#b6">[13]</ref> [14] applied formal constraints based on axiometry to compare and judge evaluation metrics depending on the grade of satisfaction of these constraints. Busin et al. <ref type="bibr" target="#b10">[15]</ref> used axiometrics to define a formal and general notation that fits any effectiveness metric. Based on this notation, they proposed several axioms that should be satisfied by an effectiveness met- ric. They used these axioms as criteria to evaluate met- rics. All these papers deal with the problem only from a theoretical axiometrical point of view without taking into account the classification goal and the nature and properties of data being classified.</p><p>Sakai <ref type="bibr" target="#b2">[11]</ref> proposed a method for evaluating evalu- ation metrics by measuring their sensitivity using Boot- strap Hypothesis Tests, and used this method in com- We propose a method for choosing the most suitable metric for evaluating image segmentation. In Sec- tions 3.1 to 3.3, the method is described and discussed formally. Then this formal description is explained in a step by step demonstration with a real example in Sec- tion 4, which also provides an experimental evaluation of the method.</p><p>Given a set of effectiveness metrics M and a set of segmentations C, each of the segmentations is evalu- ated against its ground truth segmentation using all metrics to obtain a ranking per metric. Now, choos- ing the most suitable metric goes in two main steps: (i) Constructing different partitions on the segmenta- tion set C and ranking the subsets of each partition ac- cording to their average quality regarding each metric.</p><p>(ii) Inferring the metric bias from the rank correlations across all partitions and all metrics. In the following, each of the steps is described in more details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Constructing partitions and rank structure:</head><p>1. For each metric m â M, evaluate each of the segmen- tations x â C against its ground truth segmentation to get the score matrix s where s(x, m) is the score of segmentation x measured by metric m.</p><p>2. For each metric m â M, assign each segmentation x â C a rank depending on its score to get the rank matrix r where r(x, m) is the rank of segmentation x measured by metric m.</p><p>3. Define a set F of t segmentation properties. given a particular partition R (we will call this correla- tion the biased correlation K). They are given by</p><formula xml:id="formula_0">Ë K(m t ) = 1 | Ë P|.|M| â â r[ Ë R(i, ., m t ), Ë R(i, ., m)] (4) iâËPiâË iâËP mâM K(m t , p) = 1 |M| â mâM r[R(p, ., m t ), R(p, ., m)] (5) P ij (x) = 1 x â subset j of partition i 0 otherwise<label>(1)</label></formula><p>5. Construct t random partitionÅ¡ P = { Ë P 1 , .., Ë P t } by ran- domly assigning segmentations to s equal subsets in each partition. The functioÅ P ij (x) that assigns a seg- mentation x to the subset j of the random partition i is defined by where m t is the metric being evaluated, p is a given partition, and r(x1, x2) is the Pearson's correlation co- efficient between the rankings x1 and x2 (the point de- notes all possible values, e.g. R(p, ., m) means all pos- sible subset ranks in partition p measured by metric m)</p><p>Now, we define the overall bias of metric m t to be the average of the absolute correlation change B(m t ) which is given by </p><formula xml:id="formula_1">B(m t ) = 1 Ë P ij (x) = 1 x â subset j of random partition i 0 otherwise (2) |P| â abs[K(m t , i) â Ë K(m t )]<label>(6)</label></formula><formula xml:id="formula_2">s mi (j) = ( â P ij (x)=1 r(m, x))/n ij<label>(3)</label></formula><p>where x â C are the individual segmentations and n ij is the number of segmentations in the subset j of Partition i. Now, use the rank averages from Equa- tion 3 to compute the rank structure R = R(i, j, m) that gives the rank of subset j of partition i mea- sured by metric m according to descending rank av- erage. Analogously, Ë R = Ë R(i, j, m) gives the rank of subset j of the random partition i measured by met- ric m. Note that ranking the subsets using the aver- ages of the individual ranks in each subset is a rank- ing method inspired by the Mann-Whitney-Wilcoxon (MWW) test <ref type="bibr" target="#b15">[18]</ref>. This is because straightforwardly computing the ranks from score averages is sensitive to outliers and may produce unreasonable rankings if the scores are not normally distributed <ref type="bibr" target="#b17">[19]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Discussion:</head><p>To understand the key idea, let's think about the following two cases: Case 1, partition- ing the segmentations randomly. Case 2, partitioning the segmentations according to a particular property (e.g. class imbalance in the underlying segmentations). Given a particular metric m, the base correlatioÅ K(m) given by Equation 4 (related to Case 1) depends on the nature of the metric and is not affected by the proper- ties of the segmentations, since the partition is random. Now, if this correlation changes when we consider Case 2 (i.e. biased correlation K given by Equation 5), then the change is caused by the impact of the property used for partitioning (in this case class imbalance) on the metric and therefore it characterizes the bias of the metric towards/against this property. If many parti- tions (many properties) are used, then the sum of the correlation differences is a measure of the overall bias for the given metric.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Inferring metric bias:</head><p>Now, the rank structureÅ¡ R (rankings of the random partitions) and R (rankings of the non-random partitions) provide a statistical ba- sis to infer metric bias by analyzing how rankings of the different metrics and different partitions are corre- lated. The analysis is primarily based on comparing two correlations: the average of the rank correlations given the random partitionÅ¡ R (we will call this corre- lation the base correlatioÅ K) and the rank correlation</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section, the proposed method is demonstrated and tested with a real example, namely a set of 229 au- tomatic brain tumor segmentations (MRI 3D volumes) from the BRATS2012 challenge 1 . The segmentations correspond to 47 medical cases and were produced by five different algorithms participating in BRATS chal- lenge. To build the rank structure (described in Sec- tion 3.1), all segmentations were evaluated against their ground truth segmentations using 18 metrics (listed in <ref type="table">Table 1</ref>) to get the score matrix s (Step 1). Then global ranks were calculated from scores to get a ranking per metric r (Step 2). A set of 7 properties, namely seg- ment size, noise, class imbalance, connected compo- nent count, point variance, sphereness, and recall, was defined (Step 3). Now, 7 partitions of the segmentations were constructed each time using one of the defined properties. Each consists of 10 subsets with 229 10 â 22 segmentations (Step 4). A random partition with 10 equal subsets was constructed (Step 5). For each par- tition, the subsets were ranked using the sum of indi- vidual global ranks r to get 18 rankings per partition (126 rankings in total). The random partition was also ranked to get 18 rankings (Step 6). To infer metric bias, Equations 4, 5, and 6 in Section 3.2 were applied to the resulting rank structure. The result of this step is a metric list ranked according to bias.</p><p>To validate the suitability ranking produced by the proposed approach, a manual ranking done by a ra- diology expert was used: for each medical case, the five corresponding segmentations were ranked by their quality from a medical point of view (we call these the manual rankings). Analogously, for each medical case, 18 rankings of the five segmentations were produced each time using one of metrics (we call these the met- ric rankings). The average correlation between manual rankings and metric rankings was computed for each metric and finally the metrics were sorted according to this average correlation. The resulting metric ranking <ref type="table">(Table 1</ref> column 'manual') was used as a ground truth suitability ranking of the metrics to validate the auto- matic ranking. <ref type="table">Table 1</ref> column 'automatic' contains for each metric the bias (Equation 6) and the corresponding suitability rank computed according to ascending bias. A moderate to strong correlation between the two rank- ings can be observed. The six best metrics are the same in both rankings. This correlation shows that metrics with low bias produce rankings that are more corre- lated to manual rankings than others.  <ref type="table">Table 1</ref>. Manual and automatic metric suitability rank- ings. In column 'manual', the average correlation be- tween metric rankings and the manual rankings as well as corresponding suitability ranks according to decend- ing correlation. In column 'automatic', the metric bias calculated automatically by the proposed method as well as the ranks according to ascending bias (detailed data and results available in <ref type="bibr" target="#b19">[20]</ref>) mentations of other types and validated against rank- ings from different experts. A further issue to be in- vestigated in future work is the influence of weight- ing the properties in Equation 6 on the metric suit- ability ranking, if it is known that particular proper- ties are more/less important for the segmentation goal. For example, the manual ranking used to validate this method is done by a radiologist who may emphasise recall on cost of precision to assure that the tumor is completely removed. In this case weighting the recall and precision properly could improve the result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion and future work 6 Acknowledgments</head><p>For evaluating segmentations, metrics can be chosen according to their bias (Equation 6) toward/against the properties of the segmentations being evaluated. Test results show that the ranking produced by metrics with low bias generally have higher correlation with man- ual ranking than rankings produced by other metrics. In future work, the method will be tested with seg- </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">References</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>iâP 6. For each metric m â M, for each partition i â P, rank the subsets j according to the average of the individ- ual ranks in each subset using the rank function Finally, the metrics in M are ranked according to their overall bias, where the metric with the lowest bias is the most suitable.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Construct t different partitions on the segmentation set C, each partition according to one feature from F, i.e. according to the grade of occurrence of the feature in the segmentations. One gets the set of par- titions P = {P 1 , .., P t }. Each partition should have the same number of subsets s. The function P ij (x) assigns the segmentation x to the subset j according to partition i.</head><label></label><figDesc></figDesc><table>These 
can be any features thought to impact metrics e.g. 
class imbalance, number of segments, segment 
size, noise, deviation, shape signatures, sphereness, 
boundary smoothness, resolution, moments, etc. 
Furthermore, features can also be score-dependent 
e.g. precision and recall for utilizing trade-off i.e. for 
evaluation that tends to reward precision on cost of 
recall and vice versa. If no features are known to 
impact metrics, simply use all available features. 4. </table></figure>

			<note place="foot" n="1"> MICCAI 2012 Challenge on Multimodal Brain Tumor Segmentation, www2.imm.dtu.dk/projects/BRATS2012</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Analysis of distance/similarity measures for diffusion tensor imaging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">H J M</forename><surname>Peeters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">R</forename><surname>Rodrigues</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vilanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ter Haar Romeny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Visualization and Processing of Tensor Fields: Advances and Perspectives</title>
		<meeting><address><addrLine>Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Beware of relatively large but meaningless improvements</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Blanco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zaragoza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Tech. Rep., Yahoo! Research</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Evaluating evaluation metrics based on the bootstrap</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tetsuya</forename><surname>Sakai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting>the 29th annual international ACM SIGIR conference on Research and development in information retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="525" to="532" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Comparison of evaluation metrics in classification applications with imbalanced datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehrdad</forename><surname>Fatourechi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rabab</forename><forename type="middle">K</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><forename type="middle">G</forename><surname>Mason</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jane</forename><surname>Huggins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alois</forename><surname>Schloegl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gary</forename><forename type="middle">E</forename><surname>Birch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICMLA</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="777" to="782" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Using AUC and accuracy in evaluating learning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><forename type="middle">X</forename><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="299" to="310" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Valmet: A new validation tool for assessing and improving 3D object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guido</forename><surname>Gerig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Jomier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miranda</forename><surname>Chakos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4th International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<meeting>the 4th International Conference on Medical Image Computing and Computer-Assisted Intervention</meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="516" to="523" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A comparison of extrinsic clustering evaluation metrics based on formal constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enrique</forename><surname>Amigo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julio</forename><surname>Gonzalo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Artiles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felisa</forename><surname>Verdejo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf. Retr</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="461" to="486" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Image similarity using mutual information of regions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">B</forename><surname>Russakoff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlo</forename><surname>Tomasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Torsten</forename><surname>Rohlfing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Calvin</forename><forename type="middle">R</forename><surname>Maurer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jr</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">8th European Conference on Computer Vision, ECCV</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="596" to="607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Information theoretic measures for clusterings comparison: Variants, properties, normalization and correction for chance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Nguyen Xuan Vinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Epps</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bailey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">9999</biblScope>
			<biblScope unit="page" from="2837" to="2854" />
			<date type="published" when="2010-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Information theoretic measures for clusterings comparison: is a correction for chance necessary?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Nguyen Xuan Vinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Epps</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bailey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th Annual International Conference on Machine Learning</title>
		<meeting>the 26th Annual International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1073" to="1080" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Axiometrics: An axiomatic approach to information retrieval effectiveness metrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Busin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Mizzaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on the Theory of Information Retrieval</title>
		<meeting>the 2013 Conference on the Theory of Information Retrieval<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">29</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">On the reliability of information retrieval metrics based on graded relevance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tetsuya</forename><surname>Sakai</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
	<note>Information Processing Management</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Evaluation: From precision, recall and F-factor to ROC, informedness, markedness correlation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">W</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Powers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Technologies</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="37" to="63" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Evaluating evaluation measure stability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Buckley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ellen</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd annual international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting>the 23rd annual international ACM SIGIR conference on Research and development in information retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2000" />
			<biblScope unit="page" from="33" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The effect of topic set size on retrieval experiment error</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ellen</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Buckley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th annual international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting>the 25th annual international ACM SIGIR conference on Research and development in information retrieval</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="316" to="323" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">On a test of whether one of two random variables is stochastically larger than the other</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">B</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">R</forename><surname>Whitney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Mathematical Statistics</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="50" to="60" />
			<date type="published" when="1947" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A subjective experiment for 3d-mesh segmentation evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Halim</forename><surname>Benhabiles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Lavoue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><forename type="middle">Phillipe</forename><surname>Vandeborre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Daoudi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Workshop on Multimedia Signal Processing (MMSP)</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Statistical comparisons of classifiers over multiple data sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janez</forename><surname>Demsar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page">30</biblScope>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Comparing the sensitivity of information retrieval metrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Filip</forename><surname>Radlinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Craswell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33rd international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting>the 33rd international ACM SIGIR conference on Research and development in information retrieval</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Test data and results of the automatic metric selection method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdel</forename><forename type="middle">Aziz</forename><surname>Taha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Allan</forename><surname>Hanbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Jimenez</surname></persName>
		</author>
		<ptr target="http://publik.tuwien.ac.at/files/PubDat229008.pdf" />
		<imprint>
			<date type="published" when="2014" />
		</imprint>
		<respStmt>
			<orgName>Vienna University of Technology</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Tech. Rep</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
