"path";"PDF";"original";"extracted";"original value";"extracted value";"Precision";"Recall";"F1"
"TUW-137078";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\groundtruth\TUW-137078.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\result\result-TUW-137078-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\extracted\grobid\grobid-TUW-137078-xstream.xml"")";"The aim of the ECIC project is the dissemination of participative methods of organizational development originating from Scandinavian countries to other European countries. To support this process we developed a hypertextual learning system. Because of the heterogeneous target group and the ill-structured domains of ECIC detailed usability testing is necessary. First tentative results indicate that graphical overview maps play an important role and interactive examples are very motivating.";"The aim of the ECIC project is the dissemination of participative methods of organizational development originating from Scandinavian countries to other European countries. To support this process we developed a hypertextual learning system. Because of the heterogeneous target group and the ill-structured domains of ECIC detailed usability testing is necessary. First tentative results indicate that graphical overview maps play an important role and interactive examples are very motivating. INTRODUCTION The aim of the European Continuous Improvement Circles Project (ECIC) is to disseminate participative methods of organizational development originating from Scandinavian countries to the rest of Europe. Examples for such methods are improvement circles where people come together to discuss a given topic, for instance implications of new information technology for their workplace. A facilitator guides this discussion process in order to attain agreed goals.";"0,00";"0,00";"0,00"
"TUW-138011";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\groundtruth\TUW-138011.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\result\result-TUW-138011-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\extracted\grobid\grobid-TUW-138011-xstream.xml"")";"The role of medical guidelines is becoming more and more important in the medical field. Within the Protocure project it has been shown that the quality of medical guidelines can be improved by formalisation. However formal-isation turns out to be a very time-consuming task, resulting in a formal guideline that is much more complex than the original version and the relation with this original guideline is often unclear. This paper presents a case study where the relation between the informal medical guideline and its formal counterpart is investigated. This has been used to determine the gaps between the formal and informal guidelines and the cause of the size explosion of the formal guidelines.";;"none extracted value";"0,00";"0,00"
"TUW-138447";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\groundtruth\TUW-138447.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\result\result-TUW-138447-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\extracted\grobid\grobid-TUW-138447-xstream.xml"")";"A key problem in multimedia systems is the faithful reproduction of color. One of the main reasons why this is a complicated issue are the different color reproduction technologies used by the various devices; displays use easily modeled additive color mixing, while printers use a subtractive process, the characterization of which is much more complex than that of self-luminous displays. In order to resolve these problems several processing steps are necessary, one of which is accurate device characterization. Our study examines different learning algorithms for one particular neural network technique which already has been found to be useful in related contexts – namely radial basis function network models – and proposes a modified learning algorithm which improves the colorimetric characterization process of printers. In particular our results show that is possible to obtain good performance by using a learning algorithm that is trained on only small sets of color samples, and use it to generate a larger look–up table (LUT) through use of multiple polynomial regression or an interpolation algorithm. We deem our findings to be a good start point for further studies on learning algorithms used in conjunction with this problem.";"A key problem in multimedia systems is the faithful reproduction of color. One of the main reasons why this is a complicated issue are the different color reproduction technologies used by the various devices; displays use easily modeled additive color mixing, while printers use a subtractive process, the characterization of which is much more complex than that of self-luminous displays. In order to resolve these problems several processing steps are necessary, one of which is accurate device characterization. Our study examines different learning algorithms for one particular neural network technique which already has been found to be useful in related contexts-namely radial basis function network models-and proposes a modified learning algorithm which improves the colorimetric characterization process of printers. In particular our results show that is possible to obtain good performance by using a learning algorithm that is trained on only small sets of color samples, and use it to generate a larger look-up table (LUT) through use of multiple polynomial regression or an interpolation algorithm. We deem our findings to be a good start point for further studies on learning algorithms used in conjunction with this problem. Keywords: Radial basis function networks, regression, colorimetric characterization of printing devices 1. INTRODUCTION In multimedia systems, different color reproduction devices-while serving the same purpose-exhibit large discrepancies in their raw output. This is due to the fact that they usually employ different color mixing technologies (additive or subtractive), Correspondence: Email: artusi@cg.tuwien.ac.at use different input color spaces and hence have different gamuts, and that their device characteristics can change with time and usage. These facts usually do not permit a faithful matching of colors between devices if no precautions are taken. Colorimetric characterization is one step in the colorimetric reproduction process that permits faithful image reproduction across different display devices. Its goal is to define a mapping function between the device-dependent color spaces in question (such as RGB or CMYK) and device-independent color spaces (such as CIELAB or CIEXYZ), and vice versa. There are three main approaches to defining this mapping function: physical models, empirical models and exhaustive measurements. Physical modeling of imaging devices involves building mathematical models that find a relationship between the colorimetric coordinates of the input (or output) image element and the signals used to drive an output device (or the signals originating from an input device). The advantage of these approaches is that they are robust, typically require few colorimetric measurements in order to characterize the device, and allow for easy recharacterization if some component of the imaging system is modified. The disadvantage is that the models are often quite complex to derive and can be complicated to implement. Physical models are often used for the colorimetric characterization of displays and scanners. Empirical modeling of imaging devices involves collecting a fairly large set of data and then statistically fitting a relationship between device coordinates and colorimetric coordinates. Empirical models are often higher-order multidimensional polynomials, or neural network models. They require fewer measurements than LUT techniques, but they need more than physical models. Empirical models are often used for scanners and printers. Often the colorimetric characterization of printers requires an exhaustive measurement in order to obtain good performances. samples of the device drive signals are sampled and colorimetrically measured. Many more measurements have to be used for devices with poor repeatability. Lookup tables can be used to process image data via multidimensional interpolation. This technique has different disadvantages: the large number of measurements that has to be made, difficulties in interpolating the highly nonlinear data and difficult recharacterization if any aspect of the device changes. The advantage of exhaustive measurement techniques is that they require no knowledge of the device physics. In general a good algorithm for colorimetric characterization must have the following characteristics: small training set, fast response, good accuracy and it must allow for a fast recharacterization. This paper proposes a modification of an existing";"0,00";"0,00";"0,00"
"TUW-138544";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\groundtruth\TUW-138544.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\result\result-TUW-138544-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\extracted\grobid\grobid-TUW-138544-xstream.xml"")";"If visual information retrieval should make further progress, it will be necessary to identify new ways to derive visual properties from higher levels of understanding than the pixel level (e.g. from low-level features). The paper outlines the implementation of modelling of feature hierarchies in the visual information retrieval framework VizIR (free under GPL). The approach allows for the derivation of high-level features from low-level features by aggregation and localisa-tion as well as semantic enrichment with additional knowledge. The technical implementation is based on the MPEG-7 structures for aggregation and specialisation.";"If visual information retrieval should make further progress, it will be necessary to identify new ways to derive visual properties from higher levels of understanding than the pixel level (e.g. from low-level features). The paper outlines the implementation of modelling of feature hierarchies in the visual information retrieval framework VizIR (free under GPL). The approach allows for the derivation of high-level features from low-level features by aggregation and localisa-tion as well as semantic enrichment with additional knowledge. The technical implementation is based on the MPEG-7 structures for aggregation and specialisation. 1. INTRODUCTION Visual information retrieval (VIR) research is driven by the desire to derive semantically meaningful information directly from the content of media objects. Recent years have seen considerable efforts to overcome the enormous gravity of the pixel and to extract more than just colour histograms, edge information and other low-level features. The MPEG-7 standard [1] supports these efforts in two ways: Firstly, it provides aggregation and localisation structures for spatio-temporal feature enrichment. Secondly, it standardises cookbooks for the most prominent low-level features and, by that, provides a foundation for semantic enrichment of low-level features. Below, we will regard aggregated/localised and semantically enriched as two relevant types of high-level features. The paper describes the way high-level feature structures (e.g. based on MPEG-7 descriptors) can be implemented in the VIR framework VizIR. The VizIR project [2] aims at providing VIR researchers with a workbench of feature extraction , querying and evaluation tools. The paper is organised as follows: Section 2 gives background information on relevant MPEG-7 models and principles of semantic feature design. Section 3 shortly sketches the VizIR project. Section 4 describes the architecture of high-level feature modelling in VizIR. Additionally, Subsection 4.4 covers important implementation aspects.";"0,00";"0,00";"0,00"
"TUW-138547";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\groundtruth\TUW-138547.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\result\result-TUW-138547-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\extracted\grobid\grobid-TUW-138547-xstream.xml"")";"This paper describes solutions for parallel video processing based on LAN-connected PC-like workstations. We outline application scenarios for the processing of video with broadcast TV resolution and promising areas for future research. Additionally, we describe a prototype system implemented in our workgroup. This Network-of-Workstations is based on Gigabit Ethernet, free Java software and standard network protocols. Video data is streamed to and from processing hosts using IP multicast and the Real-time Transfer Protocol. Control mechanisms are based on network file sharing. In comparison to related approaches, our system makes use of high-performance network hardware and open source software. In consequence, our system is easier to implement, cheaper and more flexible than, for example, highly integrated commercial solutions.";"This paper describes solutions for parallel video processing based on LAN-connected PC-like workstations. We outline application scenarios for the processing of video with broadcast TV resolution and promising areas for future research. Additionally, we describe a prototype system implemented in our workgroup. This Network-of-Workstations is based on Gigabit Ethernet, free Java software and standard network protocols. Video data is streamed to and from processing hosts using IP multicast and the Real-time Transfer Protocol. Control mechanisms are based on network file sharing. In comparison to related approaches, our system makes use of high-performance network hardware and open source software. In consequence, our system is easier to implement, cheaper and more flexible than, for example, highly integrated commercial solutions. 1. Introduction";"100,00";"100,00";"100,00"
"TUW-139299";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\groundtruth\TUW-139299.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\result\result-TUW-139299-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\extracted\grobid\grobid-TUW-139299-xstream.xml"")";"This thesis presents methods to support protocol-based care in medicine. Time-oriented treatment plans and patient data are represented visually providing various interaction possibilities to aid execution and analysis of medical therapy plans formulated in the representation language Asbru. We introduce a two-view approach consisting of a Logical View and a Temporal View. The Logical View depicts therapy plans using a flow-chart like representation based on ""clinical algorithm maps"". The Temporal View on the other hand depicts plans as well as patient data in form of parameters and variables over time. The plan visualization method within the Temporal View is based on the idea of LifeLines. For being able to depict hierarchical structures and temporal uncertainties, we extended this concept and a novel glyph called PlanningLine has been developed. The development is embedded into a 3-step evaluation process including a user study with eight domain experts (physicians) at the beginning to acquire users' needs, a design evaluation, and an evaluation of our software prototype at the end of the thesis project.";"This thesis presents methods to support protocol-based care in medicine. Time-oriented treatment plans and patient data are represented visually providing various interaction possibilities to aid execution and analysis of medical therapy plans formulated in the representation language Asbru. We introduce a two-view approach consisting of a Logical View and a Temporal View. The Logical View depicts therapy plans using a flow-chart like representation based on ""clinical algorithm maps"". The Temporal View on the other hand depicts plans as well as patient data in form of parameters and variables over time. The plan visualization method within the Temporal View is based on the idea of LifeLines. For being able to depict hierarchical structures and temporal uncertainties, we extended this concept and a novel glyph called PlanningLine has been developed. The development is embedded into a 3-step evaluation process including a user study with eight domain experts (physicians) at the beginning to acquire users' needs, a design evaluation, and an evaluation of our software prototype at the end of the thesis project. 1 Introduction";"100,00";"100,00";"100,00"
"TUW-139761";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\groundtruth\TUW-139761.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\result\result-TUW-139761-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\extracted\grobid\grobid-TUW-139761-xstream.xml"")";"This master thesis describes how to price options by means of Genetic Programming. The underlying model is the Generalized Autoregressive Conditional Heteroskedastic (GARCH) asset return process. The goal of this master thesis is to nd a closed-form solution for the price of European call options where the underlying securities follow a GARCH process. The data are simulated over a wide range to cover a lot of existing options in one single equation.
Genetic Programming is used to generate the pricing function from the data. Genetic Programming is a method of producing programs just by defining a problemdependent fitness function. The resulting equation is found via a heuristic algorithm inspired by natural evolution. Three different methods of bloat control are used. Additionally Automatic Defined Functions (ADFs) and a hybrid approach are tested, too. To ensure that a good configuration setting is used, preliminary testing of many different settings has been done, suggesting that simpler configurations are more successful in this environment.
The resulting equation can be used to calculate the price of an option in the given range with minimal errors. This equation is well behaved and can be used in standard spread sheet programs. It offers a wider range of utilization or a higher accuracy, respectively than other existing approaches.";"This master thesis describes how to price options by means of Genetic Programming. The underlying model is the Generalized Autoregressive Conditional Heteroskedastic (GARCH) asset return process. The goal of this master thesis is to find a closed-form solution for the price of European call options where the underlying securities follow a GARCH process. The data are simulated over a wide range to cover a lot of existing options in one single equation. Genetic Programming is used to generate the pricing function from the data. Genetic Programming is a method of producing programs just by defining a problem-dependent fitness function. The resulting equation is found via a heuristic algorithm inspired by natural evolution. Three different methods of bloat control are used. Additionally Automatic Defined Functions (ADFs) and a hybrid approach are tested, too. To ensure that a good configuration setting is used, preliminary testing of many different settings has been done, suggesting that simpler configurations are more successful in this environment. The resulting equation can be used to calculate the price of an option in the given range with minimal errors. This equation is well behaved and can be used in standard spread sheet programs. It offers a wider range of utilization or a higher accuracy, respectively than other existing approaches. Zusammenfassung Diese Diplomarbeit beschreibt, wie Optionen mit Hilfe Genetischer Programmierung bewertet werden können. Das zugrunde liegende Modell nennt sich GARCH (Gen-eralized Autoregressive Conditional Heteroskedastic) Renditeprozess. Das Ziel dieser Diplomarbeit ist eine geschlossene Formel, die als Ergebnis den Preis einer europäischen Kaufoption liefert, dessen dahinter liegende Wertpapier einem GARCH Prozess folgt. Die Daten werden innerhalb eines breiten Wertebereiches simuliert, um die meisten existierenden Optionen mit einer Formel bewerten zu können. Die Formel wird mittels Genetischer Programmierung aus den Daten generiert. Genetische Programmierung ist eine Methode, bei der nur durch Definition einer zum Problem passenden Bewertungsfunktion vollständige Programme produziert werden können. Die Ergebnisgleichung wird schließlich mittels eines der EvolutionähnlichenEvolutionähnlichen Algorithmus gefunden. Drei verschiedene Methoden zum Bloat Control wurden ver-wendet. Zusätzlich wurden auch Automatisch Definierte Funktionen sowie ein hybrider Ansatz untersucht. Um sicherzustellen, dass eine gute Konfiguration gewählt wird, gibt es Vortests vieler verschiedener Konfigurationen. Es zeigt sich, dass in diesem Umfeld einfachere Konfigurationen erfolgreicher sind. Die Ergebnisgleichung kann schließlich zur Errechnung der Optionspreise mit min-imalem Fehler verwendet werden. Diese Gleichung verhält sich gut und kann auch in Standardtabellenkalkulationen verwendet werden. Im Vergleich mit anderen existieren-den Ansätzen, bietet diese Gleichung eine weitere Verwendbarkeit beziehungsweise eine höhere Genauigkeit. Acknowledgements I have to thank my family, my professors and all my friends. Special thanks to Dr. Hanke, who has helped me to find this interesting topic of research and to Dr. Raidl, who has showed me how to write a good master thesis. All the brave programmers who have made libraries I have used, are mentioned here too. Magister Katarina Kocian has read my thesis very often to find even the last mistake. Without these people it would not have been possible to write this thesis.";"0,00";"0,00";"0,00"
"TUW-139769";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\groundtruth\TUW-139769.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\result\result-TUW-139769-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\extracted\grobid\grobid-TUW-139769-xstream.xml"")";"This thesis deals with local branching, a local search algorithm applied on top of a Branch and Cut algorithm for mixed integer programming problems. Local branching defines custom sized neighborhoods around given feasible solutions and solves them partially or completely before exploring the rest of the search space. Its goal is to improve the heuristic behavior of a given exact integer programming solver, i.e. to focus on finding good solutions early in the computation. Local branching is implemented as an extension to the open source Branch and Cut solver COIN/BCP. The framework's main goal is to provide a generic implementation of local branching for integer programming problems. IP problems are optimization problems where some or all variables are integer values and must satisfy one or more (linear) constraints. Several extensions to the standard local branching algorithm were added to the framework. Pseudo-concurrent exploration of multiple local trees, aborting local trees and a variable fixing heuristic allow the user to implement sophisticated search metaheuristics that adjust the local branching parameters adaptively during the computation. A major design goal was to provide a clean encapsulation of the local branching algorithm to facilitate embedding of the framework in other, higher-level search algorithms, for example in evolutionary algorithms. As an example application, a solver for the multidimensional knapsack problem is implemented. A custom local branching metaheuristic imposes node limits on local subtrees and adaptively tightens the search space by fixing variables and reducing the size of the neighborhood. Test results show that local branching can offer significant advantages to standard Branch and Cut algorithms and eventually proves optimality in shorter time. Especially for large, complex test instances exploring the local neighborhood of a good feasible solution often yields better short-term results than the unguided standard Branch and Cut algorithm. Improving the solutions found early in the computation also helps to remove additional parts of the search tree, potentially leading to better solutions in longer runs. ";"This thesis deals with local branching, a local search algorithm applied on top of a Branch and Cut algorithm for mixed integer programming problems. Local branching defines custom sized neighborhoods around given feasible solutions and solves them partially or completely before exploring the rest of the search space. Its goal is to improve the heuristic behavior of a given exact integer programming solver, i.e. to focus on finding good solutions early in the computation. Local branching is implemented as an extension to the open source Branch and Cut solver COIN/BCP. The framework's main goal is to provide a generic implementation of local branching for integer programming problems. IP problems are optimization problems where some or all variables are integer values and must satisfy one or more (linear) constraints. Several extensions to the standard local branching algorithm were added to the framework. Pseudo-concurrent exploration of multiple local trees, aborting local trees and a variable fixing heuristic allow the user to implement sophisticated search metaheuristics that adjust the local branching parameters adaptively during the computation. A major design goal was to provide a clean encapsulation of the local branching algorithm to facilitate embedding of the framework in other, higher-level search algorithms, for example in evolutionary algorithms. As an example application, a solver for the multidimensional knapsack problem is implemented. A custom local branching metaheuristic imposes node limits on local subtrees and adaptively tightens the search space by fixing variables and reducing the size of the neighborhood. Test results show that local branching can offer significant advantages to standard Branch and Cut algorithms and eventually proves optimality in shorter time. Especially for large, complex test instances exploring the local neighborhood of a good feasible solution often yields better short-term results than the unguided standard Branch and Cut algorithm. Improving the solutions found early in the computation also helps to remove additional parts of the search tree, potentially leading to better solutions in longer runs.";"100,00";"100,00";"100,00"
"TUW-139781";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\groundtruth\TUW-139781.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\result\result-TUW-139781-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\extracted\grobid\grobid-TUW-139781-xstream.xml"")";;"Recent work in combinatorial optimization indicates the high potential of combining meta-heuristics with integer linear programming (ILP) techniques. We study here a hybrid system in which a memetic algorithm (MA) and a general purpose ILP solver based on branch-and-cut (B&C) are executed in parallel and continuously exchange information in a bidirectional, asyn-chronous way. As target problem, we consider the multidimensional knapsack problem (MKP). The memetic algorithm uses a direct binary encoding of candidate solutions and repair and local improvement strategies that are steered by pseudo-utility ratios. As B&C framework we use the general purpose commercial ILP-solver CPLEX. The information exchanged between the two heterogenous algorithms are so-far best primal solutions and promising dual variable values of solutions to certain linear programming (LP) relaxations. These dual variable values are used in the MA to update the pseudo-utility ratios of local improvement and repair. We will see that this combination of a metaheuristic and an exact optimization method is able to benefit from synergy: Experimental results document that within the same limited total time, the cooperative system yields better heuristic solutions than each algorithm alone. In particular, the cooperative system also competes well with today's best algorithms for the MKP, needing substantially shorter total running times.";"0,00";"none expected";"0,00"
"TUW-139785";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\groundtruth\TUW-139785.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\result\result-TUW-139785-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\extracted\grobid\grobid-TUW-139785-xstream.xml"")";"In this master thesis a generic libray of efficient metaheuristics for combinatorial optimization is presented. In the version at hand classes that feature local search, simulated annealing, tabu search, guided local search and greedy randomized adaptive search procedure were implemeted. Most notably a generic implementation features the advantage that the problem dependent classes and methods only need to be realized once without targeting a specific algorithm because these parts of the sourcecode are shared among all present algorithms contained in EAlib. This main advantage is then exemplarily demonstrated with the quadratic assignment problem. The sourcecode of the QAP example can also be used as an commented reference for future problems. Concluding the experimental results of the individual metaheuristics reached with the presented implementation are presented.";"In this master thesis a generic libray of efficient metaheuristics for combinatorial optimization is presented. In the version at hand classes that feature local search, simulated annealing, tabu search, guided local search and greedy randomized adap-tive search procedure were implemeted. Most notably a generic implementation features the advantage that the problem dependent classes and methods only need to be realized once without targeting a specific algorithm because these parts of the sourcecode are shared among all present algorithms contained in EAlib. This main advantage is then exemplarily demonstrated with the quadratic assignment problem. The sourcecode of the QAP example can also be used as an commented reference for future problems. Concluding the experimental results of the individual metaheuristics reached with the presented implementation are presented. Eine generische Implementierung bietet vorallem den Vorteil das bei einem neuen zu lösendem Problem nur einige bestimmte problemabhängige Klassen und Metho-den realisiert werden müssen ohne sich schon im Vorhinein einen speziellen Algorith-mus festzulegen, da diese Klassen und Methoden von allen in der EAlib vorhanden Metaheuristiken verwendet werden. Die Vorteile dieser Bibliothek werden anschließend anhand des Quadratic Assignment Problems ausführlich dargestellt. Dieses Beispiel dient zusätzlich auch noch als kommentierte Referenz für zukünftige Problemimplentierungen. Abschließend werden die Resulate der Experimente mit den verschiedenen Meta-heuristiken präsentiert. Danksagung An dieser stelle möchte ich mich bei allen Menschen bedanken die zum Gelingen dieser Diplomarbeit beigetragen haben. Dieser Dank gilt meinem Betreuer Prof. Raidl, der mich mit großer Geduld am Weg zum Abschluß begleitet hat und mit mir in den vielen Treffen oft nützliche Ideen entwickelt hat. Meinen Eltern und meinem Bruder Ronald danke ich für ein sorgloses Studium und die moralische Unterstützung wenn die Motivation einmal nicht so groß war. Bei meinen Studienkollegen, besonders bei Harry und Zamb, bedanke ich mich für die Freundschaft, den Spaß und die gegenseitige Unterstützung. Last but not least möchte ich mich auch bei meinen Mitbewohnern Sic0 und Leo bedanken, die mir während meiner Arbeit die nötige Ruhe zukommen ließen, aber natürlich auch ab und zu für willkommene Ablenkung gesorgt haben. Natascha danke ich für die schöne gemeinsame Zeit.";"0,00";"0,00";"0,00"
"TUW-140047";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\groundtruth\TUW-140047.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\result\result-TUW-140047-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\extracted\grobid\grobid-TUW-140047-xstream.xml"")";"This paper describes our submission to the MIREX 2006 Audio Music Similarity and Retrieval task. The task was to submit an audio feature extraction algorithm, to compute music similarity measures and to return a distance matrix from an audio collection consisting of 5000 pieces, which was subsequently evaluated through human listening tests as well as objective statistics. We submitted a new implementation of the Statistical Spectrum Descriptor (SSD) audio feature extractor and computed the distance matrix directly from feature space. Results from the human evaluation show that our approach is among the top 5 algorithms which furthermore show no statistically significant performance differences. The evaluation of a number of objective statistics ranked our algorithm 3rd in most of the cases. Our submission was one of the two fastest in terms of total runtime, having the shortest distance computation time. The approach has also been evaluated on Audio Cover Song Identification, where it was the bestperforming ""Audio Music Similarity and Retrieval"" submission, outperformed, however, by 4 submissions which were specifically designed for the cover identification task.";"This paper describes our submission to the MIREX 2006 Audio Music Similarity and Retrieval task. The task was to submit an audio feature extraction algorithm, to compute music similarity measures and to return a distance matrix from an audio collection consisting of 5000 pieces, which was subsequently evaluated through human listening tests as well as objective statistics. We submitted a new implementation of the Statistical Spectrum Descriptor (SSD) audio feature extractor and computed the distance matrix directly from feature space. Results from the human evaluation show that our approach is among the top 5 algorithms which furthermore show no statistically significant performance differences. The evaluation of a number of objective statistics ranked our algorithm 3rd in most of the cases. Our submission was one of the two fastest in terms of total run-time, having the shortest distance computation time. The approach has also been evaluated on Audio Cover Song Identification, where it was the best-performing ""Au-dio Music Similarity and Retrieval"" submission, outper-formed, however, by 4 submissions which were specifically designed for the cover identification task. 1. Introduction creation, music recommendation, or retrieval of ""similar"" pieces of music from an archive. Regardless of this open issue, MIREX is well established as the forum for annual exchange within the research community and evaluation of algorithm performance for a range of different tasks and approaches the MIR community is faced with, and that researchers are currently working on. Our department has a strong background on information retrieval with a focus on data visualization and clustering of data, e.g. on Self-Organizing Maps. As ""Department of Software Technology and Interactive Systems"" we are also developing applications for interaction with data, such as music and text archives [1]. Besides the traditional focus on unsupervised clustering of archives, we furthermore investigate machine learning approaches for classification of data collections, for tasks such as music genre classification [2]. In any of these areas efficient feature extraction from audio is required and therefore we are as well active in research on audio feature extractors. MIREX is a great opportunity for us to evaluate the audio features we employ in our applications and to compare them with state-of-the-art algorithms, both in terms of efficiency with regard to similarity as well";"0,00";"0,00";"0,00"
"TUW-140048";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\groundtruth\TUW-140048.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\result\result-TUW-140048-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\extracted\grobid\grobid-TUW-140048-xstream.xml"")";"UN/CEFACT's Modeling Methodology (UMM) is a well accepted and formal notation to analyze and design B2B business processes. In a service oriented architecture (SOA) environment process specification languages like the Business Process Specification Schema (BPSS) are used to configure B2B information systems. However , mappings from UMM models to BPSS process specifications currently exist just on a conceptual level. This results in a gap between defined B2B processes and BPSS configurable e-commerce systems. Thus, a model driven code generation of BPSS descriptions is required. In this paper we present a technical implementation of a transformation engine that generates BPSS process specifications from a UMM model represented in the XML Metadata Interchange (XMI) language. This implementation bridges the gap mentioned above. It has been used in the EU project GILDAnet to generate BPSS descriptions from logistic processes modeled in UMM.";"is a well accepted and formal notation to analyze and design B2B business processes. In a service oriented architecture (SOA) environment process specification languages like the Business Process Specification Schema (BPSS) are used to configure B2B information systems. However , mappings from UMM models to BPSS process specifications currently exist just on a conceptual level. This results in a gap between defined B2B processes and BPSS configurable e-commerce systems. Thus, a model driven code generation of BPSS descriptions is required. In this paper we present a technical implementation of a transformation engine that generates BPSS process specifications from a UMM model represented in the XML Metadata Interchange (XMI) language. This implementation bridges the gap mentioned above. It has been used in the EU project GILDAnet to generate BPSS descriptions from logistic processes modeled in UMM. 1 Motivation Business process modeling has traditionally focused on describing intra-organizational processes. In a business-to-business (B2B) environment two or more organizations take part in an inter-organizational process. Consequently an agreement of all participants on a shared business process, also called collaboration, is required. However, each of the participating partners describes the shared process from his point of view. Therefore the described sights will not match. Thus, in order to specify shared processes it is inevitable to use a method that describes the process from a common point of view. UN/CEFACT's Modeling Methodology (UMM) [UN/01] is such a well accepted method in the B2B sector. Furthermore, it has to be a modeler's goal to use the designed processes for real business instead of leaving them in some unread manuals or strategic papers. It is the intention of UMM to describe processes not just for human understanding but also to create machine-interpretable artifacts. In order to configure e-commerce information systems dynamically in changing environments (e.g. partners, processes, etc.), system-executable process specifications are needed. The Business Process Specification Schema (BPSS) [UN/03], known as a part of the ebXML framework, is such an XML-based process definition language. However, the mapping of relevant segments of a UMM model to a BPSS instance is only conceptually denoted so far [HH04a] [HHK05]. Thus a tool implementation is required , which generates BPSS instances out of UMM models. In this paper we present the implementation of a tool which transforms UMM models into BPSS files. In the EU funded GILDAnet project this transformation engine was developed to generate system-executable BPSS descriptions from modeled supply chain processes. The remainder of this paper is structured in six sections. In section 2 we present related work before giving an overview about the transformation process in section 3. Section 4 to 6 describe each of the three transformation stages. Section 7 finishes the paper with a conclusion. 2 The transformation process: An overview The transformation engine we implemented transforms UMM models to BPSS process specifications. The overall transformation process covered by our implementation spans over three major stages (denoted by the gray arrows in figure 1). The engine needs a valid UMM model as input to the first stage and outputs a BPSS compliant process specification after completing the third stage. Considering the input format, it is a definitive goal of this implementation to stay independent of specific UML modeling tools. Thus, an input format is required which is supported by a wide range of different modeling environments. For us the widespread XML Meta-data Interchange (XMI) [Obj00] standard was the candidate of choice as input format specification. In the first stage of the workflow, the object structure that corresponds to the XMI tree is dissolved. Then equal UML element types are grouped in list data structures in order to ease and speed up further processing. These collections are input to the second stage, where we map these elements to an object structure that conforms to UMM meta model. Furthermore, some basic consistency checks are applied during the mapping to ensure a valid UMM instance. Within the third stage the valid UMM object representation is mapped to a BPSS instance according to the approach described in [HH04a]. Splitting the workflow into the three stages described above enhances the modularization of the transformation engine. The object representation of the UMM meta model acts as the core of the engine. Additional modules that use this core, but implement different input or output formats may easily be implemented. Candidates for other output formats of a UMM transformation might be the";"0,00";"0,00";"0,00"
"TUW-140229";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\groundtruth\TUW-140229.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\result\result-TUW-140229-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\extracted\grobid\grobid-TUW-140229-xstream.xml"")";"Detecting the needs of learners is a challenging but essential task to be able to provide adaptivity. In this paper we present a tool that enables learning management systems (LMS) to detect learning styles based on the behavior of learners during an online course. By calculating the learning styles and filling the student model of LMS with such personal data, a basis for adaptivity is provided.";"Detecting the needs of learners is a challenging but essential task to be able to provide adaptivity. In this paper we present a tool that enables learning management systems (LMS) to detect learning styles based on the behavior of learners during an online course. By calculating the learning styles and filling the student model of LMS with such personal data, a basis for adaptivity is provided. MASPLANG [2] and CS383 [3]. But these systems use a questionnaire for detecting learning styles. Garcia et al. [4] investigated the use of Bayesian networks to detect learning styles based on the behavior of learners in a web-based educational system. While their work is focused on the use of Bayesian networks, our approach sums up indications of preferences based on patterns, equally to the approach of learning style questionnaires. Moreover, we propose a tool for LMS in general rather than for one specific system.";"0,00";"0,00";"0,00"
"TUW-140253";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\groundtruth\TUW-140253.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\result\result-TUW-140253-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\extracted\grobid\grobid-TUW-140253-xstream.xml"")";"Process types – a kind of behavioral types – specify constraints on message acceptance for the purpose of synchronization and to determine object usage and component behavior in object-oriented languages. So far process types have been regarded as a purely static concept for Actor languages incompatible with inherently dynamic programming techniques. We propose solutions of related problems causing the approach to become useable in more conventional dynamic and concurrent languagues. The proposed approach can ensure message acceptability and support local and static checking of race-free programs.";"a kind of behavioral types-specify constraints on message acceptance for the purpose of synchronization and to determine object usage and component behavior in object-oriented languages. So far process types have been regarded as a purely static concept for Actor languages incompatible with inherently dynamic programming techniques. We propose solutions of related problems causing the approach to become useable in more conventional dynamic and concurrent languagues. The proposed approach can ensure message acceptability and support local and static checking of race-free programs. 1 INTRODUCTION";"100,00";"100,00";"100,00"
"TUW-140308";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\groundtruth\TUW-140308.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\result\result-TUW-140308-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\extracted\grobid\grobid-TUW-140308-xstream.xml"")";"We show that algorithmic thinking is a key ability in informatics that can be developed independently from learning programming. For this purpose we use problems that are not easy to solve but have an easily understandable problem definition. A proper visualization of these problems can help to understand the basic concepts connected with algorithms: correctness, termination , efficiency, determinism, parallelism, etc. The presented examples were used by the author in a pre-university course, they may also be used in secondary schools to help understanding some concepts of computer science.";"We show that algorithmic thinking is a key ability in informatics that can be developed independently from learning programming. For this purpose we use problems that are not easy to solve but have an easily understandable problem definition. A proper visualization of these problems can help to understand the basic concepts connected with algorithms: correctness, termination , efficiency, determinism, parallelism, etc. The presented examples were used by the author in a pre-university course, they may also be used in secondary schools to help understanding some concepts of computer science. 1 Introduction In autumn 2005 the Faculty of Informatics at the Vienna University of Technology started to offer a pre-university course [1] (propaedeutic course), similar to other universities, e.g. [2], for all applicants who intended to start one of the bachelor studies in Informatics. This course addresses all beginners and has many-fold reasons: 1. Lack of pre-knowledge, what is Informatics after all 2. Lack of pre-knowledge, how computers work 3. Lack of pre-knowledge about algorithms 4. Lack of pre-knowledge about programming 5. Lack of sufficient knowledge in mathematics These facts were observed by our lecturers and were confirmed by a survey of our beginners. Although these topics are part of the secondary school curriculum and should be known by all students passing secondary school, most of our beginners have not enough skills and pre-knowledge that is necessary to start a university study in Computer Science. The consequences were a very high drop out rate during the first study year and a low success rate in the topics Programming and Algorithms & Data Structures. The pre-university course should overcome the lack of usual pre";"0,00";"0,00";"0,00"
"TUW-140533";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\groundtruth\TUW-140533.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\result\result-TUW-140533-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\extracted\grobid\grobid-TUW-140533-xstream.xml"")";"Formal specification and verification of software have made small but continuous advances throughout its long history, and have reached a point where commercial tools became available for verifying programs semi-automatically or automatically. The aim of the master thesis is to evaluate commercial and academic verification tools with respect to their usability in developing software and in teaching formal methods. The thesis will explain the theoretical foundation and compare the capabilities and characteristics of selected commercial and academic tools on concrete examples. The theoretical foundations deal on the one hand with the general ideas and principles of formal software verification, on the other hand present some internals of the selected tools to give a comprehensive understanding. The discussed tools are the Frege Program Prover, KeY, Perfect Developer, and the Prototype Verification System. The examples encompass simple standard computer science problems. The evaluation of these tools concentrates on the whole development process of specification and verification, not just on the verification results.";"Zusammenfassung Formale Spezifikation und Verifikation sind durch die durch kontinuierliche Weiterentwicklung in letzter Zeit an einem Punkt angelangt, wo Programme beinahe automatisch verifiziert werden können. Das Ziel dieser Magisterarbeit ist es, sowohl kommerzielle als auch für wissenschaftliche Zwecke entwickelte Verifikationsprogramme zu testen. Der Hauptaugenmerk liegt auf dem Nutzen dieser Werkzeuge in der Software-Entwicklung und in der Lehre. Hierzu wird diese Magisterarbeit die theo-retischen Grundlagen vorstellen und auf die verschiedenen Fähigkeiten und Eigenheiten der ausgewählten Werkzeuge eingehen. Die theoretischen Grundlagen behandeln einerseits Ansätze, die für die formale Verifikation gebraucht werden, andererseits wird die Funktionsweise der ausgewählten Werkzeuge erklärt. Die begutachteten Programme sind der Frege Program Prover, KeY, Perfect Developer und das Prototype Verification System. Die Beispiele, mit denen diese Werkzeuge getestet werden, sind typische Problemstellung der Informatik. Bei der Evaluation wird auf den ganzen Ablauf beim Einsatz dieser Werkzeuge eingegangen und nicht nur auf das Endergebnis. Abstract Formal specification and verification of software have made small but continuous advances throughout its long history, and have reached a point where commercial tools became available for verifying programs semi-automatically or automatically. The aim of the master thesis is to evaluate commercial and academic verification tools with respect to their usability in developing software and in teaching formal methods. The thesis will explain the theoretical foundation and compare the capabilities and characteristics of selected commercial and academic tools on concrete examples. The theoretical foundations deal on the one hand with the general ideas and principles of formal software verification, on the other hand present some internals of the selected tools to give a comprehensive understanding. The discussed tools are the Frege Program Prover, KeY, Perfect Developer , and the Prototype Verification System. The examples encompass simple standard computer science problems. The evaluation of these tools concentrates on the whole development process of specification and verification , not just on the verification results. Acknowledgements I would like to thank my family, especially my mother Inge, for supporting me. Gernot Salzer, my advisor, helped me whenever he could and invested a lot of time in discussing and investigating problems together with me. David Crocker gave excellent support on Perfect Developer, Andreas Roth and Steffen Schlager offered helpful instructions on KeY, Jürgen Winkler provided papers and references on FPP. Also the subscribers of the PVS mailing list came up with nice ideas. 1 Introduction Formal software verification has become a more and more important issue in developing security related software during the last decades. As a reaction, ISO-the International Organisation for Standardisation-issued the ISO 15408 Standard, defining exactly various quality levels for tested and verified software. This standard is represented in the Common Criteria Project, with members of security organisations around the globe. During the last years, formal specification and verification tools have been introduced, especially designed for standard development processes. The focus ranges from security related projects, over hardware circuit verification to software driver verification. In particular model checking has been very successful. Based on this evolution this thesis deals with four specification and verification tools that enable the user to build software complying with the most demanding restrictions of the ISO 15408 Standard. The aim is to construct software that meets the Evaluation Assurance Levels 6 and 7 (EAL 6, EAL 7) defined in the Common Criteria Project. In other words this means fully verified specification and code. For a long time users of these tools have been assumed to be experts in formal methods. With new target groups requirements changed. Therefore this thesis evaluates the tools with respect to two groups: software engineers with a good knowledge of computer science but without specific training in formal methods, and students of computer science and software engineering in the middle of their studies, being confronted with formal verification tools for the first time. The four tools that will be investigated are the Frege Program Prover, KeY, Perfect Developer, and the Prototype Verification System. In the first part of this work, the theoretical background-main calculi and ideas of formal verification-is presented. Then the internals of the tools are discussed , showing the different approaches and techniques from the theoretical side. Finally, by going through a set of simple standard computer science examples, the different characteristics and capabilities are presented in a practical form. By examining the tools from both sides, theory and practice, their usability in developing software and in teaching formal methods for the above defined target group is discussed.";"0,00";"0,00";"0,00"
"TUW-140867";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\groundtruth\TUW-140867.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\result\result-TUW-140867-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\extracted\grobid\grobid-TUW-140867-xstream.xml"")";"We present the status of formal methods at our university, and describe our course on formal software verification in more detail. We report our experiences in using Perfect Developer for the course assignments.";;"none extracted value";"0,00";"0,00"
"TUW-140895";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\groundtruth\TUW-140895.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\result\result-TUW-140895-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\extracted\grobid\grobid-TUW-140895-xstream.xml"")";"This master thesis presents an ant colony optimisation algorithm for the bounded diameter minimum spanning tree problem, a N P-hard combinatorial optimisation problem with various application fields, e.g. when considering certain aspects of quality in communication network design. The algorithm is extended with local optimisation in terms of a variable neighbourhood descent algorithm based on four different neighbourhood structures. These neighbourhood structures have been designed in a way to enable a fast identification of the best neighbouring solution. The proposed algorithm is empirically compared to various evolutionary algorithms and a variable neighbourhood search implementation on Euclidean instances based on complete graphs with up to 1000 nodes considering either solution quality as well as computation time. It turns out that the ant colony optimisation algorithm performs best among these heuristics with respect to quality of solution, but cannot reach the results of the variable neighbourhood search implementation concerning computation time.";"This master thesis presents an ant colony optimisation algorithm for the bounded diameter minimum spanning tree problem, a N P-hard combinatorial optimisation problem with various application fields, e.g. when considering certain aspects of quality in communication network design. The algorithm is extended with local optimisation in terms of a variable neighbourhood descent algorithm based on four different neighbourhood structures. These neighbourhood structures have been designed in a way to enable a fast identification of the best neighbouring solution. The proposed algorithm is empirically compared to various evolutionary algorithms and a variable neighbourhood search implementation on Euclidean instances based on complete graphs with up to 1000 nodes considering either solution quality as well as computation time. It turns out that the ant colony optimisation algorithm performs best among these heuristics with respect to quality of solution, but cannot reach the results of the variable neighbourhood search implementation concerning computation 1. INTRODUCTION The bounded diameter minimum spanning tree (BDMST) problem is a combinatorial op-timisation problem. Combinatorial optimisation problems belong to the group of optimi-sation problems, that in turn are divided into two groups. One, encoding solutions with real-valued variables and one, encoding solutions with discrete variables. Combinatorial op-timisation problems belong to the latter one. The definition of a combinatorial optimisation problem given here, follows that by Blum and Roli in [ also called search or solution space. To solve a combinatorial optimisation problem as defined above, a solution T ? S with either minimum objective value to be found. T is called a global optimal solution. Representative combinatorial optimisation problems are the travelling salesman problem, the quadratic assignment problem, timetabling and scheduling problems. After having introduced the definition of the combinatorial optimisation problem that of the bounded diameter minimum spanning tree problem can be given. m = |E| edges, where each edge e has associated costs c e ? 0, the bounded diameter minimum spanning tree problem is defined as the spanning minimum weight e?E T c e , where the diameter D is bounded above by a constant ? 2. The eccentricity of a node v, with v ? V , is defined as the maximum number of edges on a path from v to any other node in the minimum spanning tree T. The diameter bound D 1. Introduction 7 From this definition follows that the centre of T is either, in case of an even diameter bound, the single node or, in case of an odd diameter bound, the pair of adjacent nodes of minimum eccentricity. Thus the bounded diameter minimum spanning tree problem can also be interpreted as the search for a minimum spanning tree rooted at an unknown centre (having depth 0) and whose maximal depth is restricted to The BDMST problem is known to be N P-hard for 4 ? D ? n ? 1 [15]. Within the context of this master thesis, simply BDMST problem instances based on complete graphs are considered, since incomplete graphs can be anytime transformed into complete graphs, by setting the edge costs for not really available edges extremely high, so that these edges do not surface in solutions. The BDMST problem is not just of scientific interest, there are various real world applications , for example in communication network design. When considering certain aspects of quality of service, e.g. a maximum communication delay or minimum noise-ratio, BDMSTs become of great importance. A further practical application can be found in [5]: When planning a Linear Light-wave Network (LLN) an undirected multi-graph G = (V, E) is used, representing the network. This multi-graph G has to be decomposed into edge disjoint trees forming at least one spanning tree. Nevertheless, the aim of this decomposition process is to gain many spanning trees with a small diameter. Another application field of the BDMST problem is data compression. Bookstein et al. have introduced a way of transferring the problem of compressing correlated bit-vectors into the problem of finding a minimum spanning tree [7]. The decompression time of a given bitmap vector is proportional to its depth within the tree. Thus the whole running time depends on the height of the built tree. So a MST with a bounded diameter is preferable. The BDMST problem is also met in the field of mutual exclusion algorithms. When con";"0,00";"0,00";"0,00"
"TUW-140983";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\groundtruth\TUW-140983.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\result\result-TUW-140983-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\extracted\grobid\grobid-TUW-140983-xstream.xml"")";"In this paper we construct adaptive user profiles from tagging data. We present and evaluate an algorithm for creating such profiles, characterizing its behavior through statistical analysis.";"In this paper we construct adaptive user profiles from tagging data. We present and evaluate an algorithm for creating such profiles, characterizing its behavior through statistical analysis. Keywords tagging user profiles adaptivity 1. Introduction";"0,00";"0,00";"0,00"
"TUW-141024";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\groundtruth\TUW-141024.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\result\result-TUW-141024-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\extracted\grobid\grobid-TUW-141024-xstream.xml"")";"Clinical practice guidelines are widely used to support medical staff in treatment planning and decision-making, whereas, the classification of different recommendations in the CPGs are one of the most important information sources to use. However, there is a lack of consensus amongst guideline developers, regarding those classification schemes. To address this problem, we mapped the different graded and ungraded evidence information used by different guideline developing organizations into our meta schema. In this paper we describe how guideline representation languages, such as Asbru and PROforma can be extended to model our meta schema.";"Clinical practice guidelines are widely used to support medical staff in treatment planning and decision-making, whereas, the classification of different recommendations in the CPGs are one of the most important information sources to use. However, there is a lack of consensus amongst guideline developers, regarding those classification schemes. To address this problem, we mapped the different graded and ungraded evidence information used by different guideline developing organizations into our meta schema. In this paper we describe how guideline representation languages, such as Asbru and PROforma can be extended to model our meta schema. 1. Introduction Clinical practice guidelines (CPGs) are one of the central topics of research in Artifical Intelligence in medicine. They can be described as ""systematically developed statements to assist practitioner and patient decisions about appropriate health care for specific clinical circumstances"" [3]. The major goal of CPGs is to support physicians in their daily work providing information needed for the decision-making process for a particular patient and disease. We can say that recommendations described in CPGs are one of the most important information sources to use during decision-making, because they provide phycisians various treatment options. Recommendations are, in general, based on some kind of evidence, represented by different levels of evidence (LoEs), and on strengths of recommendations (SoRs). Several definitions of LoEs and SoRs exist by now. In the context of our work, the following definitions seem appropriate [2]: Levels of evidence (LoEs): The validity of an individual study is based on an assessment of its study type. According to some methodologies, LoEs can refer not only to individual studies but also to the quality of evidence from multiple studies about a specific question or the quality of evidence supporting a clinical intervention. Strengths of recommendation (SoRs): The SoRs for clinical practice is based on a body of evidence. This approach takes into account the LoEs of individual studies, the type of outcomes measured by these studies, the number, consistency, and coherence of the evidence as a whole, the relationship between benefits, harms, and costs. However, in addition to such graded recommendations, there also exist ungraded recommendations , where the guidelines do not contain any classification of the LoEs or SoRs. They usually appear in guidelines as ordinary text fragments. This circumstance makes the classification";"0,00";"0,00";"0,00"
"TUW-141065";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\groundtruth\TUW-141065.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\result\result-TUW-141065-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\extracted\grobid\grobid-TUW-141065-xstream.xml"")";"The need to provide more holistic adaptivity to students has brought us to investigate the relationship between learning styles and working memory capacity (WMC). The aim of this investigation is to study the relationship between learning styles and WMC in order to get additional information about the students. This information can be used to make more holistic adaptivity possible by improving the student modelling process of both learning styles and WMC. An experiment with 297 participants was conducted. Findings suggest that relationships from WMC to the active/reflective, the sensing/intuitive, and the visual/verbal learning styles exist, whereas the suggested relationship from WMC to sequential/global learning styles could not be found.";"The need to provide more holistic adaptivity to students has brought us to investigate the relationship between learning styles and working memory capacity (WMC). The aim of this investigation is to study the relationship between learning styles and WMC in order to get additional information about the students. This information can be used to make more holistic adaptivity possible by improving the student modelling process of both learning styles and WMC. An experiment with 297 participants was conducted. Findings suggest that relationships from WMC to the active/reflective, the sensing/intuitive, and the visual/verbal learning styles exist, whereas the suggested relationship from WMC to sequential/global learning styles could not be found. 1. Introduction Providing courses that fit the needs of learners makes learning easier for students. In recent years, researchers focused on the impact of individual characteristics such as learning styles and cognitive traits in technology enhanced learning and how such differences can be incorporated in order to provide adaptive courses. To be able to provide proper adaptivity, the needs of students have to be known first. With respect to technology enhanced learning, Brusilovsky [1] distinguished between two different ways for getting information about the learners' needs: the collaborative and the automatic student modelling approach. In the former, the students explicitly provide information about themselves (e.g. filling out a questionnaire), whereas in the latter, the system infers the needs from the behaviour and actions of the students automatically while they are working/learning in the system. The automatic approach is direct and is free from the problem of inaccurate self-conceptions of students. However, a problem with the automatic approach is to get enough reliable information to build a robust student model. As a solution, Brusilovsky [1] recommended the use of additional sources of information. Hence, it is beneficial to find mechanisms that use whatever information about the learner is already available to get as much reliable information to build a more robust student model. In this paper, we investigate the relationship between learning styles, in particular the Felder-Silverman learning style model (FSLSM) [2], and working memory capacity (WMC). WMC is one of the cognitive traits included in the model to profile students' cognitive traits using student behaviour. The found relationship can then be used to strengthen the inference procedure of student models such as the CTM by including additional information from learning styles. Similarly, the findings can also be used to strength a learning style model which is built not from a questionnaire but from student behaviour observations.";"0,00";"0,00";"0,00"
"TUW-141121";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\groundtruth\TUW-141121.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\result\result-TUW-141121-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\extracted\grobid\grobid-TUW-141121-xstream.xml"")";"Multimedia content can be described in versatile ways as its essence is not limited to one view. For music data these multiple views could be a song's audio features as well as its lyrics. Both of these modalities have their advantages as text may be easier to search in and could cover more of the 'content semantics' of a song, while omitting other types of semantic categorisation. (Psycho)acoustic feature sets, on the other hand, provide the means to identify tracks that 'sound similar' while less supporting other kinds of semantic categorisation. Those discerning characteristics of different feature sets meet users' differing information needs. We will explain the nature of text and audio feature sets which describe the same audio tracks. Moreover, we will propose the use of textual data on top of low level audio features for music genre classification. Further, we will show the impact of different combinations of audio features and textual features based on content words.";"Multimedia content can be described in versatile ways as its essence is not limited to one view. For music data these multiple views could be a song's audio features as well as its lyrics. Both of these modalities have their advantages as text may be easier to search in and could cover more of the 'content semantics' of a song, while omitting other types of semantic categorisation. (Psycho)acoustic feature sets, on the other hand, provide the means to identify tracks that 'sound simi-lar' while less supporting other kinds of semantic categorisation. Those discerning characteristics of different feature sets meet users' differing information needs. We will explain the nature of text and audio feature sets which describe the same audio tracks. Moreover, we will propose the use of textual data on top of low level audio features for music genre classification. Further, we will show the impact of different combinations of audio features and textual features based on content words.";"100,00";"100,00";"100,00"
"TUW-141140";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\groundtruth\TUW-141140.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\result\result-TUW-141140-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\extracted\grobid\grobid-TUW-141140-xstream.xml"")";"Current model-driven Web Engineering approaches (such as OO-H, UWE or WebML) provide a set of methods and supporting tools for a systematic design and development of Web applications. Each method addresses different concerns using separate models (content, navigation, presentation, business logic, etc.), and provide model compilers that produce most of the logic and Web pages of the application from these models. However, these proposals also have some limitations, especially for exchanging models or representing further modeling concerns, such as architectural styles, technology independence , or distribution. A possible solution to these issues is provided by making model-driven Web Engineering proposals interoperate, being able to complement each other, and to exchange models between the different tools. MDWEnet is a recent initiative started by a small group of researchers working on model-driven Web Engineering (MDWE). Its goal is to improve current practices and tools for the model-driven development of Web applications for better interoperability. The proposal is based on the strengths of current model-driven Web Engineering methods, and the existing experience and knowledge in the field. This paper presents the background, motivation, scope, and objectives of MDWEnet. Furthermore, it reports on the MDWEnet results and achievements so far, and its future plan of actions.";;"none extracted value";"0,00";"0,00"
"TUW-141336";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\groundtruth\TUW-141336.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\result\result-TUW-141336-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\extracted\grobid\grobid-TUW-141336-xstream.xml"")";"The QXOR-SAT problem is the quantified version of the satisfiability problem XOR-SAT in which the connective exclusive-or is used instead of the usual or. We study the phase transition associated with random QXOR-SAT instances. We give a description of this phase transition in the case of one alternation of quantifiers, thus performing an advanced practical and theoretical study on the phase transition of a quantified problem.";"The QXOR-SAT problem is the quantified version of the satisfiability problem XOR-SAT in which the connective exclusive-or is used instead of the usual or. We study the phase transition associated with random QXOR-SAT instances. We give a description of this phase transition in the case of one alternation of quantifiers, thus performing an advanced practical and theoretical study on the phase transition of a quantified problem. 1. Introduction The last decade has seen a growth of interest in the phase transition for Boolean satisfiability (SAT). The clausal version of this problem shows a sharp transition in the sense that, as the density of clauses is increased, formulas abruptly change from being satisfiable to being unsatisfiable at a critical threshold point. Numerous experimental studies have been performed in the investigation of phase transition for different variants of SAT problems, thus giving strong evidence that the location of the transition coincides with such instances that are hard to solve. In the meantime, theoretical studies have been conducted in order to better understand such transitions. Determining the nature of the phase transition (sharp or coarse) 1 , locating it, determining a precise scaling window and better understanding the structure of the space of solutions turn out to be very challenging tasks, which have aroused a lot of interest in different communities, namely mathematics, computer science and statistical physics (see e.g., Dubois, Monasson, Selman, & Zecchina, 2001). From a computer science point of view, the success of the SAT problem is due to two features. The problem SAT provides a framework in which problems within the complexity class NP can 1. Definitions of sharp and coarse phase transitions can be found on p. 21 in (Janson, Luczak, & Rucinski, 2000). Creignou, Daudé, & Egly be adequately expressed, and moreover practically efficient and highly optimized solvers are available. In order to obtain even stronger systems, many researchers have turned to a powerful generalization of Boolean satisfiability, QSAT, where both universal and existential quan-tifiers over Boolean variables are permitted. The QSAT problem permits the adequate representation and modeling of problems having higher complexity-within the complexity class PSPACE-and coming from various fields of computer science such as knowledge representation , verification and logic. Recently, random instances of these quantified problems have started to attract some attention (see Gent & Walsh, 1999; Chen & Interian, 2005). Models for generating random instances have been developed, and experimental studies have shown that in these models the QSAT property undergoes a phase transition that is qualitatively similar to the one that appears for the ordinary SAT property. As stated by Chen and Interian (2005), the hope is that research on developing competitive solvers for quantified Boolean formulas could benefit from a better understanding of the typical behavior of random instances. Our study follows the pioneering work from Chen and Interian (2005) who have made precise a promising model for generating random instances of the QSAT problem. We use their model and apply it to the satisfiability problem QXOR-SAT that we present below. The difficulty of identifying transition factors and of performing theoretical explorations of the SAT transition has incited many researchers to turn to a variant of the SAT problem: the e-XOR-SAT problem. This satisfiability problem deals with Boolean formulas in con-junctive normal form with e variables per clause, in which the ""usual or"" is replaced by the ""exclusive or"" (we call clauses with ""exclusive or"" as the only connective XOR-clauses). This problem has contributed to develop or validate techniques, thus revealing typical behaviors of both random instances and their space of solutions for SAT-type problems (see, e.g., in order to understand how phase transitions evolve for satisfiability when introducing quantified variables, it is quite natural to consider this problem.";"0,00";"0,00";"0,00"
"TUW-141618";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\groundtruth\TUW-141618.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\result\result-TUW-141618-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\extracted\grobid\grobid-TUW-141618-xstream.xml"")";"On-line analytical processing (OLAP) is a powerful data analysis technology. However, current implementations bypass the 3-layer model of databases for performance reasons and create a redundant database
	
The clear separation between external views, conceptional model and internal physical representations in the three layer database architecture constitutes one of the key factors to the success of database technology for on-line transaction processing (OLTP). However, experience has shown that it is not feasible to construct an on-line analytical processing (OLAP) view onto the data within this model. As a compromise, materialized views were introduced. Yet, these limit the applicability of OLAP to fairly standard applications. Some application domains such as the analysis of stock market data or telecommunication systems with a diverse structure and unpredictable aggregation and query patterns can not be timely handled using pre-materialization. As a result, the benefits of OLAP are not available when building analysis and visualization tool for this kind of applications.

We envision OLAP-on-demand as a foundation for the re-unification of the transaction processing and analysis in databases. Instead of pre-materializing expected queries, analysts can integrate the available data sources on the fly where the analysis of information is not delayed by certain``certain``update windows''. Although current computing hardware does not provide the necessary data throughput, we expect future generation systems to cope with the high demands of this approach. In the meantime, parallel and distributed query processing can be used to achieve the required response times.

As an ultimate goal the realisation of Codd's formulaic model is envisioned.";"On-line analytical processing (OLAP) is a powerful data analysis technology. However, current implementations bypass the 3-layer model of databases for performance reasons and create a redundant database The clear separation between external views, conceptional model and internal physical representations in the three layer database architecture constitutes one of the key factors to the success of database technology for on-line transaction processing (OLTP). However, experience has shown that it is not feasible to construct an on-line analytical processing (OLAP) view onto the data within this model. As a compromise, materialized views were introduced. Yet, these limit the applicability of OLAP to fairly standard applications. Some application domains such as the analysis of stock market data or telecommunication systems with a diverse structure and unpredictable aggregation and query patterns can not be timely handled using pre-materialization. As a result, the benefits of OLAP are not available when building analysis and visualization tool for this kind of applications. We envision OLAP-on-demand as a foundation for the re-unification of the transaction processing and analysis in databases. Instead of pre-materializing expected queries, analysts can integrate the available data sources on the fly where the analysis of information is not delayed by certain``certain``update windows''. Although current computing hardware does not provide the necessary data throughput, we expect future generation systems to cope with the high demands of this approach. In the meantime, parallel and distributed query processing can be used to achieve the required response times. As an ultimate goal the realisation of Codd's formulaic model is envisioned. On-line analytical processing (OLAP) is a powerful data analysis technology. However, current implementations bypass the 3-layer model of databases for performance reasons and create a redundant database. In an extreme sense, we may consider OLAP in its current form to be harmful, as it increases the complexity of implementing and maintaining data analysis solutions, although it should simplify this. However, we expect future improvements to the implementation techniques and the increased performance of future generation computing hardware to reconcile the transaction processing and analysis components in databases. Instead of pre-materializing expected queries, analysts will integrate the available data sources on the fly such that the result of their analysis is not based on outdated information. Although current computing hardware does not provide the necessary data throughput, we expect future generation systems to cope with the high demands of such implementations. In the meantime, parallel and distributed query processing can be used to achieve the required response times. This will eventually turn OLAP into a paradigm for the formulation of all data manipulation and data analysis task. With the addition of a rule-language, even real-time control application could be expressed in the terms of multi-dimensional data transformation operators. Section 2 describes the basic principles of OLAP. Section 3 discusses in some detail how the current implementations of OLAP systems contradict the principles of database systems. OLAP on-demand, the expected reconciliation of data management and analysis is put forth.";"0,00";"0,00";"0,00"
"TUW-141758";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\groundtruth\TUW-141758.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\result\result-TUW-141758-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\extracted\grobid\grobid-TUW-141758-xstream.xml"")";"Medical information is often stored in a narrative way, which makes the automated processing a difficult and time-consuming task. Persons responsible for the authoring of medical documents do not take care of a further processing with automated systems. So, information stored in medical writings is not directly usable for the processing with computers. Due to this, efforts have been made to transfer these narrative documents in a format easier processable with computers. This matter of fact also applies to clinical practice guidelines (CPGs). As many medical documents , CPGs are written in a narrative speech as well, without regards to a computer-assisted processing. For the implementation of CPGs in medical facilities an automated processing is therefore desirable. An important fact is that a lot of information in CPGs is provided in a negated form, expressing that certain circumstances in patients or treatments are not available, existing or applicable. Although negated, this information is nevertheless very useful, since it can express the absence of certain conditions or diseases in patients. Moreover, negations can describe which treatment options should not be taken into account for a given patient, helping a practising physician or nurse in his/her decision process for the assortment of a proper treatment. Thus, a proper Negation Detection in CPGs is an important task for the automated processing of this type of medical documents. It helps to accelerate the decision making process and can support medical staff in their care for patients. We developed algorithms capable of Negation Detection in CPGs. We use syntactical methods provided by the English language to achieve a precise detection of occuring negations. According to our results we are convinced that the involvement of syntactical methods can improve Negation Detection, not only in medical writings but also in arbitrary narrative texts.";"Medical information is often stored in a narrative way, which makes the automated processing a difficult and time-consuming task. Persons responsible for the authoring of medical documents do not take care of a further processing with automated systems. So, information stored in medical writings is not directly usable for the processing with computers. Due to this, efforts have been made to transfer these narrative documents in a format easier processable with computers. This matter of fact also applies to clinical practice guidelines (CPGs). As many medical documents , CPGs are written in a narrative speech as well, without regards to a computer-assisted processing. For the implementation of CPGs in medical facilities an automated processing is therefore desirable. An important fact is that a lot of information in CPGs is provided in a negated form, expressing that certain circumstances in patients or treatments are not available, existing or applicable. Although negated, this information is nevertheless very useful, since it can express the absence of certain conditions or diseases in patients. Moreover, negations can describe which treatment options should not be taken into account for a given patient, helping a practising physician or nurse in his/her decision process for the assortment of a proper treatment. Thus, a proper Negation Detection in CPGs is an important task for the automated processing of this type of medical documents. It helps to accelerate the decision making process and can support medical staff in their care for patients. We developed algorithms capable of Negation Detection in CPGs. We use syntactical methods provided by the English language to achieve a precise detection of occuring negations. According to our results we are convinced that the involvement of syntactical methods can improve Negation Detection, not only in medical writings but also in arbitrary narrative texts. Kurzfassung Medizinische Daten werden in der Regel in natürlicher Sprache abgespeichert. Dies liegt daran, dass medizinisches Personal im Normalfall keine Rücksicht auf eine spätere Verarbeitung in au-tomatisierten Systemen nimmt. Die Informationen sind daher nicht sofort für eine Verarbeitung mit Computern verfügbar. Aus diesem Grund wurden vielfach Anstrengungen unternommen, um die Daten in ein für Computer verarbeitbares FormatüberzuführenFormat¨Formatüberzuführen. Dieser Umstand gilt auch für medizinische Leitlinien (sogenannte ""Clinical Practice Guide-lines"" (CPGs)). Sie sind, wie alle medizinischen Dokumente, ebenfalls in natürlicher Sprache verfaßt, ohne Rücksichtnahme auf eine computergestützte Verarbeitbarkeit. Die Einführung von medizinischen Leitlinien im klinischen Umfeld macht allerdings eine automatisierte Verarbei-tung wünschenswert. In diesem Zusammenhang muß weiters beachtet werden, dass ein großer Teil der Information in medizinischen Leitlinien in negierter Form vorhanden ist. Die Tatsache, dass diese negiert vorliegt, macht sie aber deswegen nicht irrelevant oder weniger beachtens-wert. Im Gegenteil, oftmals werden auf diese Art und Weise ungeeignete Behandlungsverfahren gekennzeichnet, beziehungsweise Behandlungen, die bei gegebenen physiologischen Zuständen nicht durchgeführt werden sollen oder dürfen. Solche Informationen können den Behandelnden helfen, die richtigen Therapieverfahren anzuwenden oder auch gewisse Verfahren abzulehnen. Dementsprechend ist die Erfassung von Negationen (die sogenannte ""Negation Detection"") in medizinischen Leitlinien ein essentieller Bestandteil für die automatisierte Weiterverarbeitung dieser Art von medizinischen Dokumenten. Entscheidungsprozesse werden somit beschleunigt und dies wirkt sich positiv auf die Behandlung der Patienten aus. Um diese Aufgabe zu lösen haben wir Algorithmen entwickelt, die das Auffinden von Nega-tionen in medizinischen Leitlinien ermöglichen sollen. Dazu benutzen wir syntaktische Verfahren, die die englische Grammatik gezielt ausnützen, um aufgetretene Negationen exakt aufzuspüren. Unsere gewonnenen Resultate zeigen, dass die Verwendung syntaktischer Methoden dabei hilft, die Identifikation von Negationen zu verbessern, wobei unsere Methoden nicht nur auf medizi-nische Dokumente, sondern auf jede Art von natürlichsprachlichem Text anwendbar sind.";"0,00";"0,00";"0,00"
"TUW-168222";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\groundtruth\TUW-168222.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\result\result-TUW-168222-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\extracted\grobid\grobid-TUW-168222-xstream.xml"")";"Recent research results on communication science mention a relation between the inter-locutor's perceptional preferences, his or her mental representations and his or her choice of expressions. However, there is not any tool for analyzing such. In my research I combined lexical with computational linguistic methods to develop a software prototype that is able to analyze text on the usage of perceptional expressions. This analyzing tool can help to identify the interlocutor's perceptional preference for easier meeting his or her way of thinking and thereby facilitate the understanding.";"Recent research results on communication science mention a relation between the inter-locutor's perceptional preferences, his or her mental representations and his or her choice of expressions. However, there is not any tool for analyzing such. In my research I combined lexical with computational linguistic methods to develop a software prototype that is able to analyze text on the usage of perceptional expressions. This analyzing tool can help to identify the interlocutor's perceptional preference for easier meeting his or her way of thinking and thereby facilitate the understanding. Still, for the German language, not much research";"100,00";"100,00";"100,00"
"TUW-168482";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\groundtruth\TUW-168482.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\result\result-TUW-168482-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\extracted\grobid\grobid-TUW-168482-xstream.xml"")";"Formal dialogue games are a traditional approach to characterize the semantics of logics. In the 1970s Robin Giles attempted to provide an operational foundation for formal reasoning in physical theories by dialogue games based on atomic experiments that may show dispersion. This thesis motivates, describes and analyzes his approach and the connection to t-norm based fuzzy logics. We give a short introduction into t-norms and many-valued logics based on t-norms. In particular we focus on three fundamental t-norm based fuzzy logics: Lukasiewicz Logic, Gödel Logic, and Product Logic. We present and discuss several approaches for extending the game rules of Giles's Game in order to make it adequate for Gödel Logic and Product Logic. Moreover, we give hints at a strong correspondence between winning strategies in the game and derivations in an analytic proof system based on relational hypersequents. Another type of dialogue games are truth comparison games. This type is suitable for Gödel Logic and relates more to the degree based semantics of that logic than Giles's Game. We present the game and discuss winning strategies for both players indicating the validity or refutability of a formula. Additionally, several utilities implemented in the context of this thesis are presented. Amongst these is a web-based application which allows for the interactive exploration of Giles's Game and its extensions.";"Formal dialogue games are a traditional approach to characterize the semantics of logics. In the 1970s Robin Giles attempted to provide an operational foundation for formal reasoning in physical theories by dialogue games based on atomic experiments that may show dispersion. This thesis motivates, describes and analyzes his approach and the connection to t-norm based fuzzy logics. We give a short introduction into t-norms and many-valued logics based on t-norms. In particular we focus on three fundamental t-norm based fuzzy logics: Lukasiewicz Logic, Gödel Logic, and Product Logic. We present and discuss several approaches for extending the game rules of Giles's Game in order to make it adequate for Gödel Logic and Product Logic. Moreover, we give hints at a strong correspondence between winning strategies in the game and derivations in an analytic proof system based on relational hypersequents. Another type of dialogue games are truth comparison games. This type is suitable for Gödel Logic and relates more to the degree based semantics of that logic than Giles's Game. We present the game and discuss winning strategies for both players indicating the validity or refutability of a formula. Additionally, several utilities implemented in the context of this thesis are presented. Amongst these is a web-based application which allows for the interactive exploration of Giles's Game and its extensions. ""Everything is vague to a degree you do not realize till you have tried to make it precise."" Bertrand Russell CHAPTER 1 Introduction Vagueness is a ubiquitous and pervasive phenomenon in information processing. Modelling vagueness is often accomplished by assigning degrees of truth to propositions. Fuzzy logics, taken here in Zadeh's ""narrow sense"" [Zad96], are based on the extension of the two classical truth values by infinitely many intermediary degrees of truth. Degrees of truth should strictly be distinguished from degrees of belief, and therefore require methods different from probability theory. In particular, in fuzzy logics the semantics of the logical connectives is required to be truth functional. In the 1970s Robin Giles attempted to provide an operational foundation for formal reasoning in physical theories based on atomic experiments that may show dispersion. A formula is interpreted by a strategic game, where two players bet on the combined results of atomic experiments, that may show different outcomes when repeated. The degree of truth is then related to the expected loss of the player initially asserting the formula. Giles proved that the propositions that a player can bet on without expecting loss coincide with those that are valid in Lukasiewicz Logic, one of three fundamental, so-called t-norm based fuzzy logics. Giles's remarkable result can, with hindsight, be seen as one of the few attempts to solve a fundamental problem in approximate reasoning: how to derive a fuzzy logic from first principles, in this case from dialogue rules combined with a betting scheme. 1. Introduction In Chapter 2 we motivate fuzzy logics based on so-called t-norms. We present the three fundamental t-norm based fuzzy logics Logic G. An analytic proof system for these three logics based on relational hypersequents is described which is of special interest in the context of Giles's Game. Chapter 3 presents Giles's Game itself. Giles's approach of reasoning in physical theories by assigning a ""tangible meaning"" to propositions is motivated and discussed. The dialogue game rules for subsequently decomposing compound propositions as well as an adequate betting scheme for atomic propositions are given. This chapter also includes a version of Giles's proof which relates the game to Lukasiewicz Logic. Moreover, we provide hints at an extension of Giles's Game adequate for first order Lukasiewicz Logic. Chapter 4 shows that variants of Giles's Game are adequate for Product Logic and Gödel Logic. This is accomplished in the first place by changing the underlying betting scheme. We also have to modify the game rules for decomposing implications. Several ways of doing so are presented. Chapter 5 presents so-called truth comparison games. This type of game is a dialogue game adequate for Gödel Logic G which, in contrast to L and ?, only requires the comparison of different degrees of truth and no arithmetical operations on these. We also show how to model strategies for both players in this framework. These strategies can be used for characterizing validity and refutability of formulas in Gödel Logic. In the context of this thesis four small tools have been implemented. The applications and their usage are subject of Chapter 6: Webgame: A web-based application which enables the interactive exploration of Giles's Game and its variants for Product Logic and Gödel Logic. After playing the game and having reached a final game state, one can simulate the evaluation of atomic propositions based on (dispersive) experiments using the respective betting scheme. Giles: A utility which creates and visualizes game trees of Giles's Game. Most illustrations of game trees in this thesis are generated using giles. Hypseq: A tool which constructs and displays derivations of hypersequents in the hyperse-quent calculus rH presented in Section 2.6. TCGame: A tool which searches for winning strategies for the proponent in the truth comparison game presented in Chapter 5. Such a strategy can be displayed as a tree and can be seen as a proof of the initial proposition of the game. ""There is nothing worse than a sharp image of a fuzzy concept."" In this chapter we concretize the notion of a fuzzy logic by choosing so called t-norms as a starting point for defining a logic. We then describe three important t-norms and corresponding logics in more detail, which are of special relevance when dealing with dialogue games later on. This chapter also includes a brief description of a proof system using relational hypersequents and uniform rules for those three logics. 2.1 Design Choices Fuzzy logics are commonly understood as many valued logics that allow to model reasoning in presence of different degrees of truth. In order to achieve this, we follow Petr Hájek ([Há02] and [Háj02]) in making the following fundamental ""design choices"": 1. The real unit interval [0, 1] is taken as our set of truth values, 0 meaning absolute falsity, 1 standing for absolute truth. The truth values are linearly ordered using the usual ordering An important implication of this choice is that we always can compare the truth values of two propositions. For example, we can say, the proposition ""I am tall"" is true to a higher degree than the proposition ""the weather is good today"". 2.1. 2. The truth value of a formula B) shall solely be determined by the truth values of the formulas A and B. This notion is called truth functionality and is formalized as follows: Each binary connective shall have a truth function determining for any pair of formulas the truth degree of the compound formula and similarly for unary connectives. This requirement, for example, hinders us to simply regard truth values as probabilities of arbitrary events; the resulting logic would not be truth functional: The probability value of the proposition ""A and B"" then is not composable into the truth values of A and B alone, but depends on how these events are related to each other. In general one cannot assume that arbitrary experiments are independent of each other. The same applies to propositions of the form ""A or B"" as the probability P(A or B) can be calculated are approaches to probabilistic reasoning in a logical framework; see e.g. [HGE95]. 3. When choosing truth functions for connectives each fuzzy logic shall be a generalization of classical logic. So, for the truth values 0 and 1 the truth functions must behave classically. For example, if we call a connective ""?"" implication, its truth function tautologies (formulas that always evaluate to the truth value 1 regardless of the truth values of their atoms) in our fuzzy logic are also classical";"0,00";"0,00";"0,00"
"TUW-169511";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\groundtruth\TUW-169511.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\result\result-TUW-169511-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\extracted\grobid\grobid-TUW-169511-xstream.xml"")";;"Many emergency service providers, especially ambulance departments and companies who provide non-public maintenance services, face the problem that their fleet of vehicles has to provide two different types of services: 1. Cover a certain region and provide immediate service when an emergency occurs; 2. Provide some regular service (e.g., the pickup and delivery of patients , predetermined service tasks, periodic pickups . . .). This is the current situation for the largest Austrian emergency service providers (e.g., the Austrian Red Cross), where the same fleet is used to provide both types of services. Dynamic aspects thus directly influence the schedule for the regular service. When an emergency occurs and an ambulance is required, the vehicle with the shortest distance to the emergency is assigned to serve the emergency patient. Therefore , it may happen that an ambulance vehicle that has to carry out a scheduled transport order of a patient, which has not started yet, is used to serve the emergency request and the schedule for the regular services has to be re-optimized and another vehicle has to be reassigned";"0,00";"none expected";"0,00"
"TUW-172697";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\groundtruth\TUW-172697.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\result\result-TUW-172697-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\extracted\grobid\grobid-TUW-172697-xstream.xml"")";"Creating a concrete plan for preserving an institution's collection of digital objects requires the evaluation of available solutions against clearly defined and measurable criteria. Preservation planning aids in this decision making process to find the best preservation strategy considering the institution's requirements, the planning context and possible actions applicable to the objects contained in the repository. Performed manually, this evaluation of possible solutions against requirements takes a good deal of time and effort. In this demonstration, we present Plato, an interactive software tool aimed at creating preservation plans.";"Creating a concrete plan for preserving an institution's collection of digital objects requires the evaluation of available solutions against clearly defined and measurable criteria. Preservation planning aids in this decision making process to find the best preservation strategy considering the institution's requirements, the planning context and possible actions applicable to the objects contained in the repository. Performed manually, this evaluation of possible solutions against requirements takes a good deal of time and effort. In this demonstration, we present Plato, an interactive software tool aimed at creating preservation plans.";"100,00";"100,00";"100,00"
"TUW-174216";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\groundtruth\TUW-174216.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\result\result-TUW-174216-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\extracted\grobid\grobid-TUW-174216-xstream.xml"")";"During the last decades, digital objects have become the primary medium to create, shape, and exchange information. However, in contrast to analog objects such as books that directly represent their content, digital objects are not usable without a corresponding technical environment. The fast changes in these environments and in formats and technologies mean that digital documents have a short lifespan before they become obsolete. Digital preservation, i.e. actions to ensure longevity of digital information, thus has become a pressing challenge. The dominant strategies prevailing today are migration and emulation; for each strategy, different tools are available. When converting an object to a different representation, a validation of the content is needed to verify that the transformed objects are still authentically representing the same intellectual content. This validation so far is largely done manually, which is infeasible for large collections.
Preservation planning supports decision makers in reaching accountable decisions by evaluating potential strategies against well-defined requirements. Especially the evaluation of different migration tools for digital preservation has to rely on validating the converted objects and thus on an analysis of the logical structure and the content of documents. Existing approaches for characterising and describing objects do not attempt to fully extract the informational content of digital objects and thus are not suffficient for an in-depth validation of transformed content.
This paper describes the eXtensible Characterisation Languages (XCL) that support the automatic validation of document conversions and the evaluation of migration quality by hierarchically decomposing a document and representing documents from different sources in an abstract XML language. The description language XCDL provides an abstract representation of digital content in XML, while the extraction language XCEL allows an extraction engine to create such an abstract description by mapping file format structures to XCDL concepts. We present the context of the development of these languages and tools and describe the overall concept and features of the languages. We further give examples and show how the languages can be applied to the evaluation of digital preservation solutions in the context of preservation planning.";"During the last decades, digital objects have become the primary medium to create, shape, and exchange information. However, in contrast to analog objects such as books that directly represent their content, digital objects are not usable without a corresponding technical environment. The fast changes in these environments and in formats and technologies mean that digital documents have a short lifespan before they become obsolete. Digital preservation, i.e. actions to ensure longevity of digital information, thus has become a pressing challenge. The dominant strategies prevailing today are migration and emulation; for each strategy, different tools are available. When converting an object to a different representation, a validation of the content is needed to verify that the transformed objects are still authentically representing the same intellectual content. This validation so far is largely done manually, which is infeasible for large collections. Preservation planning supports decision makers in reaching accountable decisions by evaluating potential strategies against well-defined requirements. Especially the evaluation of different migration tools for digital preservation has to rely on validating the converted objects and thus on an analysis of the logical structure and the content of documents. Existing approaches for characterising and describing objects do not attempt to fully extract the informational content of digital objects and thus are not suffficient for an in-depth validation of transformed content. This paper describes the eXtensible Characterisation Languages (XCL) that support the automatic validation of document conversions and the evaluation of migration quality by hierarchically decomposing a document and representing documents from different sources in an abstract XML language. The description language XCDL provides an abstract representation of digital content in XML, while the extraction language XCEL allows an extraction engine to create such an abstract description by mapping file format structures to XCDL concepts. We present the context of the development of these languages and tools and describe the overall concept and features of the languages. We further give examples and show how the languages can be applied to the evaluation of digital preservation solutions in the context of preservation planning. The last decades have made digital objects the primary medium to create, shape, and exchange information. An increasing part of our cultural and scientific heritage is being created and maintained in digital form; digital content is at the heart of today's economy, and its ubiquity is increasingly shaping private lives. The ever-growing complexity and heterogeneity of digital file formats together with rapid changes in underlying technologies have posed extreme challenges to the longevity of information. So far, digital objects are inherently ephemeral. Memory institutions such as national libraries and archives were amongst the first to approach the problem of ensuring long-term access to digital objects when the original software or hardware to interpret them correctly becomes unavailable [UNESCO, 2003]. Digital preservation denotes the efforts to preserve digital objects for a given purpose over long periods of time. The urgency of digital preservation has recently been reemphasised by the results of a survey among archiving professionals [The 100 Year Archive Task Force, 2007]. In the last years, numerous research initiatives have started around the world that aim at mastering this challenge. The two strategies generally considered to prevail today are migration[Testbed, 2001; Mellor et al., 2002] and emulation[Rothenberg, 1999; van der Hoeven and van Wijngaarden, 2005]. Migration, the conversion of a digital object to another representation, is the most widely applied solution for standard object types such as electronic documents or images. The critical problem generally is how to ensure consistency and authenticity and preserve all the essential features and the conceptual characteristics of the original object whilst transforming its logical representation. Lawrence et. al. presented different kinds of risks for a migration project [Lawrence et al., 2000]. While migration operates on the objects and transforms them to more stable or more widely adopted representations, em-ulation operates on the environment of an object, trying to simulate the original environment that the object needs. Consider a large collection of documents written in an old version of Word several years ago. This application does not run on modern operating systems. One could try to emulate the original operating system; but the emulation software still depends on current hardware, and emulating the hardware might be even harder. On the other hand, one could also migrate the documents to a current version of Word, or to PDF. While this would lose the original look-and-feel of the application, it would probably preserve the content and layout. With PDF, one would exchange the tool support provided by text processing software, as well as metadata such as an edit history, for a file format that is generally considered to be quite stable. An important part of ongoing efforts in many large international projects is";"0,00";"0,00";"0,00"
"TUW-175428";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\groundtruth\TUW-175428.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\result\result-TUW-175428-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\extracted\grobid\grobid-TUW-175428-xstream.xml"")";"Within this thesis a real-world problem related to a warehouse for spare parts is considered. Regarding several constraints typically stated by spare parts suppliers the time needed to collect articles should be minimized. Based on continuously arriving orders by customers predefined delivery times and capacity constraints have to be met. To accomplish this task efficient pickup tours need to be determined which is the main issue covered by this work which comes to an end with experimental results of a concrete implementation of the proposed approach.
The algorithm presented embeds a specifically developed dynamic program for computing optimal walks through the warehouse into a general variable neighborhood search (VNS) scheme. Several stages are used for first splitting up all orders, then creating tours out of the results and finally assigning them to available workers. The VNS uses a variant of the variable neighborhood descent (VND) as local improvement procedure. While the neighborhood structures defined are intended to produce candidate solutions, a dynamic program specially designed to compute optimal order picking tours is used to evaluate them. For this purpose properties specific to warehouses are exploited such to compute optimal routes within polynomial time. The final assignment of workers to tours is realized by another VNS. The task is then to find an allocation such that the last article to be picked up will be collected as early as possible.
Evaluations of experimental results of a concrete implementation indicate that the presented approach provides valuable pickup plans and computation times can be kept low. Moreover the performed test runs have been compared to a reference solution which was computed based on an approach found in relevant literature. A detailed analysis of the obtained results showed that the application of the proposed approach to real-world instances is promising whereas the savings with respect to working time can be kept high. Overall an efficient as well as effective approach is introduced to solve this real-world problem.";"ii Danksagung Die vorliegende Arbeit durfte ich am Institut für Computergraphik und Algorithmen der Technischen Universität Wien erstellen und es freut mich diese nun fertig in Händen halten zu können. Ich möchte mich für die Geduld und Mithilfe von Seiten Günther Raidls bedanken und ebenso Matthias Prandtstetter großen Dank für seine Betreuung aussprechen. In den vergangenen Monaten hat er einen besonders großen Beitrag zur Vervollständi-gung dieser Arbeit geleistet. Natürlich gilt meine Anerkennung auch allen anderen Personen, die von Seiten des Instituts ihren Anteil an der Entstehung dieser Arbeit hatten. Es ist mir weiters ein Anliegen auch all jene zu erwähnen, die mich im Laufe des gesamten Studiums begleitet und unterstützt haben. Dazu zählen vor allem meine Studienkollegen Christian und Gerhard, sowie gleichermaßen auch meine Eltern, die mir zu jedem Zeitpunkt eine große Hilfe waren und Linda, der ich an dieser Stelle für ihre Ausdauer und Motivation danken will. iii iv Erklärung zur Verfassung der Arbeit Hiermit erkläre ich, Thomas Misar, wohnhaft in 1070 Wien, Seidengasse 3/108, dass ich diese Arbeit selbständig verfasst habe, dass ich die verwendeten Quellen und Hilfs-mittel vollständig angegeben habe und dass ich die Stellen der Arbeit (einschließlich Tabellen, Karten und Abbildungen), die anderen Werken oder dem Internet im Wort-laut oder dem Sinn nach entnommen sind, auf jeden Fall unter Angabe der Quelle als Entlehnung kenntlich gemacht habe. Wien, 20.04.2009 (Unterschrift Verfasser) vi Kurzfassung Im Rahmen dieser Arbeit wird eine konkrete Problemstellung aus dem Bereich der Lagerverwaltung behandelt. Dabei soll die benötigte Zeit zum Ausfassen von Artikeln aus dem Lager unter Berücksichtigung von domänenspezifischen Nebenbedingungen minimiert werden. Ausgehend von durch Kunden laufend aufgegebenen Bestellungen sollen feste Lieferzeiten eingehalten werden und Einschränkungen wie etwa Kapa-zitätslimits oder das Vermeiden von Kollisionen zwischen Arbeitern beachtet werden. Die für die gegebene Problemstellung zentrale Bestimmung effizienter Touren steht im Mittelpunkt der Arbeit, welche mit Ergebnissen aus einer konkreten Implemen-tierung des vorgestellten Ansatzes abschließt. Es wird ein Algorithmus vorgestellt, der ein eigens entwickeltes Dynamisches Pro-gramm zur Berechnung optimaler Wege durch das Warenlager mit der Umsetzung Lösungen erzeugt werden, erfolgt deren Bewertung durch die Berechnung von konkreten Touren mittels eines für diesen Zweck entwickelten Dynamischen Programms. Dabei werden spezielle Eigenschaften der zugrundeliegenden Lagerstruktur ausgenutzt, um so in po-lynomieller Zeit die bestmögliche Wegführung durch das Lager berechnen zu können. Für die Zuordnung von Arbeitern zu den auf diese Weise berechneten Touren wird schließlich eine zusätzliche VNS verwendet, deren Aufgabe es ist, die notwendigen Touren derart zu verteilen, dass der letzte Artikel zum frühest möglichen Zeitpunkt ausgefasst werden kann. Die anhand des implementierten Programms durchgeführten Tests zeigen, dass die erfolgte Tourenplanung wertvolle Ergebnisse liefert und die notwendige Rechenzeit niedrig gehalten werden kann. Getestet wurde mit Bezug auf eine Referenzlösung, welche auf Basis eines aus der Literatur entnommenen Ansatzes erzeugt werden konn-te. Eine ausführliche Auswertung der Testergebnisse zeigte, dass die Anwendung des hier vorgestellten Ansatzes im Echtbetrieb als sehr vielversprechend gilt und er-hebliche Einsparungen bezüglich der benötigten Arbeitszeit erreicht werden können. Insgesamt betrachtet wird ein effizientes und zielführendes Verfahren zur Lösung des vorliegenden Problems vorgestellt. vii viii Abstract Within this thesis a real-world problem related to a warehouse for spare parts is considered. Regarding several constraints typically stated by spare parts suppliers the time needed to collect articles should be minimized. Based on continuously arriving orders by customers predefined delivery times and capacity constraints have to be met. To accomplish this task efficient pickup tours need to be determined which is the main issue covered by this work which comes to an end with experimental results of a concrete implementation of the proposed approach. The algorithm presented embeds a specifically developed dynamic program for computing optimal walks through the warehouse into a general variable neighborhood search (VNS) scheme. Several stages are used for first splitting up all orders, then creating tours out of the results and finally assigning them to available workers. The VNS uses a variant of the variable neighborhood descent (VND) as local improvement procedure. While the neighborhood structures defined are intended to produce candidate solutions, a dynamic program specially designed to compute optimal order picking tours is used to evaluate them. For this purpose properties specific to warehouses are exploited such to compute optimal routes within polynomial time. The final assignment of workers to tours is realized by another VNS. The task is then to find an allocation such that the last article to be picked up will be collected as early as possible. Evaluations of experimental results of a concrete implementation indicate that the presented approach provides valuable pickup plans and computation times can be kept low. Moreover the performed test runs have been compared to a reference solution which was computed based on an approach found in relevant literature. A detailed analysis of the obtained results showed that the application of the proposed approach to real-world instances is promising whereas the savings with respect to working time can be kept high. Overall an efficient as well as effective approach is introduced to solve this real-world problem. 1 Einleitung Die vorliegende Arbeit ist aus einer Zusammenarbeit des Instituts für Computergra-phik und Algorithmen der Technischen Universität Wien mit der Firma Dataphone GmbH entstanden, welche sich mit Problemen der Lagerverwaltung auseinandersetzt und in diesem Fall Aufgaben innerhalb des Lagers eines Ersatzteillieferanten analy-sieren soll. Neben der Verwaltung sämtlicher Stammdaten und Auftragsdaten des Lagers gilt es, die tatsächliche Anordnung von Artikeln im Lager zu erfassen und davon ausgehend die benötigte Zeit zum Ausfassen von bestellten Artikeln (im Wei-teren wird dieser Vorgang auch Kommissionierung genannt) zu minimieren. Dadurch müssen auch die Arbeitsschritte des Lagerpersonals berücksichtigt und im System abgebildet werden. Der Aufbau des Lagers gleicht im Wesentlichen dem, was man sich gemeinhin beim Gedanken an ein Warenlager vorstellt. Es sind parallel zueinander angeordnete Re-gale vorhanden, zwischen denen jeweils Gänge verlaufen, um entsprechend lagernde Artikel ausfassen zu können. An den beiden Enden jedes dieser Regalgänge verlaufen orthogonal dazu etwas breitere Hauptgänge (siehe dazu Abb. 1.1). Innerhalb die-ses Gangsystems bewegen sich dann meist mehrere Arbeiter gleichzeitig, die unter Zuhilfenahme von Kommissionierungswagen diverse Artikel einsammeln. Welche Ar-tikel benötigt werden, ergibt sich aus den jeweils vorliegenden Bestellungen, welche im Laufe des Arbeitstages durch Kunden in Auftrag gegeben werden. Da der Er-satzteillieferant gewisse Lieferzeiten einhalten will und muss, ist eine entsprechend effiziente Bearbeitung der Aufträge notwendig. Sobald die verlangten Artikel einge-sammelt wurden, werden sie zu einer zentralen Stelle im Lager, der Verdichtungszone, gebracht, wo sie verpackt und für den Versand an die Kunden vorbereitet werden. Ohne Unterstützung durch ein entsprechendes System obliegt es nun dem Lager-leiter die vorhandenen Aufträge auf seine Mitarbeiter derart aufzuteilen, dass in möglichst kurzer Zeit die gewünschten Artikel in der Verdichtungszone bereitstehen. Die Reihenfolge, in der Artikel ausgefasst werden, ist nicht weiter vorgegeben und so entscheidet jeder Lagerarbeiter selbstüberselbst¨selbstüber seinen Weg durch das Lager. Diese Wege oder Touren durch das Lager sind nun genau jener Teil innerhalb des gesamten Bestellablaufs, der großes Optimierungspotential bietet. Ziel der Arbeit ist es, das Ressourcenmanagement innerhalb des Lagers dahingehend zu optimieren, dass zunächst die Zusammenstellung von einzusammelnden Artikeln geschickt gewählt wird und damit im Weiteren das Erstellen von dafür kürzest mögli-chen Touren einen erheblichen Vorteil in der Planung des Ablaufs bringt. Ein wesent-1 Einleitung VZ Abbildung 1.1: Schematische Darstellung des Lagers licher Beitrag dazu ist ein von mir im Rahmen dieser Arbeit erstelltes Programm, das der Umsetzung sämtlichersämtlicher¨ sämtlicher¨Uberlegungen innerhalb einer Heuristik dient. Dabei wird die vorhandene Problemstellung in vier algorithmischen Schritten bearbeitet. Es wird anhand bestimmter Kriterien, wie etwa Lieferzeit oder Platzbedarf eines Artikels, bestimmt, welche Artikel innerhalb einer Tour eines Arbeiters im Lager eingesammelt werden sollen. Der Schritt zur Berechnung von Touren ist dabei ein zentraler Bestandteil des Algorithmus und wurde mittels eines speziell entwickelten Dynamischen Programms umgesetzt. Sobald alle benötigten Touren berechnet wur-den, können diese auf alle verfügbaren Lagermitarbeiter aufgeteilt werden, was im letzten Schritt erfolgt. Im Anschluss an die Beschreibung des Algorithmus kann man anhand der Testergeb-nisse sehen, dass die Effizienz des Programms einen durchwegs positiven Eindruck vermittelt. Große Probleminstanzen können zwar sehr lange Laufzeiten im Bereich von mehreren Stunden benötigen, bis der Algorithmus keine weitere Verbesserung bringt, allerdings werden schon innerhalb der ersten Minuten gute Lösungen erzeugt und somit bleibt der Einsatz in der Praxis vielversprechend. Zuerst werde ich nun eine detaillierte Problembeschreibung geben, die als Basis für meine Untersuchungen gedient hat. Im Anschluss daran möchte ich anhand von ver-wandten Arbeiten aus der Literatur einen¨Uberblickübereinen¨ einen¨Uberblickeinen¨Uberblick¨einen¨Uberblicküber den Rahmen geben, in dem sich diese Arbeit bewegt. Ich werde im Weiteren beschreiben, aus welchen Teilen der letztlich verwendete Algorithmus aufgebaut ist und mit welchen Methoden der zuvor beschriebenen Arbeiten hierbei vorgegangen wird, wobei auch diese, wo sinnvoll und passend, im Detail behandelt werden. Im Zuge der Erläuterungen zum algorithmi-schen Verlauf, werde ich auch auf einige spezielle Probleme eingehen, die in diesem Zusammenhang zu lösen waren. Abschließend werden dann die Ergebnisse, die mit dem vorhandenen Programm erzielt werden konnten und aufgrund statistischer Aus-wertungen mehrerer Testläufe zustande gekommen sind, die Arbeit abschließen.";"0,00";"0,00";"0,00"
"TUW-176087";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\groundtruth\TUW-176087.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\result\result-TUW-176087-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\extracted\grobid\grobid-TUW-176087-xstream.xml"")";"Reviews and few non-controlled studies showed the effectiveness of several specific designed computer video-games as an additional form of treatment in several areas. However, there is a lack in the literature of specially designed serious-games for treating mental disorders. Playmancer (ICT European initiative) aims to develop and assess a serious videogame that may help to treat underlying processes (e.g. lack of self-control strategies) in Eating and Impulse control disorders. Preliminary data will be shown.";;"none extracted value";"0,00";"0,00"
"TUW-177140";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\groundtruth\TUW-177140.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\result\result-TUW-177140-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\extracted\grobid\grobid-TUW-177140-xstream.xml"")";"The system cc? is a tool for testing correspondence between propo-sitional logic programs under the answer-set semantics with respect to different refined notions of program correspondence. The underlying methodology of cc? is to reduce a given correspondence problem to the satisfiability problem of quantified propositional logic and to employ extant solvers for the latter language as back-end inference engines. In a previous version of cc?, the system was designed to test correspondence between programs based on relativised strong equivalence under answer-set projection. Such a setting generalises the standard notion of strong equivalence by taking the alphabet of the context programs as well as the projection of the compared answer sets to a set of designated output atoms into account. This paper outlines a newly added component of cc? for testing similarly parameterised correspondence problems based on uniform equivalence.";;"none extracted value";"0,00";"0,00"
"TUW-179146";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\groundtruth\TUW-179146.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\result\result-TUW-179146-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\extracted\grobid\grobid-TUW-179146-xstream.xml"")";"The publish/subscribe paradigm is a common concept for delivering events from information producers to consumers in a decoupled manner. Some approaches allow durable subscriptions or the transportation of events even to mobile subscribers in a dynamic network infrastructure. However, in the safety-critical telematics durable delivery of events is not sufficient enough. Short network connectiv-ity time and small bandwidth limit the number and size of events to be transmitted hence relevant information needed for safety-critical decision making may not be timely delivered.
In this paper we propose the integration of publish/ subscribe systems and Aspect-oriented Space Containers (ASC) distributed via Distributed Hash Tables (DHT) in the network. The approach allows storage, manipulation, pre-processing, and prioritization of messages sent to mobile peers during bursts of connectivity.
The benefits of the proposed approach are a) less complex application logic due to the processing capabilities of Space Containers, and b) increased efficiency due to delivery of essential messages only aggregated and processed while mobile peers are not connected. We describe the architecture of the proposed approach and explain its benefits by means of an industry use case.";"The publish/subscribe paradigm is a common concept for delivering events from information producers to consumers in a decoupled manner. Some approaches allow durable subscriptions or the transportation of events even to mobile subscribers in a dynamic network infrastructure. However, in the safety-critical telematics durable delivery of events is not sufficient enough. Short network connectiv-ity time and small bandwidth limit the number and size of events to be transmitted hence relevant information needed for safety-critical decision making may not be timely delivered. In this paper we propose the integration of publish/ subscribe systems and Aspect-oriented Space Containers (ASC) distributed via Distributed Hash Tables (DHT) in the network. The approach allows storage, manipulation, pre-processing, and prioritization of messages sent to mobile peers during bursts of connectivity. The benefits of the proposed approach are a) less complex application logic due to the processing capabilities of Space Containers, and b) increased efficiency due to delivery of essential messages only aggregated and processed while mobile peers are not connected. We describe the architecture of the proposed approach and explain its benefits by means of an industry use case. 1 Introduction The publish/subscribe paradigm [8] is a common and largely recognized concept for delivering messages in an";"100,00";"100,00";"100,00"
"TUW-180162";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\groundtruth\TUW-180162.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\result\result-TUW-180162-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\extracted\grobid\grobid-TUW-180162-xstream.xml"")";"Web spam denotes the manipulation of web pages with the sole intent to raise their position in search engine rankings. Since a better position in the rankings directly and positively affects the number of visits to a site, attackers use different techniques to boost their pages to higher ranks. In the best case, web spam pages are a nuisance that provide undeserved advertisement revenues to the page owners. In the worst case, these pages pose a threat to Internet users by hosting malicious content and launching drive-by attacks against unsuspecting victims. When successful, these drive-by attacks then install malware on the victims' machines. In this paper, we introduce an approach to detect web spam pages in the list of results that are returned by a search engine. In a first step, we determine the importance of different page features to the ranking in search engine results. Based on this information, we develop a classification technique that uses important features to successfully distinguish spam sites from legitimate entries. By removing spam sites from the results, more slots are available to links that point to pages with useful content. Additionally, and more importantly, the threat posed by malicious web sites can be mitigated, reducing the risk for users to get infected by malicious code that spreads via drive-by attacks.";"Web spam denotes the manipulation of web pages with the sole intent to raise their position in search engine rankings. Since a better position in the rankings directly and positively affects the number of visits to a site, attackers use different techniques to boost their pages to higher ranks. In the best case, web spam pages are a nuisance that provide undeserved advertisement revenues to the page owners. In the worst case, these pages pose a threat to Internet users by hosting malicious content and launching drive-by attacks against unsuspecting victims. When successful, these drive-by attacks then install malware on the victims' machines. In this paper, we introduce an approach to detect web spam pages in the list of results that are returned by a search engine. In a first step, we determine the importance of different page features to the ranking in search engine results. Based on this information, we develop a classification technique that uses important features to successfully distinguish spam sites from legitimate entries. By removing spam sites from the results, more slots are available to links that point to pages with useful content. Additionally, and more importantly, the threat posed by malicious web sites can be mitigated, reducing the risk for users to get infected by malicious code that spreads via drive-by attacks. 1 Introduction Search engines are designed to help users find relevant information on the Internet. Typically, a user submits a query (i.e., a set of keywords) to a search engine, which then returns a list of links to pages that are most relevant to this query. To determine the most-relevant pages, a search engine selects a set of candidate pages that contain some or all of the query terms and calculates a page score for each page. Finally, a list of pages, sorted by their score, is returned to the user. This score is calculated from properties of the candidate pages, so-called features. Unfortunately, details on the exact algorithms that calculate these ranking values are kept secret by search engine companies, since this information directly influences the quality of the search results. Only general information is made available. For example, in 2007, Google claimed to take more than 200 features into account for the ranking value [8]. The way in which pages are ranked directly influences the set of pages that are visited frequently by the search engine users. The higher a page is ranked, the more likely it is to be visited [3]. This makes search engines an attractive target for everybody who aims to attract a large number of visitors to her site. There are three categories of web sites that benefit directly from high rankings in search engine results. First, sites that sell products or services. In their context, more visitors imply more potential customers. The second category contains sites that are financed through advertisement. These sites aim to rank high for any query. The reason is that they can display their advertisements to each visitor, and, in turn, charge the advertiser. The third, and most dangerous, category of sites that aim to attract many visitors by ranking high in search results are sites that distribute malicious software. Such sites typically contain code that exploits web browser vulnerabilities to silently install malicious software on the visitor's computer. Once infected, the attacker can steal sensitive information (such as passwords, financial information, or web-banking credentials), misuse the user's bandwidth to join a denial of service attack, or send spam. The threat of drive-by downloads (i.e., automatically down-loading and installing software without the user's consent as the result of a mere visit to a web page) and distribution of malicious software via web sites has become a significant security problem. Web sites that host drive-by downloads are either created solely for the purpose of distributing malicious software or existing pages that are hacked and modified (for example, by inserting an iframe tag into the page that loads malicious content). Provos et al. [13, 14] observe that such attacks can quickly reach a large number of potential victims, as at least 1.3% of all search queries directed to the Google search engine contain results that link to malicious pages. Moreover, the pull-based infection scheme circumvents barriers (such as web proxies or NAT devices) that protect from push-based malware infection schemes (such as traditional, exploit-based worms). As a result, the manipulation of search engine results is an attractive technique for attackers that aim to attract victims to their malicious sites and spread malware via drive-by attacks [16]. Search engine optimization (SEO) companies offer their expertise to help clients improve the rank for a given site through a mixture of techniques, which can be classified as being acceptable or malicious. Acceptable techniques refer to approaches that improve the content or the presentation of a page to the benefit of users. Malicious techniques, on the other hand, do not benefit the user but aim to mislead the search engine's ranking algorithm. The fact that bad sites can be pushed into undeserved, higher ranks via malicious SEO techniques leads to the problem of web spam. Gyöngyi and Garcia-Molina [9] define web spam as every deliberate human action that is meant to improve a site's ranking without changing the site's true value. Search engines need to adapt their ranking algorithms continuously to mitigate the effect of spamming techniques on their results. For example, when the Google search engine was launched, it strongly relied on the PageRank [2] algorithm to determine the ranking of a page where the rank is proportional to the number of incoming links. Unfortunately, this led to the problem of link farms and ""Google Bombs,"" where enormous numbers of automatically created forum posts and blog comments were used to promote an attacker's target page by linking to it. Clearly, web spam is undesirable, because it degrades the quality of search results and draws users to malicious sites. Although search engines invest a significant amount of money and effort into fighting this problem, checking the results of search engines for popular search terms demonstrates that the problem still exists. In this work, we aim to post-process results returned by a search engine to identify entries that link to spam pages. To this end, we first study the importance of different features for the ranking of a page. In some sense, we attempt to reverse-engineer the ""secret"" ranking algorithm of a search engine to understand better what features are important. Based on this analysis, we attempt to build a classifier that inspects these features to identify indications that a page is web spam. When such a page is identified, we can remove it from the search results. The two main contributions of this paper are the following: • We conducted comprehensive experiments to understand the effects of different features on search engine rankings.";"0,00";"0,00";"0,00"
"TUW-181199";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\groundtruth\TUW-181199.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\result\result-TUW-181199-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\extracted\grobid\grobid-TUW-181199-xstream.xml"")";"Answer-set programming (ASP) is an emerging logic-programming paradigm that strictly separates the description of a problem from its solving methods. Despite its semantic elegance, ASP suffers from a lack of support for program developers. In particular, tools are needed that help engineers in detecting erroneous parts of their programs. Unlike in other areas of logic programming , applying tracing techniques for debugging logic programs under the answer-set semantics seems rather unnatural, since employing imperative solving algorithms would undermine the declarative flavour of ASP. In this paper, we present the system spock, a debugging support tool for answer-set programs making use of ASP itself. The implemented techniques maintain the declarative nature of ASP within the debugging process and are independent of the actual computation of answer sets.";"Answer-set programming (ASP) is an emerging logic-programming paradigm that strictly separates the description of a problem from its solving methods. Despite its semantic elegance, ASP suffers from a lack of support for program developers. In particular, tools are needed that help engineers in detecting erroneous parts of their programs. Unlike in other areas of logic programming , applying tracing techniques for debugging logic programs under the answer-set semantics seems rather unnatural, since employing imperative solving algorithms would undermine the declarative flavour of ASP. In this paper, we present the system spock, a debugging support tool for answer-set programs making use of ASP itself. The implemented techniques maintain the declarative nature of ASP within the debugging process and are independent of the actual computation of answer sets. 1 General Information Answer-set programming (ASP) [1] has become an important logic-programming paradigm for declarative problem solving, incorporating fundamental concepts of non-monotonic reasoning. A major reason why ASP has not yet found a more widespread popularity as a problem-solving technique, however, is its lack of suitable engineering tools for developing programs. In particular, realising tools for debugging answer-set programs is a clearly recognised issue in the ASP community, and several approaches in this direction have been proposed in recent years [2-5]. From a theoretical point of view, the nonmonotonicity of answer-set programs is an aggravating factor for detecting sources of errors, since every rule of a program might significantly influence the resulting answer sets. On the other hand, applying tracing techniques for debugging logic programs under the answer-set semantics seems rather unnatural, since employing imperative solving algorithms would undermine the declarative flavour of ASP.";"0,00";"0,00";"0,00"
"TUW-182414";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\groundtruth\TUW-182414.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\result\result-TUW-182414-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\extracted\grobid\grobid-TUW-182414-xstream.xml"")";"In den letzten Jahren haben Bibliotheken und Archive zunehmend die Aufgabe übernommen, neben konventionellen Publikationen auch Inhalte aus dem World Wide Web zu sammeln, um so diesen wertvollen Teil unseres kulturellen Erbes zu bewahren und wichtige Informationen langfristig verfügbar zu halten. Diese massiven Datensammlungen bieten faszinierende Möglichkeiten, rasch Zugriff auf wichtige Informationen zu bekommen, die im Live-Web bereits verloren gegangen sind. Sie sind eine unentbehrliche Quelle für Wissenschafter, die in der Zukunft die gesellschaftliche und technologische Entwicklung unserer Zeit nachvollziehen wollen.
Auf der anderen Seite stellt eine derartige Datensammlung aber einen völlig neuen Datenbestand dar, der nicht nur rechtliche, sondern auch zahlreiche ethische Fragen betreffend seine Nutzung aufwirft. Diese werden in dem Ausmaß zunehmen, in dem die technischen Möglichkeiten zur automatischen Analyse und Interpretation dieser Daten leistungsfähiger werden. Da sich die meisten Web-Archivierungsinitiativen dieser Problematik bewusst sind, bleibt die Nutzung der Daten derzeit meist stark eingeschränkt, oder es wird eine Art von ""Opt-Out""-Möglichkeit vorgesehen, wodurch Webseiteninhaber die Aufnahme ihrer Seiten in ein Webarchiv ausschließen können. Mit beiden Ansätzen können Webarchive ihr volles Nutzungspotential nicht ausschöpfen.
Das World Wide Web hat sich zu einem integralen Bestandteil unserer Publikations-und Kommunikationskultur entwickelt. Als solches bietet es uns einen reichhaltigen Schatz an wertvollen Informationen, die teilweise ausschließlich in elektronischer Form verfügbar sind, wie z.B. Informationsportale, Informationen zu zahlreichen Projekten und Bürgerinitiativen, Diskussionsforen, soziale Netze und Ähnliches. Weiters beeinflussen die technischen Möglichkeiten sowohl die Art der Gestaltung von Webseiten, als auch die Art, wie wir mit Information umgehen, wie unsere Gesellschaft vernetzt ist, wie sich Information ausbreitet bzw. wie sie genutzt wird. All dies stellt einen immens wertvollen Datenbestand dar, dessen Bedeutung uns teilweise erst bewusst werden mag, wenn dieser nicht mehr verfügbar ist. Dieser Artikel beschreibt einleitend kurz die Technologien, die zur Sammlung von Webinhalten zu Archivierungszwecken verwendet werden. Er hinterfragt Annahmen betreffend die freie Verfügbarkeit der Daten und unterschiedliche Nutzungsarten. Darauf aufbauend identifiziert er eine Reihe von offenen Fragen, deren Lösung einen breiteren Zugriff und Nutzung von Webarchiven erlauben könnte.";"Kurzfassung In den letzten Jahren haben Bibliotheken und Archive zunehmend die Aufgabe übernommen, neben konventionellen Publikationen auch Inhalte aus dem World Wide Web zu sammeln, um so diesen wertvollen Teil unseres kulturellen Erbes zu bewahren und wichtige Informationen langfristig verfügbar zu halten. Diese massiven Datensammlungen bieten faszinierende Möglichkeiten, rasch Zugriff auf wichtige Informationen zu bekommen, die im Live-Web bereits verloren gegangen sind. Sie sind eine unentbehrliche Quelle für Wissenschafter, die in der Zukunft die gesellschaftliche und technologische Entwicklung unserer Zeit nachvollziehen wollen. Auf der anderen Seite stellt eine derartige Datensammlung aber einen völlig neuen Datenbestand dar, der nicht nur rechtliche, sondern auch zahlreiche ethische Fragen betreffend seine Nutzung aufwirft. Diese werden in dem Ausmaß zunehmen, in dem die technischen Möglichkeiten zur automatischen Analyse und Interpretation dieser Daten leistungsfähiger werden. Da sich die meisten Web-Archivierungsinitiativen dieser Problematik bewusst sind, bleibt die Nutzung der Daten derzeit meist stark eingeschränkt, oder es wird eine Art von ""Opt-Out""-Möglichkeit vorgesehen, wodurch Webseiteninhaber die Aufnahme ihrer Seiten in ein Webarchiv ausschließen können. Mit beiden Ansätzen können Webarchive ihr volles Nutzungspotential nicht ausschöpfen. Das World Wide Web hat sich zu einem integralen Bestandteil unserer Publikations-und Kommunikationskultur entwickelt. Als solches bietet es uns einen reichhaltigen Schatz an wertvollen Informationen, die teilweise ausschließlich in elektronischer Form verfügbar sind, wie z.B. Informationsportale, Informationen zu zahlreichen Projekten und Bürgerinitiativen, Diskussionsforen, soziale Netze und Ähnliches. Weiters beeinflussen die technischen Möglichkeiten sowohl die Art der Gestaltung von Webseiten, als auch die Art, wie wir mit Information umgehen, wie unsere Gesellschaft vernetzt ist, wie sich Information ausbreitet bzw. wie sie genutzt wird. All dies stellt einen immens wertvollen Datenbestand dar, dessen Bedeutung uns teilweise erst bewusst werden mag, wenn dieser nicht mehr verfügbar ist. Dieser Artikel beschreibt einleitend kurz die Technologien, die zur Sammlung von Webinhalten zu Archivierungszwecken verwendet werden. Er hinterfragt Annahmen betreffend die freie Verfügbarkeit der Daten und unterschiedliche Nutzungsarten. Darauf aufbauend identifiziert er eine Reihe von offenen Fragen, deren Lösung einen breiteren Zugriff und Nutzung von Webarchiven erlauben könnte. Die fehlende langfristige Verfügbarkeit ist eine der entscheidenden Schwachstellen des World Wide Web. Unterschiedlichen Studien zufolge beträgt die durchschnittliche Lebensdauer eine Webressource zwischen wenigen Tagen und Monaten. So können schon binnen kürzester Zeit wertvolle Informationen nicht mehr über eine angegebene URL bezogen werden, bzw. stehen Forschern in naher und ferner Zukunft de-fakto keine Materialien zur Verfügung, um diese unsere Kommunikationskultur zu analysieren. Selbst Firmen haben zunehmend Probleme, Informationen über ihre eigenen Projekte, die vielfach nicht über zentrale Dokumentmanagementsysteme sondern webbasiert und kollaborativ in wikiartigen Systemen verwaltet werden, verfügbar zu halten. Web in regelmäßigen Abständen kopiert und archiviert. Die eingesetzten Technologien reichen dabei von manueller Sammlung bis hin zu großflächigen ""Crawls"", wie sie z.B. die Basis von Suchmaschinen sind. Dies bewahrt einerseits wertvolle Daten vor dem Totalverlust, stellt aber die Institutionen-sowie die Gesellschaft als ganzes-vor eine Reihe von wichtigen Fragestellungen betreffend die langfristige Bewahrung dieser Daten und ihre Nutzung. 1. Kurzlebigkeit von Webinhalten Dieser Artikel wird im folgenden Abschnitt einen kurzen Überblick über die einzelnen Sammlungsstrategien geben, die zum Aufbau eines solchen Archivs verwendet werden. In Abschnitt 3 werden einige Annahmen hinterfragt, welche die freie Verfügbarkeit und Nutzung von Daten aus dem World Wide Web";"0,00";"0,00";"0,00"
"TUW-182899";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\groundtruth\TUW-182899.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\result\result-TUW-182899-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\extracted\grobid\grobid-TUW-182899-xstream.xml"")";"As photographic technologies continue to develop, so too do the social practices surrounding their use. The focus of this paper is on the social practices surrounding images captured from a new photographic device – SenseCam – which, rather than capturing individual images when triggered by the user, automatically captures a series of images. This paper is concerned with the use of SenseCam digital images in social contexts where there is a professional purpose: supporting the collaborative reflective practices of school teachers and university tutors as part of their professional development. Analysis of video data collected from 16 in-situ case studies of reflective discussions show evidence that reflection took place as defined in the literature. Further, the phototalk around SenseCam images was found to benefit reflection in these social situations through promoting a rich shared understanding of the lesson context: supporting return to the experience, sharing of background context, grounding conversations, illustrating and providing evidence, and allowing people to see more. The paper concludes with a discussion on how different features of SenseCam images, such as variable quality, lack of audio, incompleteness, helped in this reflection or not. Finally implications from this work and participants comments are used to suggest ways in which SenseCam may be used in the future in teacher and tutors social reflection.";"As photographic technologies continue to develop, so too do the social practices surrounding their use. The focus of this paper is on the social practices surrounding images captured from a new photographic device-SenseCam-which, rather than capturing individual images when triggered by the user, automatically captures a series of images. This paper is concerned with the use of SenseCam digital images in social contexts where there is a professional purpose: supporting the collaborative reflective practices of school teachers and university tutors as part of their professional development. Analysis of video data collected from 16 in-situ case studies of reflective discussions show evidence that reflection took place as defined in the literature. Further, the phototalk around SenseCam images was found to benefit reflection in these social situations through promoting a rich shared understanding of the lesson context: supporting return to the experience, sharing of background context, grounding conversations, illustrating and providing evidence, and allowing people to see more. The paper concludes with a discussion on how different features of SenseCam images, such as variable quality, lack of audio, incompleteness, helped in this reflection or not. Finally implications from this work and participants comments are used to suggest ways in which SenseCam may be used in the future in teacher and tutors social reflection. With advances in digital photography there has been a growing interest in the novel ways that these photos are brought into social practices and the ways in which people are evolving their photographic habits. Whilst most of this work is focussed on the digital form of the stills camera, a different type of digital photo is emerging in the sphere of lifelogging. SenseCam is a prototype lifelogging device currently under development at Microsoft Research in Cambridge (e.g. Williams and Wood 2004; Cherry 2005). It is a small wearable device combining a digital camera with a number of built in sensors and is made to be worn by a person around their neck like a pendant (see Figure 1). The sensors, which measure light, motion, sound, infra-red and ambient temperature, are used to trigger digital still images to be taken at 'good' times when something interesting may be happening. Currently 'good' is defined by the developer as when there is a sudden change in light (which might happen when we move from one room to another), sound or temperature, or when the infra-red data combined with motion detection suggests another person is nearby. On average 3 or 4 photos per minute are triggered in this way. The camera also has a very wide angle, fish-eye lens which captures most of what is the field of view of the wearer from a first person perspective. These combined features allow the wearer to passively capture a whole day's worth of images without having to press a trigger or aim the camera, leaving their hands and attention free to get on with their everyday tasks. When downloaded to a PC, the images can be viewed using a rapid serial visualization tool, playing somewhat like a sped up movie (see Figures 2-7 for example images), and the whole day may only take around 10 minutes to review. In this way, SenseCam can be considered a 'life-logging' tool. Much research to date with SenseCam has focused on its potential to record life experience as a support to memory-indeed it has been shown to be an effective tool in supporting people with severe memory-loss (Cherry 2005), and to support different aspects of 'remembering' and knowing' for people with a normal memory (Sellen et al. 2007). More recently, research has suggested that SenseCam images can also evoke reflection on past life experiences (Harper et al. 2007; Harper et al. 2008); and that sharing such images with others prompts reflection on own and others' lives (Lindley et al. 2009). The focus of this research has been very much on everyday life-the way SenseCam as a life-logging tool was envisaged to be used. We have also shown that SenseCam can support reflection in a learning context, where students used the images from a field trip to reflect on their experiences (Fleck and Fitzpatrick 2006). In this paper, however, we consider the value of SenseCam images in a work setting: we explore the potential of the device to capture aspects of professional experience and share these with others, espoused as part of being a good reflective practitioner (Moon 1999). The professional practice we focus on is that of teaching-in both school and university contexts. This is a domain in which another visual experience-recording technology, video, has been advocated for over thirty years (Zuber-Skerritt 1984). We have previously undertaken research with individual school teachers and university tutors using SenseCam for self-reflection and have shown it to be useful (Fleck 2008). Here we expand on these findings and focus on how SenseCam might be used in schools and universities to support reflection in social contexts: specifically we look at how it can support reflective practice conversations between novice teacher peers, novice teachers and their mentors, and between trainee university tutors.";"0,00";"0,00";"0,00"
"TUW-185321";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\groundtruth\TUW-185321.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\result\result-TUW-185321-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\extracted\grobid\grobid-TUW-185321-xstream.xml"")";"The semantic heterogeneity of Open Source Software (OSS) development projects comes from the using of different tools and models by the various stakeholders. These differences make the process of integration become difficult, since the project managers should recognize the different structure of the tools and models for analyzing the state of the projects. This manual analysis is costly and error prone. In this work we propose a semantic web technology approach to bridge these semantic heterogeneities, by using engineering knowledge base (EKB). The EKB enables mapping between local and domain ontology layers to allow querying the local tool knowledge using the domain-level knowledge and syntax. We empirically evaluate the feasibility of an EKB-based project monitoring system based on real-world data.";"The semantic heterogeneity of Open Source Software (OSS) development projects comes from the using of different tools and models by the various stakeholders. These differences make the process of integration become difficult, since the project managers should recognize the different structure of the tools and models for analyzing the state of the projects. This manual analysis is costly and error prone. In this work we propose a semantic web technology approach to bridge these semantic heterogeneities, by using engineering knowledge base (EKB). The EKB enables mapping between local and domain ontology layers to allow querying the local tool knowledge using the domain-level knowledge and syntax. We empirically evaluate the feasibility of an EKB-based project monitoring system based on real-world data. to model the project/domain knowledge that are used";"100,00";"100,00";"100,00"
"TUW-185441";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\groundtruth\TUW-185441.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\result\result-TUW-185441-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\extracted\grobid\grobid-TUW-185441-xstream.xml"")";"Shape analysis is a static program analysis technique for discovering properties of heap-allocated data structures. It is crucial to finding software bugs or to verify high-level correctness properties. Various analyses have been introduced but their relation in terms of precision often remains unclear as different analyses use different abstractions of the heap. The aim of our work is to compare the precision of shape analyses. We propose a novel algorithm based on three-valued logic that extracts alias sets from shape graphs. Smaller sets are more precise and indicate a more precise underlying shape analysis. Using this metric, we experimentally compare – for the first time – the relative quality of the state-of-the-art graph-based shape analyses and make recommendations concerning the combination of analysis parameters.";"Shape analysis is a static program analysis technique for discovering properties of heap-allocated data structures. It is crucial to finding software bugs or to verify high-level correctness properties. Various analyses have been introduced but their relation in terms of precision often remains unclear as different analyses use different abstractions of the heap. The aim of our work is to compare the precision of shape analyses. We propose a novel algorithm based on three-valued logic that extracts alias sets from shape graphs. Smaller sets are more precise and indicate a more precise underlying shape analysis. Using this metric, we experimentally compare-for the first time-the relative quality of the state-of-the-art graph-based shape analyses and make recommendations concerning the combination of analysis parameters.";"100,00";"100,00";"100,00"
"TUW-186227";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\groundtruth\TUW-186227.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\result\result-TUW-186227-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\extracted\grobid\grobid-TUW-186227-xstream.xml"")";"One indispensable precondition for designing a functional software product for the modeling and execution of a computerized clinical practice guideline (CPG) is the comprehensive investigation of the different user groups involved and the issues they encounter. This led us to conduct a comprehensive literature study about the tasks involved in modeling a CPG into a formal representation as well as about the information needs of caregivers, i.e., physicians and nurses, and last but not least the information needs of patients. We have assessed and categorized the above mentioned information in order to create a reliable starting point for the development of a functional software tool.";"One indispensable precondition for designing a functional software product for the modeling and execution of a computerized clinical practice guideline (CPG) is the comprehensive investigation of the different user groups involved and the issues they encounter. This led us to conduct a comprehensive literature study about the tasks involved in modeling a CPG into a formal representation as well as about the information needs of caregivers, i.e., physicians and nurses, and last but not least the information needs of patients. We have assessed and categorized the above mentioned information in order to create a reliable starting point for the development of a functional software tool.";"100,00";"100,00";"100,00"
"TUW-189842";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\groundtruth\TUW-189842.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\result\result-TUW-189842-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\extracted\grobid\grobid-TUW-189842-xstream.xml"")";"New technologies open up possibilities for designing interactive experiences that can engage and motivate post-stroke survivors to undertake what would otherwise be boring repetitive movements. In this paper we outline a few of the challenges we met as part of the cross-disciplinary Motivating Mobility project. These are: the extended 'user'; autonomy and motivation; and early prototype studies.";"New technologies open up possibilities for designing interactive experiences that can engage and motivate post-stroke survivors to undertake what would otherwise be boring repetitive movements. In this paper we outline a few of the challenges we met as part of the cross";"0,00";"0,00";"0,00"
"TUW-191715";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\groundtruth\TUW-191715.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\result\result-TUW-191715-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\extracted\grobid\grobid-TUW-191715-xstream.xml"")";"In this paper we give an overview of the current research trends and explore the challenges in several subfields of the scientific discipline of computer graphics: interactive and photorealistic rendering, scientific and information visualization, and visual analytics. Five challenges are extracted that play a role in each of these areas: scalability, semantics, fusion, interaction, acquisition. Of course, not all of these issues are disjunct to each other, however the chosen structure allows for a easy to follow overview of the concrete future challenges.";"In this paper we give an overview of the current research trends and explore the challenges in several subfields of the scientific discipline of computer graphics: interactive and photorealistic rendering, scientific and information visualization, and visual analytics. Five challenges are extracted that play a role in each of these areas: scalability, semantics, fusion, interaction, acquisition. Of course, not all of these issues are disjunct to each other, however the chosen structure allows for a easy to follow overview of the concrete future challenges. Computer graphics studies methods for producing digital images of data with the goal to communicate computer output to a human user in the form of pictures. The visual input channel has by far the broadest bandwidth of all our senses and therefore enables the most effective transport of information from computers to humans. Production includes synthesizing, manipulating and displaying the underlying data. The data may be almost any content one can think of: geometric or other spatial data just as well as statistical data, simulation results or abstract data, all real or virtual. Roughly speaking, the ultimate goal of rendering research is to create perfectly realistic looking real-time images of real-world objects, whereas visualization tries to create images of data and structures that are otherwise invisible to the human eye or completely abstract. 10 Werner Purgathofer and Robert F. Tobler Computer graphics has been among the most successful computer science fields during the last three decades and the methods and results available today have exceeded the expectations by far. Therefore some people consider most computer graphics problems as solved, providing a ready to use set of tools for applications. But while this is true for some areas with simple use of computer images, the embedding of computer graphics technology in increasingly complex surroundings generates many new challenges. Usage of computer graphics is embedded in more and more complicated environments, making its combined use with other technologies more and more natural, so that many people talk about disciplines growing together. Such fields are computer vision, image processing, pattern recognition, tracking, scanning, video augmentation, information theory, user interface design, large data bases and several more. Multiple articles in the past decades have extracted future research problems in various subfields of computer graphics, e.g. [1],…,[3]. In this article we will describe the research trends of the coming years based on five major challenges that are common to all computer graphics sub-fields. These are: 1. Scalability = how to cope with huge amounts of data, highly parallel computers and distributed devices. 2. Semantics = how can meaning be extracted from data and context and be used for better insight. 3. Fusion = how can multiple techniques, data streams, and models be combined to solve complex problems. 4. Interaction = how to combine multiple and ubiquitous input devices to create ergonomic user interfaces. 5. Acquisition = how can data from various input sources be processed to deal with missing data, contradictions, and uncertainty. These challenges are often interdependent to some degree. For example, semantics can assist scalability by determining features that can be left away at a given level of detail, and acquisition by allowing a meaningful extrapolation of missing data and resolution of contradictions; petascale data will require new interaction metaphors and acquisition and data processing methods. Consequently, any research in these areas will typically cross boundaries and cannot be strictly attributed to a single challenge. Still, this discrimination facilitates the following description of the specific aspects of each challenge. 2. Scalability 2.1. Challenges in Scalability The challenges posed by the enormous amount of data generated by";"0,00";"0,00";"0,00"
"TUW-191977";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\groundtruth\TUW-191977.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\result\result-TUW-191977-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\extracted\grobid\grobid-TUW-191977-xstream.xml"")";"Digital Library (DL) interoperability requires addressing a variety of issues associated with functionality. We report on the analysis and solutions identified by the Functionality Working Group of the DL.org project during its deliberations on DL interoperability. Ultimately, we hope that work based on our perspective will lead to improved architectures and software, as well as to greater interoperability, for next-generation DL systems.";;"none extracted value";"0,00";"0,00"
"TUW-192724";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\groundtruth\TUW-192724.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\result\result-TUW-192724-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\extracted\grobid\grobid-TUW-192724-xstream.xml"")";"Human resource strategy can emerge within a decentralized decision structure that gives managers autonomy to take responsive actions while overall strategic direction is considered within a strategic planning process. This study defines the concept of employee selection (especially strategic employee selection) and hypothesizes on the positive correlation between innovation characteristic in SME and value of strategic employee (the so called personnel usefulness function). An empirical study illustrates the importance of both elements in an integrative human resource strategy formation process particularly for firms operating in the international environments.";"Human resource strategy can emerge within a decentralized decision structure that gives managers autonomy to take responsive actions while overall strategic direction is considered within a strategic planning process. This study defines the concept of employee selection (especially strategic employee selection) and hypothesizes on the positive correlation between innovation characteristic in SME and value of strategic employee (the so called personnel usefulness function). An empirical study illustrates the importance of both elements in an integrative human resource strategy formation process particularly for firms operating in the international environments. Keywords: the personnel usefulness function, innovation characteristics, the employee selection efficiency method. 1. Introduction Growing international competition necessitates increasing interest in new scientific advancements and new solutions in the field of management based on innovation transfer. Management staff forms a basis for expediting structural changes in the regional economy. Staff is also a decisive factor for enhancing competitiveness. In the knowledge based economy, while trying to maintain their position in the market or access new markets, managers and employees need to improve their skills. This will help in being able to use innovative technologies that exist all over the world. It is generally recognized that effective human resource strategy formation processes consist of central planning activities orchestrated at the corporate strategic apex as well as emerging strategic decisions influenced by empowered managers within the organization [Hill and Jones, 2000; Johnson and Scholes, 2002; Mintzberg et al., 2003]. 1 Acknowledgement: The research leading to these results has received funding from the European Companies operating in a market economy need to introduce changes to their systems organisation and management. In business practice, decision making in a company depends on activity, competition, changing external factors, e.g. technical advancement, and results achieved by research and development departments, knowledge, employees' skills, social relations, know-how, and in particular, effective investment in intellectual are an added value for a company. Companies investing in human capital and work systems acquire competitive advantage thanks to their readiness to learn and adopt new qualifications as well as efficient communication and information channels. Small and medium sizes enterprises (SMEs) that are about to make decision concerning the employee selection tend to make a pre-evaluation of the efficiency of this decision. There is a demand for developing the method that would diminish the risk of an inadequate employee selection and at the same time would allow to solve the problems which otherwise could be missed. A relevant framework to this issue is based on a database referring to: The SME, with defined the selected functional area, the business processes, the workplaces and the personnel usefulness function for m-the employee in the n-th SME: W nm , n,m?N is given. Allowing responding to the following question: Whether a existed (given) the employee selection efficiency method based on the innovation transfer in the small and medium enterprises (SME)? It can be concluded that there is a need to define the criteria of the employee selection (especially strategic employee) efficiency in the SME. Consequently an appropriate reference model of the company should be developed to enable both defining: the needs in the areas of functionality and the success evaluation of employee selection. Such a model should provide a kind of guideline for the future employee selection framework. The decision as far as the selection of the proper employee should be preceded by the management profitability assessment and the prospect advantage. In order to view the prediction of the described innovation indicators the authorized consulting system supporting decision making enabling to asses and forecast of knowledge in SMEs (software) can be used (an Advisory Computer System for Forecasting the Efficiency of a Strategic Employee Selection in Small and Medium Enterprises). This tool is the implementation of the method of assessment of efficiency strategic employee selection based on innovation indicators in the small and medium enterprises (SME), which in this paper is presented. This computer program could be a supplement as the module to the for example compute system: Enterprise Resource Planning (ERP). ERP is process oriented software that implementation can improve business processes running through functional areas of a company onto computer system [Klaus at al., 2000]. Implementation of ERP is a strategic decision, that results about efficiency of business processes, charge of resources and generally about compete and management efficiency of the enterprise for the next years.";"0,00";"0,00";"0,00"
"TUW-194085";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\groundtruth\TUW-194085.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\result\result-TUW-194085-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\extracted\grobid\grobid-TUW-194085-xstream.xml"")";"Topology is the general mathematical theory of convergence. Distributed computing is the formal investigation of communicating concurrent processes. We explore applications of topology to distributed computing in two directions: (1) Point-set topology and (2) algebraic topology.
We use the former to study the topological structure of infinite execution trees. This enables us to unify a number of impossibility proofs, in particular, the impossibility of distributed consensus — the task of all processes in a system agreeing on a single value — in various (close to) asynchronous systems with crash failures.
The latter is used to look into the combinatorial structure of configurations, i.e., the collection of current process states in the system. Configurations are regarded as simplices in a simplicial complex, and topological incompatibility of such complexes is utilized to prove the impossibility of a generalization of distributed consensus in certain systems. The particular problem considered is k-set agreement, which is the task of letting all processes agree to values within a set of at most k elements.";"Topology is the general mathematical theory of convergence. Distributed computing is the formal investigation of communicating concurrent processes. We explore applications of topology to distributed computing in two directions: (1) Point-set topology and (2) algebraic topology. We use the former to study the topological structure of infinite execution trees. This enables us to unify a number of impossibility proofs, in particular, the impossibility of distributed consensus-the task of all processes in a system agreeing on a single value-in various (close to) asynchronous systems with crash failures. The latter is used to look into the combinatorial structure of configurations, i.e., the collection of current process states in the system. Configurations are regarded as simplices in a simplicial complex, and topological incompatibility of such complexes is utilized to prove the impossibility of a generalization of distributed consensus in certain systems. The particular problem considered is k-set agreement, which is the task of letting all processes agree to values within a set of at most k elements. Zweitere wird verwendet, um die kombinatorische Struktur von Konfiguratio-nen, also der Zusammenfassung aller lokaler Zustände der Prozesse, zu untersu-chen. Hierbei wird eine Konfiguration als Simplex in einem Simplizialkomplex aufgefasst. Die topologische Unvereinbarkeit solcher Komplexe ermöglicht einen Beweis der Unmöglichkeit von k-Set Agreement in gewissen Systemen. Das ist eine Verallgemeinerung des Consensus-Problems: Es wird nicht mehr verlangt, dass sich die Prozesse auf nur einen Wert einigen, sondern es wird erlaubt, dass bis zu k unterschiedliche Werte auftreten. Contents 1. Introduction This thesis deals with applications of topology to distributed computing. These are twofold: Firstly, we use point-set topology to provide a unifying topological framework for consensus impossibility proofs. Secondly, we present the impossibility proof of k-set agreement by Herlihy and Shavit (1993) which uses algebraic topology. 1.1. Distributed Computing Consider a system of N processes that communicate by means of passing messages. All processes take steps simultaneously at times t = 0, 1, 2,. .. and in zero time. All message delays are equal to 1/2, i.e., processes at time t+1 have received all messages sent in computing steps at time t. Processes are modeled as state machines and run a local algorithm which governs state transitions and message sendings. Interesting questions to ask might include: (1) How many steps does it take until the last process has terminated? (2) How many messages are sent in the execution of the algorithm? (3) Is the algorithm correct, i.e., does it indeed fulfill its task specification? The investigation of such questions is the realm of distributed computing. We can spice things up a bit by varying model parameters. For example, we may allow more general message delays than fixing them all at exactly 1/2. Likewise, we might choose not to fix the times at which processes take steps to exactly 0, 1, 2,. .. Of course, also the restriction that all processes take steps simultaneously might seem overly limiting. We may also introduce the possibility of lost messages: In the completely synchronous model with message delays equal to 1/2, suppose that in every time frame [t, t + 1), up to N ? 1 message may be lost. That is, these messages do not get delivered although all other messages are delivered timely. A surprising result (Santoro and Widmayer 1989) is that even in such a system with relatively few faults (there exist up to N 2 ? N point-to-point links; at most N ? 1 are lossy each round) it is impossible for any deterministic algorithm to solve consensus. Consensus is the task of all processes in the system agreeing on a single value. 1 Introduction 1.2 Topology 1.2. Topology Topology is the general mathematical theory of convergence. Its most popular special case is the study of metric spaces. It tackles questions like: (1) Does the image of a continuous function f : [0, 1] ? R have a maximum? (2) Does every Cauchy sequence converge? (3) How many holes does a given manifold have? (4) Can you cut two pizzas in half with a single cut, no matter how ugly they are shaped? The immediate investigation of topological spaces is called point-set topology, which questions (1) and (2) can be attributed to. Another common technique is to assign algebraic structures to topological spaces, reason about relations between these structures and map these insights back into the world of topological spaces. This method is called algebraic topology. 1.3. Structure of the Thesis Chapter 2 introduces distributed computing as a discipline and presents formal system models. In Chapter 3, we talk about an important problem specification in distributed computing: k-set agreement and its important special case, consensus. Chapter 4 investigates execution spaces of distributed algorithms by means of point-set topology and provides a unified proof of the impossibility of consensus in some important system models. Chapter 5 deals with methods from algebraic topology. It explains the approach taken by Herlihy and Shavit (1993) to prove the impossibility of k-set agreement in the presence of up to k crash failures. A summary of the thesis is given in Chapter 6. Appendix A gives a self-contained introduction to point-set";"0,00";"0,00";"0,00"
"TUW-194561";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\groundtruth\TUW-194561.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\result\result-TUW-194561-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\extracted\grobid\grobid-TUW-194561-xstream.xml"")";"We propose a generalized version of context-sensitivity in term rewriting based on the notion of ""forbidden patterns"". The basic idea is that a rewrite step should be forbidden if the redex to be contracted has a certain shape and appears in a certain context. This shape and context is expressed through forbidden patterns. In particular we analyze the relationships among this novel approach and the commonly used notion of context-sensitivity in term rewriting, as well as the feasibility of rewriting with forbidden patterns from a computational point of view. The latter feasibility is characterized by demanding that restricting a rewrite relation yields an improved termination behaviour while still being powerful enough to compute meaningful results. Sufficient criteria for both kinds of properties in certain classes of rewrite systems with forbidden patterns are presented.";"We propose a generalized version of context-sensitivity in term rewriting based on the notion of ""forbidden patterns"". The basic idea is that a rewrite step should be forbidden if the redex to be contracted has a certain shape and appears in a certain context. This shape and context is expressed through forbidden patterns. In particular we analyze the relationships among this novel approach and the commonly used notion of context-sensitivity in term rewriting, as well as the feasibility of rewriting with forbidden patterns from a computational point of view. The latter feasibility is characterized by demanding that restricting a rewrite relation yields an improved termination behaviour while still being powerful enough to compute meaningful results. Sufficient criteria for both kinds of properties in certain classes of rewrite systems with forbidden patterns are presented. 1 Introduction and Overview Standard term rewriting systems (TRSs) are well-known to enjoy nice logical and closure properties. Yet, from an operational and computational point of view, i.e., when using term rewriting as computational model, it is also well-known that for non-terminating systems restricted versions of rewriting obtained by imposing context-sensitivity and/or strategy requirements may lead to better results (e.g., in terms of computing normal forms, head-normal forms, etc.). One major goal when using reduction strategies and context restrictions is to avoid non-terminating reductions. On the other hand the restrictions should not be too strong either, so that the ability to compute useful results in the restricted rewrite systems is not lost. We introduce a novel approach to context restrictions relying on the notion of ""forbidden patterns"", which generalizes existing approaches and succeeds in handling examples in the mentioned way (i.e., producing a terminating reduction relation which is powerful enough to compute useful results) where others fail. The following example motivates the use of reduction strategies and/or context restrictions. Example 1. Consider the following rewrite system, cf. This TRS is non-terminating and not even weakly normalizing. Still some terms like 2nd(inf(x)) are reducible to a normal form while also admitting infinite reduction sequences. One goal of context restrictions and reduction strategies is to restrict derivations in a way such that normal forms can be computed whenever they exist, while infinite reductions are avoided. One way to address the problem of avoiding non-normalizing reductions in Example 1 is the use of reduction strategies. For instance for the class of (almost) orthogonal rewrite systems (the TRS of * This author has been supported by the Austrian Academy of Sciences under grant 22. 57 Example 1 is orthogonal), always contracting all outermost redexes in parallel yields a normalizing strategy (i.e. whenever a term can be reduced to a normal form it is reduced to a normal form under this strategy) [18]. Indeed, one can define a sequential reduction strategy having the same property for an even wider class of TRSs [3]. One major drawback (or asset depending on one's point of view) of using reduction strategies, however, is that their use does not introduce new normal forms. This means that the set of normal forms w.r.t. to some reduction relation is the same as the set of normal forms w.r.t. to the reduction relation under some strategy. Hence, strategies can in general not be used to detect non-normalizing terms or to impose termination on not weakly normalizing TRSs (with some exceptions cf. e,g. [3, Theorem 7.4]). Moreover, the process of selecting a suitable redex w.r.t. to a reduction strategy is often complex and may thus be inefficient. These shortcomings of reduction strategies led to the advent of proper restrictions of rewriting that usually introduce new normal forms and select respectively forbid certain reductions according to the syntactic structure of a redex and/or its surrounding context. The most well-known approach to context restrictions is context-sensitive rewriting. There, a replacement map µ specifies the arguments µ(f) ? {1,. .. , ar(f)} which can be reduced for each function f. However, regarding Example 1, context-sensitive rewriting does not improve the situation, since allowing the reduction of the second argument of ':' leads to non-termination, while disallowing its reduction leads to incompleteness in the sense that for instance a term like 2nd(inf(x)) cannot be normalized via the corresponding context-sensitive reduction relation, despite having a normal form in the unrestricted system. Other ideas of context restrictions range from explicitly modeling lazy evaluation (cf. latter generalized versions of context-sensitive rewriting are quite expressive and powerful (indeed some of them can be used to restrict the reduction relation of the TRS in Example 1 in a way, so that the restricted relation is terminating and still powerful enough to compute (head-)normal forms), but on the other hand tend to be hard to analyze and understand, due the subtlety of the strategic information specified. The approach we present in this paper is simpler in that its definition only relies on matching and simple comparison of positions rather than on laziness or prioritizing the evaluation of certain arguments of functions over others. In order to reach the goal of restricting the reduction relation in such a way that it is terminating while still being powerful enough to compute useful results, we provide a method to verify termination of a reduction relation restricted by our approach (Section 5) as well as a criterion which guarantees that normal forms computed by the restricted system are head-normal forms of the unrestricted system (Section 4). Recently it turned out that, apart from using context-sensitivity as computation model for standard term rewriting (cf. e.g. [16, 14]), context-sensitive rewrite systems naturally also appear as intermediate representations in many areas relying on transformations, such as program transformation and termination analysis of rewrite systems with conditions [6, 20] / under strategies [8]. This suggests that apart from using restrictions as guidance and thus as operational model for rewrite derivations, a general, flexible and well-understood framework of restricted term rewriting going beyond context-sensitive rewriting may be useful as a valuable tool in many other areas, too. The major problem in building such a framework is that imposing context restrictions on term rewriting in general invalidates the closure properties of term rewriting relations, i.e., stability under contexts and substitutions. Note that in the case of context-sensitive rewritingàrewriting`rewritingà la [14, 16] only stability under contexts is lost.";"0,00";"0,00";"0,00"
"TUW-194660";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\groundtruth\TUW-194660.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\result\result-TUW-194660-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\extracted\grobid\grobid-TUW-194660-xstream.xml"")";"Interaction Nets are a novel model of computation based on graph rewriting. Their main properties are parallel evaluation and sharing of computation, which leads to efficient programs. However, Interaction Nets lack several features that allow for their convenient use as a programming language. In this paper , we describe the implementation of an extension for pattern matching of interaction rules. Furthermore, we show the cor-rectness of the implementation and discuss its complexity.";"Interaction Nets are a novel model of computation based on graph rewriting. Their main properties are parallel evaluation and sharing of computation, which leads to efficient programs. However, Interaction Nets lack several features that allow for their convenient use as a programming language. In this paper , we describe the implementation of an extension for pattern matching of interaction rules. Furthermore, we show the cor-rectness of the implementation and discuss its complexity. rules model the addition of natural numbers (encoded by 0 and a successor";"100,00";"100,00";"100,00"
"TUW-197422";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\groundtruth\TUW-197422.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\result\result-TUW-197422-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\extracted\grobid\grobid-TUW-197422-xstream.xml"")";"Visual Analytics strongly emphasizes the importance of interaction. However, until now, interaction is only sparingly treated as subject matter on its own. How and why interactivity is beneficial to gain insight and make decisions is mostly left in the dark. Due to this lack of initial direction, it seems important to make further attempts in facilitating a deeper understanding of the concept of interactivity. Therefore, different perspectives towards interactivity are discussed and cognitive theories and models are investigated. The main aim of this paper is to broaden the view on interaction and spark further discussion towards a sound theoretical grounding for the field.";"Visual Analytics strongly emphasizes the importance of interaction. However, until now, interaction is only sparingly treated as subject matter on its own. How and why interactivity is beneficial to gain insight and make decisions is mostly left in the dark. Due to this lack of initial direction, it seems important to make further attempts in facilitating a deeper understanding of the concept of interactivity. Therefore, different perspectives towards interactivity are discussed and cognitive theories and models are investigated. The main aim of this paper is to broaden the view on interaction and spark further discussion towards a sound theoretical grounding for the field. Categories and Subject Descriptors (according to community is consistently stating the value and importance of interaction for visual data analysis. Despite this, empirical studies or theoretical models that support these statements are scarce or missing at all. Until now the value of interaction has largely been treated as a minor issue in empirical studies. Nevertheless, it has mainly been promoted by experts as valuable asset. ""Interactivity"" and ""interaction"" are often used terms but concise definitions are hardly ever given. From the perspective of media, we might say that a paper map is static. But what if we cut and fold the paper map? Isn't that also interaction? And what if we use a paper map to find a certain street? We are obviously interacting with the world but where does the interactivity re-side? In our tools? In us human beings? Albeit it sounds easy to define interactivity, the concept quickly becomes blurry. VA experts state that interactivity is a powerful concept that enables us to improve analytical reasoning in many ways. Studies like [SNLD06] also provide some evidence for this but research on describing why and how this is the case and the mechanisms behind it, is largely missing. As Liu et al. mention, ""the field still lacks supporting, encompassing the-ories"" [LNS08]. Therefore, they call for cognitive theories and models as the basis for research. This approach shall be picked up in this paper: First, we will discuss the concept of interactivity and its different facets. Second, cognitive theories and models will be investigated. The aim of this paper is to present different views on interaction and contribute to the research towards a science of interaction. In the research and development agenda for VA [TC05] it";"0,00";"0,00";"0,00"
"TUW-197852";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\groundtruth\TUW-197852.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\result\result-TUW-197852-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\extracted\grobid\grobid-TUW-197852-xstream.xml"")";"To describe the structure of a system, the UML Class Diagram yields the means-of-choice. Therefor, the Class Diagram provides concepts like class, attribute, operation, association, generalization, aggregation, enumeration, etc. When students are introduced to this diagram, they often have to solve exercises where texts in natural language are given and they have to model the described systems. When analyzing such exercises, it becomes evident that certain kinds of phrases describing a particular concept appear again and again contextualized to the described domain.
In this paper, we present an approach which allows the automatic generation of tex-tual specifications from a given Class Diagram based on standard phrases in natural language. Besides supporting teachers in preparing exercises, such an approach is also valuable for various e-learning scenarios.";"are introduced to this diagram, they often have to solve exercises where texts in natural language are given and they have to model the described systems. When analyzing such exercises, it becomes evident that certain kinds of phrases describing a particular concept appear again and again contextualized to the described domain. In this paper, we present an approach which allows the automatic generation of tex-tual specifications from a given Class Diagram based on standard phrases in natural language. Besides supporting teachers in preparing exercises, such an approach is also valuable for various e-learning scenarios. Keywords: modeling exercises, natural language description 1 Introduction When teaching modeling, one of the most repetitive and time consuming tasks is the development of exercises and the corresponding solutions. A typical exercise consists of a given textual description of an arbitrary domain (e.g., university systems, enterprises, airports) which students have to model for example with a UML Class Diagram. In order to obtain an adequate exercise, it does not suffice to prepare the textual specification only, but also the sample solution has to be modeled for checking if all taught concepts are covered and if the difficulty level and size are reasonable. Hence, the teacher has to describe the same content twice: once as textual specification in natural language and once as a model. As there should be a one-to-one correspondence between text and model, the question at hand is if it is possible to automatically derive one artifact from the other. Already in the early 1980s attempts of automated translation of textual descriptions into program code were conducted. R.J. Abbott [Abb83] discusses a method for deriving programming concepts like data types and references by analyzing informal English descriptions. An important remark of Abbott's work relates to the automation level of such a method. He points out that such a transformation is far away from being fully automated. User interaction is still needed * Funding for this research was provided by the fFORTE WIT";"0,00";"0,00";"0,00"
"TUW-198400";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\groundtruth\TUW-198400.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\result\result-TUW-198400-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\extracted\grobid\grobid-TUW-198400-xstream.xml"")";"Although our society is critically dependent on software systems, these systems are mainly secured by protection mechanisms during operation instead of considering security issues during software design. Deficiencies in software design are the main reasons for security incidents, resulting in severe economic consequences for (i) the organizations using the software and (ii) the development companies. Lately, model-driven development has been proposed in order to increase the quality and thereby the security of software systems. This paper evaluates current efforts that position security as a fundamental element in model-driven development, highlights their deficiencies and identifies current research challenges. The evaluation shows that applying special-purpose methods to particular aspects of the problem is more suitable than applying generic ones, since (i) the problem can be represented on the proper abstraction level, (ii) the user can build on the knowledge of experts, and (iii) the available tools are more efficient and powerful.";"Although our society is critically dependent on software systems, these systems are mainly secured by protection mechanisms during operation instead of considering security issues during software design. Deficiencies in software design are the main reasons for security incidents, resulting in severe economic consequences for (i) the organizations using the software and (ii) the development companies. Lately, model-driven development has been proposed in order to increase the quality and thereby the security of software systems. This paper evaluates current efforts that position security as a fundamental element in model-driven development, highlights their deficiencies and identifies current research challenges. The evaluation shows that applying special-purpose methods to particular aspects of the problem is more suitable than applying generic ones, since (i) the problem can be represented on the proper abstraction level, (ii) the user can build on the knowledge of experts, and (iii) the available tools are more efficient and powerful. 1. Introduction As our modern society is critically dependent on software systems, the importance of software security is constantly growing [1]. For example, companies depend on applications to administer customer data, payment information and inventory tracking. But not only companies have a need for secure software: consumers also use software to communicate with friends or family, to check their banking accounts and to search for resources available on the Internet. Threats resulting from security breaches range from defeating copy protection mechanisms to attacks such as malicious intrusions into systems that control crucial infrastructure (cf. [2]). Software vulnerabilities, arising from deficiencies in the design or implementation of the software (e.g., due to increasing complexity) are one of the main reasons for security incidents (cf. [3]). These deficiencies are often caused by the increasing complexity of software systems. This is addressed with principles like abstraction, modularization, and separation of concerns, concepts which are all widely used. Although the object-oriented paradigm is mostly employed nowadays, principles like encapsulation, polymorphism, and inheritance are insufficient, and a paradigm change is necessary [4]. For this reason, as a successor of the computer-aided software engineering (CASE) approach, model-driven development (MDD) has been suggested to improve the quality of complex software systems [4], [5]. MDD is used to design abstractions, i.e., platform-independent concepts, which are then translated into more accurate ones that are adjusted to a particular platform. In a further step, such platform-specific models are transformed into production code. In such a development process, models and mappings between them have to be maintained instead of just the generated code. Aspect-oriented software development (AOSD) is an emerging approach with the goal of promoting advanced separation of concerns (cf. [6], [7]). It allows multiple concerns (e.g., security, functionality) to be expressed separately and unifies them into a working system in an automated way. Because of good characteristics in tackling software complexity, model-driven engineering was utilized to develop secure information systems. Juerjens was the first to propose a combination of model-driven development and security using UMLsec (cf. [8]). Subsequently, many proposals dealing with integrating security and modeling languages followed and were summarized under the term model-driven security (cf. [9]). It represents an approach where security is applied together with model-driven architecture [4] and focuses on building secure software systems by specifying models together with their security requirements. At the other end of the spectrum, researchers have proposed formal languages, called specification languages, to represent policies, models, and system descriptions. Such languages are based on mathematical logic systems and have also been applied to the field of information security, for instance for specifying formal security policies and for analyzing cryptographic security protocols [10]. A great number of modeling and specification approaches for describing secure information systems are available, and the question arises which method to use for which problem. When intending to apply model-driven security, or at least to";"0,00";"0,00";"0,00"
"TUW-198401";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\groundtruth\TUW-198401.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\result\result-TUW-198401-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\extracted\grobid\grobid-TUW-198401-xstream.xml"")";"IT security incidents pose a major threat to the efficient execution of corporate strategies and business processes. Although companies generally spend a lot of money on security companies are often not aware of their spending on security and even more important if these investments into security are effective. This paper provides decision makers with an overview of decision support techniques, describes pros and cons of these methodologies.";"incidents pose a major threat to the efficient execution of corporate strategies and business processes. Although companies generally spend a lot of money on security companies are often not aware of their spending on security and even more important if these investments into security are effective. This paper provides decision makers with an overview of decision support techniques, describes pros and cons of these methodologies. 1 INTRODUCTION Companies are often not aware of their spending on security and even more important if the investments into security are effective. The definition of security safeguards is often a result of current needs or influenced by security problems that may go public. When seeking to select the most appropriate set of measures and, thus, the right level of security investments, decision makers are challenged by (i) having to concentrate resources on value-generating and supplementary business processes while (ii) having to consider multiple strategic objectives that are often conflicting as well as (iii) the cost-efficient usage of the available resources and interdependencies between the systems and (iv) a variety of potential technologies and potential systems. As a consequence security decisions provide only punctual solutions and are made without considering the costs and benefits of introducing theses measures. Accordingly, a variety of approaches have been introduced that aim to support decision makers in identifying the ""right"" investment candidates. This paper provides decision makers with an overview of common methods for the evaluation and selection of security safeguards and describes pros and cons of these methodologies. One major focus of this evaluation lies on identifying the method's capabilities in (i) considering business processes for aligning expenditure to actual business needs and (ii) inte";"0,00";"0,00";"0,00"
"TUW-198405";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\groundtruth\TUW-198405.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\result\result-TUW-198405-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\extracted\grobid\grobid-TUW-198405-xstream.xml"")";"As business processes gain more importance in todays business environment, their unimpeded execution is crucial for a company's success. Corporate decision makers are faced with a wide spectrum of potential risks on the one hand and a plenitude of security safeguards on the other hand. This paper gives an overview of a new approach for the elicitation of security requirements of business processes, for the analysis of threats and vulnerabilities and for the interactive selection of an optimal security level according to the given business processes as well as multiple objectives. It provides decision makers with an instrument for interactively defining Secure Business Processes that are economically and technically efficient.";"As business processes gain more importance in todays business environment, their unimpeded execution is crucial for a company's success. Corporate decision makers are faced with a wide spectrum of potential risks on the one hand and a plenitude of security safeguards on the other hand. This paper gives an overview of a new approach for the elicitation of security requirements of business processes, for the analysis of threats and vulnerabilities and for the interactive selection of an optimal security level according to the given business processes as well as multiple objectives. It provides decision makers with an instrument for interactively defining Secure Business Processes that are economically and technically efficient. 1 Introduction As business processes play a major role in todays companies, Business Process Management (BPM) is applied to ""analyze and continually improve business activities [6], or in other words, to engineer lean and streamlined business processes. By introducing BPM, companies can gain several benefits such as cost reduction, quality improvements and error reduction, visibility gain or process automation. While BPM aims at efficiently creating business value, security hazards such as viruses, hacker attacks or data theft pose major threats to the reliable execution of business processes. Given the importance of business process and the fact that they are permanently exposed to numerous threats, the security of business processes is crucial for the business success of an enterprise, because skepticism about the security of a company would nullify the potential advantages of BPM. However, although security is considered being one of the most important issues in corporate environments, many companies are not aware of their level of spending on security and even more important if the investments into security are effective. As security does not directly generate business value and does not directly improve the net profit, investing in security can only prevent negative events or reduce related adverse effects. Dealing with security is often seen as a technical problem and the main focus in literature lies in giving technical solutions to specific security issues. Economic approaches use aggregated values as the Annualized Loss Expectancy (ALE) or Return on Investment (ROI) for choosing security improving measures. Relying solely on a single value for the measurement";"0,00";"0,00";"0,00"
"TUW-198408";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\groundtruth\TUW-198408.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\result\result-TUW-198408-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\extracted\grobid\grobid-TUW-198408-xstream.xml"")";"In der betrieblichen Praxis kommt der komponentenbasierten Software-Entwicklung hoher Stellenwert zu. Angesichts mehrfacher Zielsetzungen und vielfältiger Nebenbedingungen ist dabei insbesondere die Auswahl der ""besten"" Kombination von Komponenten ein nicht-triviales Entscheidungsproblem. Bislang wurden hierfür vor allem die Nutzwertanalyse bzw. der Analytic Hierarchy Process zur Entscheidungsunterstützung vorgeschlagen, wobei aber beide eine Reihe von Unzulänglichkeiten aufweisen. Diese Arbeit will dazu nunmehr eine Alternative anbieten. Darin werden in einem ersten Schritt zunächst (zulässige) Pareto-effiziente Kombinationen von Software-Komponenten berechnet und die Entscheidungsträger dann im zweiten Schritt interaktiv bei der Suche nach jener Variante unterstützt, die einen Ziele-Mix in Aussicht stellt, der den jeweiligen individuellen Präferenzen am besten entspricht. Das neue Verfahren zeichnet sich im Vergleich zu herkömmlichen Ansätzen insbesondere durch den Verzicht auf umfangreiche a priori Präferenzinformationen (wie z.B. Zielgewichtungen) aus. Darüber hinaus kann es ohne großen Anpassungsaufwand in bestehende Vorgehensmodelle zur Auswahl von Software-Komponenten integriert werden.";"In der betrieblichen Praxis kommt der komponentenbasierten Software-Entwicklung hoher Stel-lenwert zu. Angesichts mehrfacher Zielsetzungen und vielfältiger Nebenbedingungen ist dabei insbesondere die Auswahl der ""besten"" Kombination von Komponenten ein nicht-triviales Ent-scheidungsproblem. Bislang wurden hierfür vor allem die Nutzwertanalyse bzw. der Analytic Hierarchy Process zur Entscheidungsunterstützung vorgeschlagen, wobei aber beide eine Reihe von Unzulänglichkeiten aufweisen. Diese Arbeit will dazu nunmehr eine Alternative anbieten. Darin werden in einem ersten Schritt zunächst (zulässige) Pareto-effiziente Kombinationen von Software-Komponenten berechnet und die Entscheidungsträger dann im zweiten Schritt inter-aktiv bei der Suche nach jener Variante unterstützt, die einen Ziele-Mix in Aussicht stellt, der den jeweiligen individuellen Präferenzen am besten entspricht. Das neue Verfahren zeichnet sich im Vergleich zu herkömmlichen Ansätzen insbesondere durch den Verzicht auf umfang-reiche a priori Präferenzinformationen (wie z.B. Zielgewichtungen) aus. Darüber hinaus kann es ohne großen Anpassungsaufwand in bestehende Vorgehensmodelle zur Auswahl von Software-Komponenten integriert werden. 1 Einleitung Die komponentenbasierte Software-Entwicklung unterscheidet sich vom traditionellen Vorge-hen insbesondere dadurch, dass (bestehende) Komponenten als Grundlage für die Entwicklung komplexer Softwarelösungen genutzt werden. Tatsächlich kommt ihr heutzutage hoher Stellen-wert zu [Ruhe2002], da in immer kürzeren Abständen qualitativ hochwertige und zuverlässige Software auf den Markt gebracht werden muss. Zudem steigen die funktionalen Anforderungen an Software, so dass insbesondere kleinere Unternehmen bei der Erfüllung ihrer Aufträge zunehmend davon abhängig sind, auf vorhandene Komponenten zurückgreifen zu können und das Produkt nicht in allen Details von Grund auf neu entwickeln zu müssen [Alves2003]. Empirische Studien [Alves2003, Basili1996] zeigen ferner, dass durch den Rückgriff auf bewährte Komponenten Fehler im Gesamtsystem wesentlich reduziert werden. Und schließlich wird die komponentenbasierte Software-Entwicklung auch durch Technologien wie CORBA, JavaBeans/EJB, DCOM/ActiveX oder .Net sowie die Verfügbarkeit von verschiedenen Tools für die Konfiguration und den Einsatz solcher Lösungen vorangetrieben [Andrews2005]. Allerdings müssen Software-Produkte in der Regel an spezifische Anforderungen angepasst werden. Dementsprechend spielt bei der komponentenbasierten Software-Entwicklung die Aus";"0,00";"0,00";"0,00"
"TUW-200745";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\groundtruth\TUW-200745.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\result\result-TUW-200745-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\extracted\grobid\grobid-TUW-200745-xstream.xml"")";"In this work, an algorithm for the generalized minimum spanning tree problem (GMST) is developed. Given is a complete graph where the nodes are partitioned into clusters. A solution is a spanning tree which contains exactly one node of each cluster and its costs are minimal. This problem is NP-hard. In this work, a heuristic is developed for this problem.
	
In this method, an evolutionary algorithm (EA) is used with two different solution archives. Using a solution archive, it is possible to store solutions generated by the EA in order to detect duplicates and converts duplicate solutions into new solutions. One solution archive based on an encoding in which the spanned nodes of each cluster in the solution are stored. The other archive is based on an encoding which characterizes the connections between the clusters.

These archives are extended by a bounding strategy based on the branch-and-bound technique. They try to calculate appropriate bounds at a convenient positions which give information about how good the solutions in the respective area of the archive can be in the best case. If a bound was found which is worse than the best known solution, the solutions are unattractive in the course of the algorithm and will not be considered. Therefore inferior solutions can be detected at an early stage and only promising solutions that can bring improvements will be pursued.

In addition to the bounding strategy a nearest neighbor approach is implemented in which a cluster attached to the spanning tree is preferred among the the n nearest neighboring clusters.

Tests were carried out in which the bounding strategy was used in the different variants. These tests led to the conclusion that the bounding strategy leads to an improvement in comparison to the ""normal"" archives. The comparison between the archives shows that the pop version lead to better results than the gosh version. When both archives are used simultaneously, the results are better than the results of the other two variants.";"Kurzfassung In dieser Arbeit wird ein Algorithmus für das Generalized Minimum Spanning Tree-Problem (GMST) entwickelt. Beim GMST-Problem ist ein vollständiger Graph gegeben, bei dem die Knoten in Cluster partitioniert sind. Als Lösung wird ein Spannbaum gesucht, der von jedem Cluster genau einen Knoten beinhaltet und dessen Kosten minimal sind. Dieses Problem ist NP-schwierig. In dieser Arbeit wird eine Heuristik für dieses Problem entwickelt. Bei diesem Verfahren wird ein Evolutionärer Algorithmus (EA) mit zwei ver-schiedenen Lösungsarchiven verwendet. Die Lösungsarchive werden dazu benutzt Lösungen zu speichern, um Duplikate zu erkennen und diese in neue Lösungen umzuwandeln. Das eine Lösungsarchiv beruht auf einer Kodierung, bei der die ausgewählten Knoten der Cluster einer Lösung gespeichert werden, während das andere Archiv auf einer Kodierung beruht, bei der gespeichert wird, welche Cluster in der Lösung verbunden sind. Diese Archive werden in dieser Arbeit durch eine Bounding-Strategie basierend auf dem Branch and Bound Verfahren erweitert. Dabei wird versucht im Archiv an günstigen Positionen geeignete Bounds zu berechnen, die Auskunft darüber geben, wie gut die Lösungen in diesem Bereich des Archivs höchstens sein können. Wird eine Bound gefunden, die schlechter als die beste gefunden Lösung ist, sind diese Lösungen im weiteren Verlauf des Algorithmus uninteressant und werden nicht mehr berücksichtigt. Das führt dazu, dass mehrere Lösungen von vornherein als schlecht erkannt werden können und somit nur Lösungen verfolgt werden, die auch Verbesserungen bringen können. Zusätzlich zu der Bounding-Strategie wird auch noch ein Nearest Neighbour Ansatz verwendet, bei dem beim Anhängen eines Clusters an den Spannbaum die n näch-sten Nachbarcluster bevorzugt werden. Am Ende der Arbeit wurden Tests durchgeführt, bei denen die Bounding Strate-gie in den unterschiedlichen Archiven verwendet wurde. Diese Tests führten zu dem Ergebnis, dass die Bounding Strategie zu einer Verbesserung gegenüber den Archiven ohne Bounding Strategie führt. Der Vergleich zwischen den Archiven hat ergeben, dass die Pop-Variante bessere Ergebnisse liefert als die Gosh-Variante. Die Variante, in der beide Archive gleichzeitig verwendet werden, ist wiederum besser als die anderen beiden Varianten. iii Abstract In this work, an algorithm for the generalized minimum spanning tree problem (GMST) is developed. Given is a complete graph where the nodes are partitioned into clusters. A solution is a spanning tree which contains exactly one node of each cluster and its costs are minimal. This problem is NP-hard. In this work, a heuristic is developed for this problem. In this method, an evolutionary algorithm (EA) is used with two different solution archives. Using a solution archive, it is possible to store solutions generated by the EA in order to detect duplicates and converts duplicate solutions into new solutions. One solution archive based on an encoding in which the spanned nodes of each cluster in the solution are stored. The other archive is based on an encoding which characterizes the connections between the clusters. These archives are extended by a bounding strategy based on the branch-and-bound technique. They try to calculate appropriate bounds at a convenient positions which give information about how good the solutions in the respective area of the archive can be in the best case. If a bound was found which is worse than the best known solution, the solutions are unattractive in the course of the algorithm and will not be considered. Therefore inferior solutions can be detected at an early stage and only promising solutions that can bring improvements will be pursued. In addition to the bounding strategy a nearest neighbor approach is implemented in which a cluster attached to the spanning tree is preferred among the the n nearest neighboring clusters. Tests were carried out in which the bounding strategy was used in the different variants. These tests led to the conclusion that the bounding strategy leads to an improvement in comparison to the ""normal"" archives. The comparison between the archives shows that the pop version lead to better results than the gosh version. When both archives are used simultaneously, the results are better than the results of the other two variants. iv";"0,00";"0,00";"0,00"
"TUW-200748";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\groundtruth\TUW-200748.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\result\result-TUW-200748-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\extracted\grobid\grobid-TUW-200748-xstream.xml"")";"The Rooted Delay-Constrained Steiner Tree Problem (RDCSTP) is a variant of the well-known Steiner Tree Problem on a graph in which the paths to all terminal nodes are restricted by a certain maximum delay. The problem mostly appears in the context of network routing for multicasts, i.e., sending packages from a fixed source to a subset of other participants in the network. Since the RDCSTP belongs to the class of N P-hard problems it is in general not possible to solve large instances exactly in a reasonable amount of time. Therefore, the focus mostly lies on developing good heuristics that can still solve large instances comparatively fast to near optimality.
In this thesis a Multilevel Refinement heuristic – which has already been successfully applied to other problems like the Graph Partitioning Problem – is implemented as an improvement heuristic for the RDCSTP. In the general approach of this metaheuristic the problem's complexity is first iteratively reduced while still maintaining its general characteristics. The problem is thereby simplified and can at the top level finally easily be solved. Then, the solution on this highest level is refined until a solution for the original problem is obtained.
The algorithm introduced here implements the Multilevel Refinement approach as an improvement heuristic, iteratively changing an existing solution. However, it is designed in a way that also allows it to be used to construct an initial solution. Another distinctiveness is that, due to the additional delay constraints, supplementary data structures have to be used to avoid creating invalid solutions on higher levels as much as possible. In the refinement phase an additional improvement algorithm, the Key Path Improvement, is executed on each level, drastically increasing result quality.
Experimental tests are carried out, evaluating the performance of the algorithm on large instances and comparing it to other algorithms in the literature. The obtained results are promising and indicate that the Multilevel Refinement metaheuristic is indeed a competitive approach for the RDCSTP.";"Acknowledgements I would like to thank my advisor, Prof. Dr. Günther Raidl, for letting me work on this thesis and for his help and suggestions with creating it. I also thank the Vienna University of Technology for the years of education I received there, and for the prolific scientific environment it provided. My special thanks go to my mentor for this thesis, Dipl.-Ing. Mario Ruthmair. Without his countless suggestions, our numerous discussions, his help and his thorough reviews, this thesis would never have been completed. Lastly, I want to sincerely thank my parents, who supported me in every possible way throughout my education and without whom none of this would have been possible. iii Abstract The Rooted Delay-Constrained Steiner Tree Problem (RDCSTP) is a variant of the well-known Steiner Tree Problem on a graph in which the paths to all terminal nodes are restricted by a certain maximum delay. The problem mostly appears in the context of network routing for multicasts, i.e., sending packages from a fixed source to a subset of other participants in the network. Since the RDCSTP belongs to the class of N P-hard problems it is in general not possible to solve large instances exactly in a reasonable amount of time. Therefore, the focus mostly lies on developing good heuristics that can still solve large instances comparatively fast to near optimality. In this thesis a Multilevel Refinement heuristic-which has already been successfully applied to other problems like the Graph Partitioning Problem-is implemented as an improvement heuristic for the RDCSTP. In the general approach of this metaheuristic the problem's complexity is first iteratively reduced while still maintaining its general characteristics. The problem is thereby simplified and can at the top level finally easily be solved. Then, the solution on this highest level is refined until a solution for the original problem is obtained. The algorithm introduced here implements the Multilevel Refinement approach as an improvement heuristic, iteratively changing an existing solution. However, it is designed in a way that also allows it to be used to construct an initial solution. Another distinctiveness is that, due to the additional delay constraints, supplementary data structures have to be used to avoid creating invalid solutions on higher levels as much as possible. In the refinement phase an additional improvement algorithm, the Key Path Improvement, is executed on each level, drastically increasing result quality. Experimental tests are carried out, evaluating the performance of the algorithm on large instances and comparing it to other algorithms in the literature. The obtained results are promising and indicate that the Multilevel Refinement metaheuristic is indeed a competitive approach for the RDCSTP. v Kurzfassung Das Rooted Delay-Constrained Steiner Tree Problem (RDCSTP) ist eine Variante des bekannten Steinerbaum-Problems auf einem Graphen in welcher die Pfade zu allen Zielknoten durch eine bestimmte maximale Verzögerung beschränkt sind. Das Problem tritt hauptsächlich im Bereich des Netzwerk-Routings beim Multicast auf, das heißt wenn Pakete von einer einzelnen Quelle zu einer bestimmten Untermenge der anderen Netzwerk-Teilnehmer gesendet werden sollen. Da das RDCSTP, wie das ursprüngliche Steiner-Problem, zur Klasse der N P-schwierigen Probleme gehört, ist es allgemein nicht möglich die exakte Lösung einer großen Probleminstanz in vertret-barer Zeit zu finden. Der Fokus der Forschung liegt daher großteils auf der Entwicklung guter Heuristiken, die auch bei großen Probleminstanzen in der Lage sind in vergleichbar kurzer Zeit zu möglichst guten Lösungen zu kommen. In dieser Arbeit wird hierfür die Multilevel-Refinement-Heuristik-die bereits erfolgreich auf etliche andere Probleme, wie das Graph Partitioning Problem, angewandt wurde-als Ver-besserungsheuristik für das RDCSTP entwickelt. Grundsätzlich werden bei dieser Metaheuristik in einem ersten Schritt Knoten sukzessive zusammengefasst um den Graphen auf höheren ""Lev-els"", mit weniger Knoten, darzustellen. Das so vereinfachte Problem kann dann auf der höchsten Abstraktionsebene in simpler Weise gelöst werden. Dann wird diese Lösung schrittweise wieder soweit verfeinert, bis eine Lösung für das ursprüngliche Problem erreicht wird. Der hier vorgestellte Algorithmus für das RDCSTP implementiert diesen Multilevel-Ansatz als Verbesserungsheuristik, die eine existierende Lösung iterativ verändert. Er wurde allerdings in einer Weise entworfen, die es ihm ebenso erlaubt eine Anfangslösung selbst zu generieren. Eine weitere Besonderheit ist, dass wegen der zusätzlichen Verzögerungs-Einschränkung wei-tere Datenstrukturen benötigt werden, um auf höheren Levels möglichst gültige Lösungen zu erzeugen. Außerdem wird während der Verfeinerung der Lösung auf jedem Level eine weite-re Verbesserungsheuristik angewandt, das Key Path Improvement, welches die Lösungsqualität drastisch verbessert. Umfangreiche experimentelle Tests wurden durchgeführt um die Leistungsfähigkeit des Al-gorithmus bei großen Instanzen zu messen, und ihn mit anderen Algorithmen aus der Literatur zu vergleichen. Die hierbei erhaltenen Ergebnisse sind durchwegs sehr positiv und weisen somit darauf hin, dass der verfolgte Multilevel-Ansatz tatsächlich eine konkurrenzfähige Heuristik für das RDCSTP darstellt.";"0,00";"0,00";"0,00"
"TUW-200948";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\groundtruth\TUW-200948.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\result\result-TUW-200948-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\extracted\grobid\grobid-TUW-200948-xstream.xml"")";"We introduce a new approach for establishing fixed-parameter tractability of problems parameterized above tight lower bounds or below tight upper bounds. To illustrate the approach we consider two problems of this type of unknown complexity that were introduced by Mahajan, Raman and Sikdar (J. Comput. Syst. Sci. 75, 2009). We show that a generalization of one of the problems and three nontrivial special cases of the other problem admit kernels of quadratic size.";"We introduce a new approach for establishing fixed-parameter tractability of problems parameterized above tight lower bounds or below tight upper bounds. To illustrate the approach we consider two problems of this type of unknown complexity that were introduced by Mahajan, Raman and Sikdar (J. Comput. Syst. Sci. 75, 2009). We show that a generalization of one of the problems and three nontrivial special cases of the other problem admit kernels of quadratic size. 1 Introduction A parameterized problem ? can be considered as a set of pairs (I, k) where I is the main part and k (usually an integer) is the parameter. ? is called fixed-parameter tractable (FPT) if membership of (I, k) in ? can be decided in time O(f (k)|I| c), where |I| denotes the size of I, f (k) is a computable function, and c is a constant independent of k and I (for further background and terminology on parameterized complexity we refer the reader to the monographs [7, 8, 16]). If the nonparameterized version of ? (where k is just a part of the input) is NP-hard, then the function f (k) must be superpolynomial provided P = NP. Often f (k) is ""moderately exponential,"" which makes the problem practically feasible for small values of k. Thus, it is important to parameterize a problem in such a way that the instances with small values of k are of real interest. Consider the following well-known problem: given a digraph D = (V, A), find an acyclic subdigraph of D with the maximum number of arcs. We can parameterize this problem ""naturally"" by asking whether D contains an acyclic subdigraph with at least k arcs. It is easy to prove that this parameterized problem is fixed-parameter tractable by observing that D always has an acyclic subdigraph with at least |A|/2 arcs. Both subdigraphs are acyclic and at least one of them has at least |A|/2 arcs.) However, k ? |A|/2 for every small value of k and almost every practical value of |A| and, thus, our ""natural"" parameterization is of almost no practical or theoretical interest. Instead, one should consider the following parameterized problem: decide whether D = (V, A) contains an acyclic subdigraph with at least |A|/2 + k arcs. We choose |A|/2 + k because |A|/2 is a tight lower bound on the size of a largest acyclic subdigraph. Indeed, the size of a largest acyclic subdigraph of a symmetric digraph In a recent paper [15] Mahajan, Raman and Sikdar provided several examples of problems of this type and argued that a natural parameterization is one above a tight lower bound for maximization problems, and below a tight upper bound for minimization problems. Furthermore, they observed that only a few non-trivial results are known for problems parameterized above a tight lower bound [10, 11, 13, 14], and they listed several problems parameterized above a tight lower bound whose complexity is unknown. The difficulty in showing whether such a problem is fixed-parameter tractable can be illustrated by the fact that often we even do not know whether the problem is in XP, i.e., can be solved in time O(|I| g(k)) for a computable function g(k). For example, it is non-trivial to see that the above-mentioned digraph problem is in XP when parameterized above the |A|/2 bound.";"0,00";"0,00";"0,00"
"TUW-200950";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\groundtruth\TUW-200950.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\result\result-TUW-200950-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\extracted\grobid\grobid-TUW-200950-xstream.xml"")";"We study the complexity of several coloring problems on graphs, pa-rameterized by the treewidth t of the graph:
(1) The list chromatic number ?l(G) of a graph G is defined to be the smallest positive integer r, such that for every assignment to the vertices v of G, of a list Lv of colors, where each list has length at least r, there is a choice of one color from each vertex list Lv yielding a proper coloring of G. We show that the problem of determining whether ?l(G) ? r, the LIST CHROMATIC NUMBER problem, is solvable in linear time for every fixed treewidth bound t. The method by which this is shown is new and of general applicability.
(2) The LIST COLORING problem takes as input a graph G, together with an assignment to each vertex v of a set of colors Cv. The problem is to determine whether it is possible to choose a color for vertex v from the set of permitted colors Cv, for each vertex, so that the obtained coloring of G is proper. We show that this problem is W [1]-hard, parameterized by the treewidth of G. The closely related PRECOLORING EXTENSION problem is also shown to be W [1]-hard, pa-rameterized by treewidth.
(3) An equitable coloring of a graph G is a proper coloring of the vertices where the numbers of vertices having any two distinct colors differs by at most one. We show that the problem is hard for W [1], parameterized by (t, r). We also show that a list-based variation, LIST EQUITABLE COLORING is W [1]-hard for trees, parameterized by the number of colors on the lists.";"the numbers of vertices having any two distinct colors differs by at most one. We show that the problem is hard for W [1], parameterized by (t, r). We also show that a list-based variation, LIST EQUITABLE COLORING is W [1]-hard for trees, parameterized by the number of colors on the lists. Topics: Parameterized Complexity, Bounded Treewidth, Graph Coloring This research has been supported by the Australian Research Council through the Australian Centre in Bioinformatics. The first author also acknowledges the support provided by a Fellowship to the Institute of Advanced Studies, Durham University, and the support of the Infor-matics Institute at the University of Bergen during an extended visit. 1 Introduction Coloring problems that involve local and global restrictions on the coloring have many important applications in such areas as operations research, scheduling and computational biology, and also have a long mathematical history. For recent surveys of the area one can turn to [Tu97,KTV98,Al00,Wo01] and also the book [JT95]. In this paper we study the computational complexity of such problems, for graphs of bounded treewidth, in the framework of parameterized complexity [DF99,Nie06], where we take the parameter to be the treewidth bound t. Our main results are summarized:-We show that the list chromatic number (also known as the choice number [KTV98]) of a graph can be computed in linear time for any fixed treewidth bound t. (We prove this using a new ""trick"" for extending the applicability of Monadic Second Order logic that is of general interest.";"0,00";"0,00";"0,00"
"TUW-200959";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\groundtruth\TUW-200959.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\result\result-TUW-200959-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\extracted\grobid\grobid-TUW-200959-xstream.xml"")";"We consider monotonicity problems for graph searching games. Variants of these games – defined by the type of moves allowed for the players – have been found to be closely connected to graph decompositions and associated width measures such as path-or tree-width.
Of particular interest is the question whether these games are monotone, i.e. whether the cops can catch a robber without ever allowing the robber to reach positions that have been cleared before. The monotonicity problem for graph searching games has intensely been studied in the literature, but for two types of games the problem was left unresolved. These are the games on digraphs where the robber is invisible and lazy or visible and fast. In this paper, we solve the problems by giving examples showing that both types of games are non-monotone.
Graph searching games on digraphs are closely related to recent proposals for digraph decom-positions generalising tree-width to directed graphs. These proposals have partly been motivated by attempts to develop a structure theory for digraphs similar to the graph minor theory developed by Robertson and Seymour for undirected graphs, and partly by the immense number of algorith-mic results using tree-width of undirected graphs and the hope that part of this success might be reproducible on digraphs using a ""directed tree-width"". For problems such as disjoint paths and Hamiltonicity, it has indeed been shown that they are tractable on graphs of small directed tree-width. However, the number of such examples is still small.
We therefore explore the limits of the algorithmic applicability of digraph decompositions. In particular, we show that various natural candidates for problems that might benefit from digraphs having small ""directed tree-width"" remain NP-complete even on almost acyclic graphs.";"arXiv:0802.2228v1 [cs.DM] 15 Feb 2008 We consider monotonicity problems for graph searching games. Variants of these games-defined by the type of moves allowed for the players-have been found to be closely connected to graph decompositions and associated width measures such as path-or tree-width. Of particular interest is the question whether these games are monotone, i.e. whether the cops can catch a robber without ever allowing the robber to reach positions that have been cleared before. The monotonicity problem for graph searching games has intensely been studied in the literature, but for two types of games the problem was left unresolved. These are the games on digraphs where the robber is invisible and lazy or visible and fast. In this paper, we solve the problems by giving examples showing that both types of games are non-monotone. Graph searching games on digraphs are closely related to recent proposals for digraph decom-positions generalising tree-width to directed graphs. These proposals have partly been motivated by attempts to develop a structure theory for digraphs similar to the graph minor theory developed by Robertson and Seymour for undirected graphs, and partly by the immense number of algorith-mic results using tree-width of undirected graphs and the hope that part of this success might be reproducible on digraphs using a ""directed tree-width"". For problems such as disjoint paths and Hamiltonicity, it has indeed been shown that they are tractable on graphs of small directed tree-width. However, the number of such examples is still small. We therefore explore the limits of the algorithmic applicability of digraph decompositions. In particular, we show that various natural candidates for problems that might benefit from digraphs having small ""directed tree-width"" remain NP-complete even on almost acyclic graphs. 1 Introduction The seminal work of Robertson and Seymour in their graph minor project has focused much attention on graph decompositions and associated measures of graph connectivity such as tree-or path-width. Aside from the interest in graph structure theory, these notions have also proved fruitful in the development of algorithms. The tree-width of a graph is a measure of how tree-like the graph is and small tree-width allows for graph decompositions along which recursive algorithms can work. Many problems that are intractable in general can be solved efficiently on graphs of bounded tree-width. These include such classical NP-complete problems as finding a Hamiltonian-cycle in a graph or detecting if a graph is three-colourable. See [9, 8] and references therein for an introduction to tree-width. Closely related to the theory of graph decompositions is the theory of graph searching games. In a graph searching game a number of searchers, or cops, tries to catch a fugitive, or robber, hiding in the graph. There are many variants of these games. The robber can hide on edges or vertices, he can be fast or lazy, he can be visible or not, the game can be played on undirected or directed graphs, and many more. Graph searching games are particularly interesting in relation to graph decompositions, as many width measures for graphs based on decompositions can also be described in terms of variants of Cops and Robber games. For instance, in 1993, Seymour and Thomas [12] showed that the tree-width of a graph equals the minimal number of cops required to catch a visible and fast robber (minus one). Dendris, Kirousis, and Thilikos [10] gave an analogous characterisation in terms of an invisible, lazy robber game. Other variants of Cops and Robber games have also been used to characterise the path-width of graphs and similar connectivity measures. An important concept in the theory of graph searching games is monotonicity. A game is monotone , if whenever k cops can catch a robber on a graph they can do so without allowing the robber to re-occupy vertices. In general, restricting the cops to monotone strategies may require additional cops to catch a robber. LaPaugh [20] gave a first proof of monotonicity for a Cops and Robber game. Since then, monotonicity has been intensely studied and a large number of monotonicity results have been established. See e.g. [20, 7, 10, 4, 13, 14, 21, 27] or the survey [2] and references therein. Despite the considerable interest and the large number of results in this field, two cases have so far resisted any attempts to solve the monotonicity problem-the Cops and Robber game with a visible, fast robber and the game with an invisible, lazy robber, both played on digraphs. In this paper,we solve the problems by showing that both games are non-monotone. Digraph decompositions. In recent years, attempts have been made to generalise the notion of tree-decompositions and their algorithmic applications to directed graphs. Clearly, we can define the tree-width of a directed graph as the tree-width of the undirected graph we get by forgetting the direction of edges, a process which leads to some loss of information. This loss may be significant, if the algorithmic problems we are interested in are inherently directed. A good example is the problem of detecting Hamiltonian cycles. While we know that this can be solved easily on graphs with small tree-width, there are directed graphs with very simple connectivity structure which have large tree-width. Therefore, several proposals have been made to extend the notions of tree-decompositions and tree-width to directed graphs (see [24, 17, 4, 6, 25, 16]). In particular, Reed [24] and Johnson, Robertson, Seymour, and Thomas [17] introduce the notion of directed tree-width and they show that Hamiltonicity can be solved for graphs of bounded directed tree-width in polynomial time. Following this initial paper, several alternative definitions of directed graph decompositions have been proposed, with the aim of overcoming some shortcomings of the original definition. Obdržàlek [23] and Berwanger, Dawar, Hunter, and Kreutzer [5] introduce the notion of DAG-width and Hunter and Kreutzer [16] introduce the notion of Kelly-width. All three proposals are supported by algo-rithmic applications and various equivalent characterisations in terms of obstructions, elimination orderings, and, in particular, variants of Cops and Robber games on directed graphs. However, so";"0,00";"0,00";"0,00"
"TUW-201066";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\groundtruth\TUW-201066.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\result\result-TUW-201066-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\extracted\grobid\grobid-TUW-201066-xstream.xml"")";;"The rooted delay-constrained minimum spanning tree (RDCMST) problem is an NP-hard [3] combinatorial optimization problem. The objective is to find a minimum cost spanning tree in a given graph with cost and delay values assigned to each edge. Additionally, a delay-bound is given limiting the maximum delay allowed for each path between a specified root vertex and any other vertex in the graph. This problem appears in practice for example when designing";"0,00";"none expected";"0,00"
"TUW-201160";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\groundtruth\TUW-201160.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\result\result-TUW-201160-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\extracted\grobid\grobid-TUW-201160-xstream.xml"")";;"hard combinatorial optimization problem which arises in the design of last mile communication networks (fiber-to-the-curb scenarios) [1]. Formally, we are given an undirected, weighted graph G = (V, E), with edge costs c e ? 0, ?e ? E. The node set V = {r}?F ?T is the disjoint union of the root node r, potential facility locations F , and possible Steiner nodes T. Each facility i ? F has associated opening costs f i ? 0 and a maximum assignable capacity D i ? N. Furthermore, we are given a set of potential customers C, with individual capacity demands d k ? N and prizes p k ? 0, ?k ? C, the latter corresponding to the expected profit when supplying customer k. Each customer k ? C may be assigned to one facility of a subset F k ? F , with assignment costs a the set of opened facilities F S ? F and the root node r. C S ? C is the set of customers feasibly (i.e. respecting the capacity constraints) assigned to facilities F S , whereas the actual mapping between customers and facilities is described by ? S : C S ? F S. The objective value of a feasible solution S is given by c(S) = e?TS c e + i?FS f i + k?CS a ?S(k),k + k?C\CS p k , and we aim to identify a most profitable solution minimizing this function. This variant of CConFL has already been tackled by exact methods based on mixed integer programming [2] and hybrid approaches based on Lagrangian relaxation [1]. Here, we present the first pure metaheuristic approach, which computes high";"0,00";"none expected";"0,00"
"TUW-201167";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\groundtruth\TUW-201167.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\result\result-TUW-201167-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\extracted\grobid\grobid-TUW-201167-xstream.xml"")";;"When designing a communication network with a central server broadcasting information to all the participants of the network, some applications, such as video conferences, require a limitation of the maximal delay from the server to each client. Beside this delay-constraint minimizing the cost of establishing the network is in most cases an important design criterion. This network design problem can be modeled as an N P-hard combinatorial optimization problem called rooted delay-constrained minimum spanning tree (RDCMST) problem. The objective is to find a minimum cost spanning tree of a given graph with the additional constraint that the sum of delays along the paths from a specified root node to any other node must not exceed a given delay-bound. More formally, we are given an undirected graph G = (V, E) with a set V of n nodes, a set E of m edges, a cost function c : E ? R";"0,00";"none expected";"0,00"
"TUW-201821";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\groundtruth\TUW-201821.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\result\result-TUW-201821-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\extracted\grobid\grobid-TUW-201821-xstream.xml"")";"Multi-Context Systems are an expressive formalism to model (possibly) non-monotonic information exchange between heterogeneous knowledge bases. Such information exchange, however, often comes with unforseen side-effects leading to violation of constraints, making the system inconsistent, and thus unusable. Although there are many approaches to assess and repair a single inconsistent knowledge base, the heterogeneous nature of Multi-Context Systems poses problems which have not yet been addressed in a satisfying way: How to identify and explain a inconsistency that spreads over multiple knowledge bases with different logical formalisms (e.g., logic programs and ontologies)? What are the causes of inconsistency if inference/information exchange is non-monotonic (e.g., absent information as cause)? How to deal with inconsistency if access to knowledge bases is restricted (e.g., companies exchange information, but do not allow arbitrary modifications to their knowledge bases)? Many traditional approaches solely aim for a consistent system, but automatic removal of inconsistency is not always desireable. Therefore a human operator has to be supported in finding the erroneous parts contributing to the inconsistency. In my thesis those issues will be adressed mainly from a foundational perspective, while our research project also provides algorithms and prototype implementations.";"Multi-Context Systems are an expressive formalism to model (possibly) non-monotonic information exchange between heterogeneous knowledge bases. Such information exchange, however, often comes with unforseen side-effects leading to violation of constraints, making the system inconsistent, and thus unusable. Although there are many approaches to assess and repair a single inconsistent knowledge base, the heterogeneous nature of Multi-Context Systems poses problems which have not yet been addressed in a satisfying way: How to identify and explain a inconsistency that spreads over multiple knowledge bases with different logical formalisms (e.g., logic programs and ontologies)? What are the causes of inconsistency if inference/information exchange is non-monotonic (e.g., absent information as cause)? How to deal with inconsistency if access to knowledge bases is restricted (e.g., companies exchange information, but do not allow arbitrary modifications to their knowledge bases)? Many traditional approaches solely aim for a consistent system, but automatic removal of inconsistency is not always desireable. Therefore a human operator has to be supported in finding the erroneous parts contributing to the inconsistency. In my thesis those issues will be adressed mainly from a foundational perspective, while our research project also provides algorithms and prototype implementations. 1 Introduction Multi-Context Systems (MCSs) are an expressive formalism for (possibly) non-monotonic knowledge exchange between heterogeneous knowledge sources. These sources are called contexts and formalized as abstract 'logics'. Information flow between contexts is specified using bridge rules which look and behave similar to rules in non-monotonic logic programming (Such a rule states that information s is added to context k if for 1 ? i ? j knowledge p i is present in context c i and for j + 1 ? i ? m knowledge p i is";"0,00";"0,00";"0,00"
"TUW-202034";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\groundtruth\TUW-202034.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\result\result-TUW-202034-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\extracted\grobid\grobid-TUW-202034-xstream.xml"")";"In this thesis the application of clustering algorithms for solving the Hierarchical Ring Network Problem (HRNP) is investigated.
When the network is represented as a graph, an informal problem definition for this NP-complete problem is: Given a set of network sites (nodes) assigned to one of three layers and the costs for establishing connections between sites (i.e., edge costs) the objective is to find a minimum cost connected network under certain constraints that are explained in detail in the thesis. The most important constraint is that the nodes have to be assigned to rings of bounded size that connect the layers hierarchically.
The ring structure is a good compromise between the robustness of a network and the cost for establishing it. It is guaranteed, that the network can continue to provide its service if one network node per ring fails.
The basic idea in this thesis for solving this network design problem was to cluster the sites with hierarchical clustering heuristics and to use the resulting hierarchy as support for the ring-finding heuristics. Previous apporaches for related network design problems did not use the inherent network structure in such a way. Usual approaches are based on greedy heuristics.
Three clustering heuristics were implemented: Girvan-Newman, K-means and Kernighan-Lin. Especially the first algorithm is interesting, because it was successfully applied analyzing large network structures, also in the context of internet communities.
For finding rings three heuristics were implemented too. Strategic variation of the maximum allowed ring size helps the first heuristic to find rings using the cluster hierarchy. The second heuristic finds rings by searching for paths that are connected to previously found rings. Third a repair heuristic was implemented that tries to add remaining nodes to existing rings. Local search heuristics are applied last to improve the solution quality.
To check how the clustering approach performs for solving the problem of this thesis two test instance generators were implemented. One generates instances randomly and the second generates instances based on the popular TSPLIB archive.
The evaluation of the random test instances has shown, that all three clustering heuristics were able to solve those test instances, while Girvan-Newman and Kernighan-Lin found valid solutions in each test run this was not possible for K-means. When Kernighan-Lin was used as clustering algorithm solutions could be found faster on average, but the resulting costs where slightly higher. For the TSPLIB based instances the clustering algorithms had more problems to find valid solutions, but for each test instance at least one type of clustering was successful.";"Acknowledgements I want to thank my advisors ao.Univ.-Prof. Dipl.-Ing. Dr. techn. Günther Raidl and Univ.-Ass. Dipl.-Ing. Christian Schauer. Their constructive feedback and their experience was a big help for writing and improving this thesis. Special thanks goes to my family and my friends. Studying can be quite time-consuming and stressful sometimes and their support is invaluable. ii Abstract In this thesis the application of clustering algorithms for solving the Hierarchical Ring Network Problem (HRNP) is investigated. When the network is represented as a graph, an informal problem definition for this NP-complete problem is: Given a set of network sites (nodes) assigned to one of three layers and the costs for establishing connections between sites (i.e., edge costs) the objective is to find a minimum cost connected network under certain constraints that are explained in detail in the thesis. The most important constraint is that the nodes have to be assigned to rings of bounded size that connect the layers hierarchically. The ring structure is a good compromise between the robustness of a network and the cost for establishing it. It is guaranteed, that the network can continue to provide its service if one network node per ring fails. The basic idea in this thesis for solving this network design problem was to cluster the sites with hierarchical clustering heuristics and to use the resulting hierarchy as support for the ring-finding heuristics. Previous apporaches for related network design problems did not use the inherent network structure in such a way. Usual approaches are based on greedy heuristics. Three clustering heuristics were implemented: Girvan-Newman, K-means and Kernighan-Lin. Especially the first algorithm is interesting, because it was successfully applied analyzing large network structures, also in the context of internet communities. For finding rings three heuristics were implemented too. Strategic variation of the maximum allowed ring size helps the first heuristic to find rings using the cluster hierarchy. The second heuristic finds rings by searching for paths that are connected to previously found rings. Third a repair heuristic was implemented that tries to add remaining nodes to existing rings. Local search heuristics are applied last to improve the solution quality. To check how the clustering approach performs for solving the problem of this thesis two test instance generators were implemented. One generates instances randomly and the second generates instances based on the popular TSPLIB archive. The evaluation of the random test instances has shown, that all three clustering heuristics were able to solve those test instances, while Girvan-Newman and Kernighan-Lin found valid solutions in each test run this was not possible for K-means. When Kernighan-Lin was used as clustering algorithm solutions could be found faster on average, but the resulting costs where slightly higher. For the TSPLIB based instances the clustering algorithms had more problems to find valid solutions, but for each test instance at least one type of clustering was successful. iii Kurzfassung In dieser Diplomarbeit wird die Anwendung von Clusteringalgorithmen untersucht, um das Hierarchical Ring Network Problem (HRNP) zu lösen. Wenn das Netzwerk als Graph repräsentiert ist, ist dieses NP-vollständige Problem wie folgt definiert: Gegeben ist Menge von Knoten welche jeweils einer von drei Schichten zugewiesen sind, und eine Kostenfunktion, welche die Verbindungskosten zwischen zwei Knoten (d.h. Kantenkosten) zuweist. Gesucht ist ein zusammenhängendes Netzwerk mit minimalen Gesamtkosten, wobei dieses bestimmte Struktureigenschaften zu erfüllen hat, welche im Detail in der Diplomarbeit beschrieben werden. Die wichtigste dieser Eigenschaften ist, dass Knoten gemäß einer hierarchischen Struktur zu größenbeschränkten Ringen verbunden werden. Ringstrukturen sind ein guter Kompromiss zwischen der Verfügbarkeit von Netzwerken und deren Herstellungskosten. Die Verfügbarkeit ist gewährleistet, solange maximal ein Knoten pro Ring ausfällt. Die grundlegende Idee dieser Diplomarbeit um dieses Netzwerkdesign-Problem zu lösen, ist die Knoten mit Hilfe von hierarchischen Clusteringalgorithmen anzuordnen und die resul-tierende Hierarchie für nachfolgende Heuristiken zu verwenden, welche die Ringe finden. Vorhergehende Ansätze für vergleichbare Netzwerkdesign-Probleme haben die inhärente Netzwerkstruktur nicht auf solche Weise genützt und eher Greedy-Heuristiken eingesetzt. Um gültige Ringe zu finden, wurden drei Heuristiken implementiert. Strategisches Variieren der erlaubten Ringgröße hilft der ersten Heuristik Ringe unter Benützung der Cluster-Hierarchie zu finden. Die zweite Heuristik baut auf den in der vorherigen Schicht gefundenen Ringen auf, indem sie nach gültigen Pfaden sucht, die an diese Ringe angeschlossen werden können. Drittens wird eine Reparaturheuristik angewendet, welche versucht verbleibende Knoten zu bestehenden Ringen zuzuweisen. Zuletzt werden lokale Suchverfahren eingesetzt, um die Gesamtkosten zu verbessern. Um zu überprüfen, wie gut dieser Lösungsansatz funktioniert, wurden zwei Testinstanz-Generatoren implementiert. Der Erste generiert Instanzen zufallsbasiert, der Zweite baut auf dem bekannten TSPLIB-Archiv auf. Die Evaluierung der zufallsbasierten Testinstanzen hat gezeigt, dass alle drei Heuristiken sämtliche Instanzen lösen konnten, wobei Girvan-Newman und Kernighan-Lin in jedem Testlauf Lösungen gefunden haben, war dies bei K-means nicht der Fall. Mit Kernighan-Lin konnte im Durchschnitt schneller eine Lösung gefunden werden, aber die Gesamtkosten waren bei den beiden anderen Algorithmen etwas besser. Mit den TSPLIB-basierten Testinstanzen konnte nicht mit allen Clusteringalgorithmen eine Lösung erzielt werden, aber zumindest war für jede Testinstanz mindestens ein Clustering";"0,00";"0,00";"0,00"
"TUW-202824";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\groundtruth\TUW-202824.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\result\result-TUW-202824-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\extracted\grobid\grobid-TUW-202824-xstream.xml"")";"Cloud computing is introducing the next big shift in the IT-industry. It fundamentally changes the IT-strategy of organizations. Cloud computing promises many advantages such as reduced capital expense, support of brief surges in capacity and a better economies of scale [1]. Cloud computing is not only a useful technology for the private sector rather it also can benefit the public sector in many ways [2]. It makes e-government systems faster and cheaper and accelerates the adaptation of use of IT by citizens [3]. Cloud computing is high on the agenda of Obama administration and is being used by federal and local governments with significant benefits [4]. In the European Union the potentials of the Cloud computing have been recognized and the Cloud agenda is being pushed forward, not only because of its cost saving potentials but also because of its impact on the environment.
In this work we have conducted a case study for integration of Cloud computing in the Austrian Public sector. The contribution of this work is identification of the requirements of the public sector and obstacles for integration of cloud computing in the Austrian public sector. In this case study eight ministries and the office of chancellor have been interviewed.";"Cloud computing is introducing the next big shift in the IT-industry. It fundamentally changes the IT-strategy of organizations. Cloud computing promises many advantages such as reduced capital expense, support of brief surges in capacity and a better economies of scale [1]. Cloud computing is not only a useful technology for the private sector rather it also can benefit the public sector in many ways [2]. It makes e-government systems faster and cheaper and accelerates the adaptation of use of IT by citizens [3]. Cloud computing is high on the agenda of Obama administration and is being used by federal and local governments with significant benefits [4]. In the European Union the potentials of the Cloud computing have been recognized and the Cloud agenda is being pushed forward, not only because of its cost saving potentials but also because of its impact on the environment. In this work we have conducted a case study for integration of Cloud computing in the Austrian Public sector. The contribution of this work is identification of the requirements of the public sector and obstacles for integration of cloud computing in the Austrian public sector. In this case study eight ministries and the office of chancellor have been interviewed.";"100,00";"100,00";"100,00"
"TUW-203409";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\groundtruth\TUW-203409.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\result\result-TUW-203409-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\extracted\grobid\grobid-TUW-203409-xstream.xml"")";"Our work focuses on investigating novel ways of efficient processor simulation using just-in-time compilation techniques. We can automatically generate a cycle-accurate simulator from a processor description that captures hardware structure and instruction set. The simulator employs an adaptive two-level just-in-time compilation scheme based on LLVM to attain high simulation speeds.
As the main source of slowdown during simulation we identified the LLVM code generator. We reduced compilation time in our own experimental code generator by an order of magnitude compared to LLVM's original backend. Current work aims at leveraging instruction descriptions already available in LLVM to extend the coverage of our fast JIT code generator.";;"none extracted value";"0,00";"0,00"
"TUW-203924";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\groundtruth\TUW-203924.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\result\result-TUW-203924-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\extracted\grobid\grobid-TUW-203924-xstream.xml"")";"Genetic dispositions play a major role in individual disease risk and treatment response. Genomic medicine, in which medical decisions are refined by genetic information of particular patients, is becoming increasingly important. Here we describe our work and future visions around the creation of a distributed infrastructure for pharmacogenetic data and medical decision support, based on industry standards such as the Web Ontology Language (OWL) and the Arden Syntax.";;"none extracted value";"0,00";"0,00"
"TUW-204724";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\groundtruth\TUW-204724.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\result\result-TUW-204724-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\extracted\grobid\grobid-TUW-204724-xstream.xml"")";"The strategic management of intellectual capital involves rethinking how the companies creates value from a knowledge -centric perspective and redesigning and orchestrating the role of staff knowledge in the firm's strategy. This paper presents the author's software system for facilitating decision making at a strategic level in terms of the profitability of investment in staff knowledge. This software is a computer implementation of the method for planning and selection of personnel in SME.";"The author's software system for planning and selection of personnel in SME Abstract: The strategic management of intellectual capital involves rethinking how the companies creates value from a knowledge -centric perspective and redesigning and orchestrating the role of staff knowledge in the firm's strategy. This paper presents the author's software system for facilitating decision making at a strategic level in terms of the profitability of investment in staff knowledge. This software is a computer implementation of the method for planning and selection of personnel in SME. information systems that support knowledge management in SMEs can give guarantees a constant competitive advantage in the market. Keywords: intellectual capital, strategic knowledge management 1. Introduction The advantage of a company in the SME sector (small and medium sizes enterprises) is determined by effectiveness and the extent of the knowledge-workers' involvement. The role of intellectual capital management consists in striving to increase the share of immaterial resources (at the costs of the material ones) in the generated products, services, and the total market value of an organization (Król and Ludwiczy?ski, 2007). The article presents the description of strategic knowledge resources in companies and author's method for plannig and selection of personnel in SME (see chapter 2). In the third chapter the author's software system for facilitating decision making at a strategic level in terms of the profitability of investment in staff knowledge is presented on example of the decision-making situation in which an innovative SME is considering the employment of a new m-th employee. The summary shows the";"0,00";"0,00";"0,00"
"TUW-205557";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\groundtruth\TUW-205557.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\result\result-TUW-205557-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\extracted\grobid\grobid-TUW-205557-xstream.xml"")";"The spread of safety critical real time systems results in an increased necessity for worst case execution time (WCET) analysis of these systems: finding the time limit within which the software system responds in all possible scenarios. Computing the WCET for programs with loops or recursion is, in general, un-decidable. We present an automatic method for computing tight upper bounds on the iteration number of special classes of program loops. These upper bounds are further used in the WCET analysis of programs. The technique deploys pattern-based recurrence solving in conjunction with program flow refinement using SMT reasoning. To do so, we refine program flows using SMT reasoning and rewrite certain multi-path loops into single-path ones, possibly over-approximating the loop-bound. The multi-path loops we consider are I) abruptly-terminating loops that might terminate early due to break statements and II) loops with additional monotonic updates, that conditionally modify the loop counter. For those, the minimum increase of the loop counter is computed and used as loop step expression. Single-path loops are further translated into a set of recurrence relations over program variables. For solving recurrences we deploy a pattern-based recurrence solving algorithm, computing closed forms for a restricted class of recurrence equations. Finally, iteration bounds are derived for program loops from the computed closed forms. We only compute closed forms for a restricted class of loops, however, in practice, these recurrences describe the behavior of a large set of program loops that are relevant to WCET analysis. Our technique is implemented in the r-TuBound tool and was successfully tried out on a number of challenging WCET benchmarks: we evaluate the symbolic loop bound generation technique and present an experimental evaluation of the method carried out with the r-TuBound software tool. We evaluate our method against various academic and industrial WCET benchmarks, and compare the results to the original TuBound tool.";"Extended Abstract. The spread of safety critical real time systems results in an increased necessity for worst case execution time (WCET) analysis of these systems: finding the time limit within which the software system responds in all possible scenarios. Computing the WCET for programs with loops or recursion is, in general, un-decidable. We present an automatic method for computing tight upper bounds on the iteration number of special classes of program loops. These upper bounds are further used in the WCET analysis of programs. The technique deploys pattern-based recurrence solving in conjunction with program flow refinement using SMT reasoning. To do so, we refine program flows using SMT reasoning and rewrite certain multi-path loops into single-path ones, possibly over-approximating the loop-bound. The multi-path loops we consider are I) abruptly-terminating loops that might terminate early due to break statements and II) loops with additional monotonic updates, that conditionally modify the loop counter. For those, the minimum increase of the loop counter is computed and used as loop step expression. Single-path loops are further translated into a set of recurrence relations over program variables. For solving recurrences we deploy a pattern-based recurrence solving algorithm, computing closed forms for a restricted class of recurrence equations. Finally, iteration bounds are derived for program loops from the computed closed forms. We only compute closed forms for a restricted class of loops, however, in practice, these recurrences describe the behavior of a large set of program loops that are relevant to WCET analysis. Our technique is implemented in the r-TuBound tool and was successfully tried out on a number of challenging WCET benchmarks: we evaluate the symbolic loop bound generation technique and present an experimental evaluation of the method carried out with the r-TuBound software tool. We evaluate our method against various academic and industrial WCET benchmarks, and compare the results to the original TuBound tool.";"100,00";"100,00";"100,00"
"TUW-205933";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\groundtruth\TUW-205933.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\result\result-TUW-205933-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\extracted\grobid\grobid-TUW-205933-xstream.xml"")";"Philosophy-of-information considerations can analyse information concepts according to four ways of thinking. A Unified Theory of Information (UTI) requires the fourth way of thinking – integration. This integration can be performed, if a complex systems view is informed by the heuristics of a historical and logical account. In particular, the terms of ""difference"" or ""variety"", negentropy and semiosis are used for integration. Reference is made to Gregory Bateson, Arkady D. Ursul, Edgar Morin, and Charles Sanders Peirce. An integrated information definition is presented. Information is defined as relation such that an Evolutionary System se (signator; the signmaker) reflects (1) some perturbation P (signandum/signatum; (to-be-)signified (2) by the order O it builds up spontaneously (signans; the sign) (3) for the sake of negentropy. The process of information-generation coincides with the process of sign-production and both coincide with the process of self-organisation; so do their respective results: information, sign, and self-organised order. The concepts of self-organisation and information (sign) turn out to be co-extensive. The notion ""emergent information"" is applied to characterise the complexity of information processes that proceed between determinacy and indeterminacy. Since information generation is a process that allows novelty to emerge, it is worth noting that it is not a mechanical process that can be formalised, expressed by a mathematical function, or carried out by a computer.";"Philosophy-of-information considerations can analyse information concepts according to four ways of thinking. A Unified Theory of Information (UTI) requires the fourth way of thinking-integration. This integration can be performed, if a complex systems view is informed by the heuristics of a historical and logical account. In particular, the terms of ""difference"" or ""variety"", negentropy and semiosis are used for integration. Reference is made to Gregory Bateson, Arkady D. Ursul, Edgar Morin, and Charles Sanders Peirce. An integrated information definition is presented. Information is defined as relation such that an Evolutionary System se (signator; the signmaker) reflects (1) some perturbation P (signandum/signatum; (to-be-)signified (2) by the order O it builds up spontaneously (signans; the sign) (3) for the sake of negentropy. The process of information-generation coincides with the process of sign-production and both coincide with the process of self-organisation; so do their respective results: information, sign, and self-organised order. The concepts of self-organisation and information (sign) turn out to be co-extensive. The notion ""emergent information"" is applied to characterise the complexity of information processes that proceed between determinacy and indeterminacy. Since information generation is a process that allows novelty to emerge, it is worth noting that it is not a mechanical process that can be formalised, expressed by a mathematical function, or carried out by a computer.";"100,00";"100,00";"100,00"
"TUW-213513";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\groundtruth\TUW-213513.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\result\result-TUW-213513-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\extracted\grobid\grobid-TUW-213513-xstream.xml"")";;"Software models, traditionally used mainly for documentation and informal specification purposes, are becoming first-class development artifacts in the area of Model-driven Engineering (MDE). In MDE, code is generated automatically from multi-view models described in languages like the Unified Modeling Language (UML) 1. Maintaining consistency between the different views of a model is crucial for the generation of correct code. As software models undergo evolution, particularly in cooperative development environments, tool support for evolution tasks like versioning and merging is indispensable. It is important to thoroughly test such tools in order to avoid the introduction of inconsistent models. However, real-life test cases that cover sufficient evolution scenarios are difficult to obtain. We therefore suggest a method to generate artificial scenarios to facilitate fuzz testing of model evolution tools. In previous work [2] we presented an approach to merge concurrently evolved sequence diagrams with respect to the behavior modeled in their corresponding state machines view. We described the sequence diagram merging (SDM) problem formally, suggested a method to solve this problem, and implemented a prototype based on the EMF framework 2. As there were no benchmarks available, we manually created a set of test cases. However, this is a very cumbersome testing method particularly when a good coverage is needed. A set of randomly generated instances solves this problem, as we show in the following.";"0,00";"none expected";"0,00"
"TUW-216744";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\groundtruth\TUW-216744.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\result\result-TUW-216744-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\extracted\grobid\grobid-TUW-216744-xstream.xml"")";"Medical practitioners often have unmet information needs that impact patient care. However, currently available web-based search engines are not suitable for routine use. Finding relevant information takes too long, assessing the trustworthiness of found information is difficult, and support for the heterogeneity of languages and nomenclature across European countries is lacking. In this paper, we analyze the current barriers to web-based searching by medical practitioners and introduce the European Khresmoi project, which aims to dismantle these barriers.";;"none extracted value";"0,00";"0,00"
"TUW-217690";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\groundtruth\TUW-217690.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\result\result-TUW-217690-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\extracted\grobid\grobid-TUW-217690-xstream.xml"")";"Precise localization for mobile Augmented Reality in large indoor environments without specific tracking infrastructure is challenging. This is especially true for rooms with changing properties, like lighting, seating and carpeting. With these constraints a map for a vision based tracking approach has to be continuously updated. The Parallel Tracking and Mapping (PTAM) algorithm is capable of generating and extending a map while tracking the camera pose in an unknown environment. However, it has originally been designed for small workspace environments and has therefore certain limitations. We have extended and modified the original implementation in order to ensure efficient and robust map generation and tracking in large rooms. Furthermore, we have tested a mobile setup with the system in Festsaal in Vienna’s Hofburg, which is close to thousand square meters in size. The user’s position and path was tracked while the environment was augmented with virtual objects and the system was successfully tested for robustness and occlusions.";;"none extracted value";"0,00";"0,00"
"TUW-217971";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\groundtruth\TUW-217971.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\result\result-TUW-217971-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\extracted\grobid\grobid-TUW-217971-xstream.xml"")";"Many real-world Visual Analytics applications involve time-oriented data. I am working in a research project related to this challenge where I am responsible for the interactive visualization part. My goal are interactive visualizations to explore such time-oriented data according to the user tasks while considering the structure of time. Time is composed of many granularities that are likely to have crucial influence on the formation of the data. The challenge is to integrate the granularities into a detailed compound view on the data, like the compound eye of insects integrates many images into one view. Other members of our team are experts in temporal data mining and user centered design. The goal is to combine our research topics to an integrated system that helps domain experts to get more insight from their time-oriented data.";"Many real-world Visual Analytics applications involve time-oriented data. I am working in a research project related to this challenge where I am responsible for the interactive visualization part. My goal are interactive visualizations to explore such time-oriented data according to the user tasks while considering the structure of time. Time is composed of many granularities that are likely to have crucial influence on the formation of the data. The challenge is to integrate the granularities into a detailed compound view on the data, like the compound eye of insects integrates many images into one view. Other members of our team are experts in temporal data mining and user centered design. The goal is to combine our research topics to an integrated system that helps domain experts to get more insight from their time-oriented data. a natural context, like the change of seasons, they are usually regular , but granularities based on social context can also be irregular, e.g., easter time. They play an important role in many datasets from real-world applications, like customer data from shops.";"0,00";"0,00";"0,00"
"TUW-221215";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\groundtruth\TUW-221215.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\result\result-TUW-221215-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\extracted\grobid\grobid-TUW-221215-xstream.xml"")";"The Selective Graph Coloring Problem (SGCP) is about finding a subgraph of a particular structure whose chromatic number is as low as possible. The original graph is divided into several clusters, and from each cluster the subgraph has to contain exactly one node. This problem is NP-hard and therefore it is usually solved by means of heuristics.
	
I implemented several variants of an algorithm making use of Variable Neighborhood Search (VNS) to search the space of solution candidates and then evaluating the solution using heuristic or exact methods. Furthermore, each variant can be used with or without a solution archive, i.e. a data structure in which previously found solutions are stored so that duplicates need not be re-evaluated but can be efficiently converted into new solutions instead. For exact computation of the chromatic number integer linear programming was used. To obtain an upper bound a variant of greedy coloring was used. Another variant of the algorithm also counts the number of conflicts that would appear if one color less were used. Finally, two methods were implemented to obtain a lower bound: maximum clique and linear programming using column generation.

The program was tested with various instances from the literature. My algorithm often finished computation within a very short time, but in general it led to slightly worse results.";"Danksagung Ich möchte mich vor allem bei Günther Raidl für die Möglichkeit bedanken, meine Diplomarbeit an seiner Abteilung zu verfassen, und bei Bin Hu für die ausgezeichnete Betreuung. Weiters möchte ich mich bei meinen Eltern bedanken, die mir das Studium finanziell ermöglicht haben. Zudem habe ich ihnen auch zu verdanken, dass sie mein Interesse an Computern frühzeitig erkannt und gefördert haben. Abstract The Selective Graph Coloring Problem (SGCP) is about finding a subgraph of a particular structure whose chromatic number is as low as possible. The original graph is divided into several clusters, and from each cluster the subgraph has to contain exactly one node. This problem is NP-hard and therefore it is usually solved by means of heuristics. I implemented several variants of an algorithm making use of Variable Neighborhood Search (VNS) to search the space of solution candidates and then evaluating the solution using heuristic or exact methods. Furthermore, each variant can be used with or without a solution archive, i.e. a data structure in which previously found solutions are stored so that duplicates need not be re-evaluated but can be efficiently converted into new solutions instead. For exact computation of the chromatic number integer linear programming was used. To obtain an upper bound a variant of greedy coloring was used. Another variant of the algorithm also counts the number of conflicts that would appear if one color less were used. Finally, two methods were implemented to obtain a lower bound: maximum clique and linear programming using column generation. The program was tested with various instances from the literature. My algorithm often finished computation within a very short time, but in general it led to slightly worse results. Kurzfassung Beim Selective Graph Coloring Problem (SGCP) geht es darum, einen Teilgraphen mit spezieller Struktur zu finden, dessen chromatische Zahl so niedrig wie möglich ist. Der Ursprungsgraph ist in mehrere Cluster unterteilt, und von jedem Cluster muss der Teilgraph genau einen Knoten enthalten. Dieses Problem ist NP-schwer und wird daher meistens mit Heuristiken gelöst. Ich habe mehrere Varianten eines Algorithmus implementiert, der Variable Neighborhood Search (VNS) benutzt, um den Lösungsraum zu durchsuchen, und dann die gefundene Lösung mit heuristischen oder exakten Methoden evaluiert. Jede Variante kann mit oder ohne ein Lösungsarchiv verwendet werden. Ein Lösungsarchiv ist eine Datenstruktur, in der bereits gefundene Lösungen gespeichert werden, so dass Duplikate nicht neu evaluiert werden müssen, sondern effizient zu neuen Lösungen konvertiert werden können. Um eine obere Schranke zu errechnen, wurde eine Variante von Greedy Coloring verwendet. Eine weitere Variante des Algorithmus zählt auch die Anzahl der Konflikte, die entstünden, würde eine Farbe weniger verwendet werden. Schließlich wurden zwei Methoden umgesetzt, um eine untere Schranke zu berechnen: maximale Clique und lineare Programmierung mit Spaltengenerierung. Das Programm wurde mit verschiedenen Instanzen aus der Literatur getestet. Mein Algorithmus beendete die Berechnungen oft schon nach sehr kurzer Laufzeit, führte aber im Allgemeinen zu geringfügig schlechteren Ergebnissen. Introduction Graphs are a useful tool for modelling real-world problems, as they can serve as an abstraction for various things, such as networks and maps. For this reason, the solution of problems related to graph theory may have an impact in real life. Computer science students usually learn about some of these problems as well as algorithms for solving them in advanced courses on algorithmics. Topics commonly discussed in these courses include shortest path problems, finding the maximal flow in a network, and the Traveling Salesperson Problem. Depth First Search, Breadth First Search, Dijkstra's algorithm, the Bellman-Ford algorithm, the Floyd-Warshall algorithm, Johnson's algorithm, the Ford-Fulkerson method, preflow-push algorithms and other methods belong to the general education of any computer scientist specializing in algorithms. Graph coloring usually does not appear in these courses, but it is still an important problem about which many papers have been published. Applications of graph coloring include time tabling and various forms of allocation tasks [10]. Since it is an NP-equivalent problem, various heuristics have been proposed to get good results in a reasonable amount of time. The selective graph coloring problem is an extension of graph coloring and for this reason, it is NP-hard as well. It is about finding a subgraph consisting of one node of each cluster that has a chromatic number as low as possible. For most researchers the motivation to study this problem has been its relevance to optical networks [39]. Although some papers have been published that propose efficient solution algorithms for this problem, by far not all possible solution algorithms have been explored yet. This was my motivation for choosing this problem as the topic of my diploma thesis. In this thesis, I will present diverse variants of a heuristic solution algorithm for the selective graph coloring problem and the results obtained for some test instances. The algorithm is based on variable neighborhood search for scanning the solution space. Each solution is evaluated using exact or heuristic methods. 4 CHAPTER 1. INTRODUCTION The exact method is exact but slow. Regarding heuristics mainly an upper bound is computed, but there are also variants calculating a number of conflicts that would arise with one color less and giving a lower bound. In addition a solution archive has been implemented, which makes it possible to avoid duplicates during the search space exploration and easily find new solutions not evaluated as local optima yet. Figure 1.1 shows a graph with several clusters and a possible solution subgraph,";"0,00";"0,00";"0,00"
"TUW-223906";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\groundtruth\TUW-223906.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\result\result-TUW-223906-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\extracted\grobid\grobid-TUW-223906-xstream.xml"")";"This paper describes our contribution to the social event detection (SED) task of the MediaEval Benchmark 2013. We present a robust unsupervised approach for the clustering of tagged photos and videos into social events. Results on the SED datasets show that the proposed approach yields an excellent generalization ability and state-of-the-art clustering performance.";"This paper describes our contribution to the social event detection (SED) task of the MediaEval Benchmark 2013. We present a robust unsupervised approach for the clustering of tagged photos and videos into social events. Results on the SED datasets show that the proposed approach yields an excellent generalization ability and state-of-the-art clustering performance. 1. INTRODUCTION";"100,00";"100,00";"100,00"
"TUW-223973";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\groundtruth\TUW-223973.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\result\result-TUW-223973-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\extracted\grobid\grobid-TUW-223973-xstream.xml"")";"Translating clinical practice guidelines into a computer-inter-pretable format is a challenging and laborious task. In this project we focus on supporting the early steps of the modeling process by automatically identifying conditional activities in guideline documents in order to model them automatically in further consequence. Therefore, we developed a rule-based, heuristic method that combines domain-independent information extraction rules and semantic pattern rules. The classification also uses a weighting coefficient to verify the relevance of the sentence in the context of other information aspects, such as effects, intentions , etc. Our evaluation results show that even with a small set of training data, we achieved a recall of 75 % and a precision of 88 %. This outcome shows that this method supports the modeling task and eases the translation of CPGs into a semi-formal model.";"Translating clinical practice guidelines into a computer-inter-pretable format is a challenging and laborious task. In this project we focus on supporting the early steps of the modeling process by automatically identifying conditional activities in guideline documents in order to model them automatically in further consequence. Therefore, we developed a rule-based, heuristic method that combines domain-independent information extraction rules and semantic pattern rules. The classification also uses a weighting coefficient to verify the relevance of the sentence in the context of other information aspects, such as effects, intentions , etc. Our evaluation results show that even with a small set of training data, we achieved a recall of 75 % and a precision of 88 %. This outcome shows that this method supports the modeling task and eases the translation of CPGs into a semi-formal model. Clinical Practice Guidelines (CPGs) are defined as ""systematically developed statements to assist practitioners and patient decisions about appropriate health-care for specific circumstances"" [6]. They include recommendations describing appropriate care for the management of patients with a specific clinical condition. An important part of CPG contents refers to the procedures to perform, often formulated together with specific conditions that have to hold in order to execute an activity. CPGs are published as textual guidelines, but in order to deploy them in some kind of computerized tool (e.g., a reminder system or a more complex decision-support system) they have to be represented in specialized languages (see [15, 7] for a comparison and overview). Although different authoring/editing tools are often associated with these languages, authoring is a labor-intensive task that requires comprehensive knowledge in medical as well as computer science. There have been several approaches to ease the modeling process, amongst others by introducing intermediate representations that provide more semi-struc-tured and less formal formats. One of them is MHB, the Many-Headed Bridge 2 [21], that tries to bridge the gap between the guideline text and its corresponding formalized model. It falls in the category of document-centric approaches [22] and is devised to produce a non-executable XML document with the relevant CPG fragments, starting from the original text. The knowledge of a CPG is thereby represented in a series of chunks that correspond to a certain bit of information in the CPG (e.g., a sentence, part of a sentence, more than one sentence). The information in a chunk is structured in various dimensions, e.g., control flow, data flow. To additionally support the modeling task for non-IT experts, MHB was further developed and split into MHB-F (free-text version) and MHB-S (semantically enriched version) [19]. MHB-F now provides a very simplified structure to make the modeling even for non-IT experts feasible and to leave modeling details to knowledge engineers in a later step. In order to ease the laborious modeling, parts of the task should be automated by applying information extraction methods. In this work we will focus on the identification of condition-action sentences that form a prominent aspect of the process flow in CPGs. The discovery of such combinations is not a trivial one. On the one hand, condition-action sentences are rarely of the form 'if condition then action', but require more sophisticated identification methods. On the other hand, conditions may refer to effects, intentions, or events and not activities, and these combinations have to be sorted out by our method. Table 1 shows a few example sentences in regard to their MHB-F aspects.";"0,00";"0,00";"0,00"
"TUW-225252";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\groundtruth\TUW-225252.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\result\result-TUW-225252-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\extracted\grobid\grobid-TUW-225252-xstream.xml"")";;"The reconstruction of shredded documents is of high interest not only in forensic science but also when documents are destroyed unintentionally. Reconstructing cross-cut shredded documents (RCCSTD) is particularly difficult since the documents are cut into rectangular pieces of equal size. Since shape information along the edges-in contrast to hand torn pieces-cannot be exploited, the reconstruction solely depends on the information written on the shreds. Therefore, we use a metric for calculating the number of gray value mismatches along the edges of two shreds put next to each other either horizontally or vertically. Consequentially, we model the document reconstruction as a combinatorial optimization problem minimizing the overall mismatch of the reconstructed document. Since we focus in this work on the combinatorial aspect of the problem we use this simple metric which can be replaced in future work by more advanced pattern recognition techniques, see [2] for a sample method. In previous work, Prandtstetter and Raidl [3] developed an Ant Colony Optimization and a Variable Neighborhood Search (VNS) for the RCCSTD, while Schauer et al. [5] proposed a Memetic Algorithm (MA). Sleit et al. [6] proposed a different approach by iteratively merging two clusters that fit together well and repairing possibly occurring conflicts. In this work the MA from [5] is adapted and extended by a complete solution archive in order to avoid duplicate solutions by efficiently storing all visited solutions in a special data structure. If a duplicate solution is detected it is converted into a similar yet unconsidered one. This is done to preserve the diversity of the population and to avoid unnecessary re-evaluations of already visited solutions. This approach is a rather new method for duplicate detection and conversion which was successfully applied on several binary problems (e.g., MAX-SAT) in [4] as well as on the generalized minimum spanning tree problem [1]. 2 A Genetic Algorithm with Solution Archive For a detailed description of the GA and its operators, which is extended by our solution archive, see [5]. To encode solutions the authors used an n×n array that";"0,00";"none expected";"0,00"
"TUW-226000";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\groundtruth\TUW-226000.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\result\result-TUW-226000-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\extracted\grobid\grobid-TUW-226000-xstream.xml"")";"The user interface is the most important feature of interaction between users and (AAL) services. Explicitly defined user interfaces are bound to a specific toolkit and programming language or markup language. Thus a separate user interface definition has to be created manually for different classes of I/O devices to be supported. Compared to manual user interface creation, the automatic or semi-automatic generation of user interfaces based on interaction descriptions considerably reduces the manual effort necessary for integrating a large number of devices and therefore automatically increases the number of supported devices. The main goal of this paper is to provide an overview of selected existing solutions for the definition of generic user interactions and the generation of user interfaces. The comparison shows that the aspect of adaptability is partly covered by the presented User Interaction Description Languages. Nevertheless it is important to analyze them with respect to additional criteria, like accessibility, context-and use-case awareness, to receive a meaningful overview of advantages and drawbacks of the different approaches leading to a good basis for choosing one of the presented approaches.";"The user interface is the most important feature of interaction between users and (AAL) services. Explicitly defined user interfaces are bound to a specific toolkit and programming language or markup language. Thus a separate user interface definition has to be created manually for different classes of I/O devices to be supported. Compared to manual user interface creation, the automatic or semi-automatic generation of user interfaces based on interaction descriptions considerably reduces the manual effort necessary for integrating a large number of devices and therefore automatically increases the number of supported devices. The main goal of this paper is to provide an overview of selected existing solutions for the definition of generic user interactions and the generation of user interfaces. The comparison shows that the aspect of adaptability is partly covered by the presented User Interaction Description Languages. Nevertheless it is important to analyze them with respect to additional criteria, like accessibility, context-and use-case awareness, to receive a meaningful overview of advantages and drawbacks of the different approaches leading to a good basis for choosing one of the presented approaches.";"100,00";"100,00";"100,00"
"TUW-226016";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\groundtruth\TUW-226016.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\result\result-TUW-226016-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\extracted\grobid\grobid-TUW-226016-xstream.xml"")";"Invariant genereation is a critical problem in proving different properties for programs with loops, properties including correctnes. The problem becomes harder with the incresing numbers of quantifiers in the property to be proven. In this paper we study and combine different methods of invariant generation in order to obtain stronger properties.";"Invariant genereation is a critical problem in proving different properties for programs with loops, properties including correctnes. The problem becomes harder with the incresing numbers of quantifiers in the property to be proven. In this paper we study and combine different methods of invariant generation in order to obtain stronger properties. Kurzfassung Invariant generiert ist ein kritische Problem für Programmen mit Schleife zum Beweisen der Eigenschaften, inclusive die Richtigkeit. Die problem wird schwerer bei hohe Anzhal des Quantoren in die geprüfte Eigenschaft. In diese arbeit wir studiere diese Problem und versuchen combinieren verschieden Methoden für schwarer invariants zu beweisen. 1 Introduction The complexity of software systems is in a continuous grow. Making sure that different pieces of the same system will work together properly is a hard task. The development of software systems imply the work of more people working at different parts of it, using computer, networks, physical devices, and over millions of lines of code in various languages. Integrating, understanding and ensuring the reliability of such a system are necessary task in order to make it useful. In this paper we give attention to the task of ensuring reliability. In the past years a lot of interest was given to this task and there were developed different methods to do it, but one challenge that was not yet overcome is the analysis of loops. In particular programs dealing with arrays use loops to process the elements, and on the strength to the unbounded nature of this data structures analyzing and inferring properties for elements becomes a challenging problem on its own. One way to approach this problem is to bound the loop [BCC + 03], unfolding it just a limited number of times and afterwards analyzing the new obtained program as if no loop would occur in it. Although this approach is successfully used in model checking techniques, the limitations of applying it consist in the loss of completeness of the algorithm. An informal explanation of this fact is that obtaining a proof that a bounded subset of elements do not have a certain property is a result strong enough to consider that the property does not hold for the entire loop, but if the property holds for the first n unrollings of the loop it may be the case that it will be falsified in one of the following iterations that were cut off by the bound. Another approach to reason about program loops is to statically analyze the code and extract loop properties automatically. In this thesis we follow this approach. We are going to analyze and compare three methods that automatically extract properties for loops. These methods are the symbol elimination method of [KV09], the constraint-based invariant generation approach of [LRCR13], and the postcondition-based method of [FM10]. We analyze and compare these approaches on series of challenging academic examples which are considered difficult to reason about. The symbol elimination technique is an automatically mechanism that generates invariants based on the static analysis of programs, that does not require other information from the user about the code analyzed. For this method we use a saturation theorem prover and we choose Vampire not only for it's capability to reason with different theories but also due to the fact that is one of the fastest provers awarded several times in competitions. The constraint-based method first analyses the code discovering every possible path and checking if properties hold in some key points along them. In the next chapters we are going to explain where and why a point in the path becomes such a key point. Properties that are checked are constructed based on a template following some rules that will also be presented later in the paper. The third method is the only one from the three presented that needs additional input from the user, namely a postcondition in the form of a formula. The program makes certain changes in the formula, that will be described later on, in order to find valid invariants for the program. Using the invariants discovered by the constraint-based and postcondition-based methods, in this thesis we further extend the power of symbol elimination in order to reason about more complex loops and invariants than in [KV09]. For doing so, we strengthen the underlining first-order theory reasoning engine of symbol elimination and add additional mathematical theorems and axioms to the symbol elimination problem. The symbol elimination problem is then further fed into a saturation theorem prover and is successfully used to prove the intended loop invariants and properties of the program. Our results show that theory reasoning in first-order theorem proving is a very challenging problem and requires a good understanding of the necessary theory axiomatizations. 2 Preliminaries In this section we give a brief overview of SAT solving, SMT solving and Saturation theorem prover. Also we give insight of the mechanism of Boogie theorem prover. These are necessary for further understanding the mechanism of the tools we are studying in this paper, and since all of them have at their core first-order logic we also present its syntax and semantics. We present all the above in a step wise manner starting with proposi-tional logic. Based on the syntax and semantic of propositional logic it is easier to understand the ones of first-order logic, since the latter one extends the expressive power of the former by introducing new characters and concepts. In the same step wise manner we present SAT-solving and SMT-solving. The first problem is related to propositional logic, while the second one makes use of the results obtained in SAT-solving as a subroutine in the algorithm for solving problems encoded in first-order logic. Also we present the mechanism of a saturation theorem prover, The understanding of this concept is necessary in order to understand the differences between the methods of invariant generation and the properties that make them efficient in different types of problems. Another point that is touched in this chapter is the higher order logic theorem prover Boogie, that has more expressive power than the other tools introduced in this paper and also uses a language specially created for it that has a syntax harder to understand than the others. We are going to present how these concept interact to each other and with the underlying theory, relating their results with the concept of program verification. 2.1 Propositional logic Is a part of mathematics important for program verification. The least complex component in the syntax of propositional [DW50] logic is an atom. Every atom has a truth value, either true or false, and it represents a statement (such as "" Roses are red."") without taking into account its internal structure. In order to get more complex propositions the atoms can be connected with connectives. The propositional syntax is defined as follows: 1. If p is an atom, then p is a p and q are set to the same truth value then p ? q is true, otherwise is false; To make the propositions easier to read and write, and to save parentheses the connectives have priorities as follows: ¬, ?, ?, ?, ?(meaning that ¬ binds stronger than ?, ? binds stronger than ?, etc.). The problem of determining if the atoms that form a proposition can be given truth values in such a way that the proposition would evaluate to true is called satisfiability problem (abbreviated as SAT). Example: p, r, q are atoms in propositional logic, and p ? q ? r ? q is a syntax correct formula. Choosing the values as follows: p ? f alse, r ? f alse and q ? true will cause the formula to evaluate to true (since p ? q evaluates to true, r ? q evaluates to true). Satisfiability problem is an NP-complete problem[Coo71]. The complexity and nature of this problem makes it useful in modeling different problems such as digital circuits, constraint satisfaction problems, reasoning about specifications. 2.2 SAT solvers These are tools that are constructed to solve SAT problems [ES04]. Despite the high complexity of the problem, very good results were obtained in practice with yearly improvements of the solving algorithms and heuristics for the SAT-solving contest. In figure 1 we present a scheme on how a SAT solver is used to solve a problem. The first step is to abstract the problem into a set of propositions. Usually the problem can not be abstracted straightforward so some simplification is needed. After finding a suitable representation of the problem as a set of formulae use a SAT solver to find a solution. If a solution is not find specific improvements are made in order to get a more efficient search. The SAT solvers take the input formulae in a special form named con-junctive normal form. Every propositional formula can be transformed in";"0,00";"0,00";"0,00"
"TUW-228620";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\groundtruth\TUW-228620.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\result\result-TUW-228620-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\extracted\grobid\grobid-TUW-228620-xstream.xml"")";"Business networking relies on application-specific quantity and quality of information in order to support social infrastructures in, e.g., energy allocation coordinated by smart grids, healthcare services with electronic health records, traffic management with personal sensors, RFID in retail and logistics, or integration of individuals' social network information into good, services, and rescue operations. Due to the increasing reliance of networking applications on sharing ICT services, dependencies threaten privacy, security, and reliability of information and, thus, innovative business applications in smart societies. Resilience is becoming a new security approach, since it takes dependencies into account and aims at achieving equilibriums in case of opposite requirements. This special issue on 'Security and Privacy in Business Networking' contributes to the journal 'Electronic Markets' by introducing a different view on achieving acceptable secure business networking applications in spite of threats due to covert channels. This view is on adapting resilience to enforcement of IT security in business networking applications. Our analysis shows that privacy is an evidence to measure and improve trustworthy relationships and reliable interactions between participants of business processes and their IT systems. The articles of this special issue, which have been accepted after a double-blind peer review, contribute to this view on interdis-ciplinary security engineering in regard to the stages of security and privacy requirements analysis, enforcement of resulting security requirements for an information exchange, testing with a privacy-preserving detection of policy violations, and knowledge management for the purpose of keeping business processes resilient.";"Business networking relies on application-specific quantity and quality of information in order to support social infrastructures in, e.g., energy allocation coordinated by smart grids, healthcare services with electronic health records, traffic management with personal sensors, RFID in retail and logistics , or integration of individuals' social network information into good, services, and rescue operations. Due to the increasing reliance of networking applications on sharing ICT services , dependencies threaten privacy, security, and reliability of information and, thus, innovative business applications in smart societies. Resilience is becoming a new security approach , since it takes dependencies into account and aims at achieving equilibriums in case of opposite requirements. This special issue on 'Security and Privacy in Business Networking' contributes to the journal 'Electronic Markets' by introducing a different view on achieving acceptable secure business networking applications in spite of threats due to covert channels. This view is on adapting resilience to enforcement of IT security in business networking applications. Our analysis shows that privacy is an evidence to measure and improve trustworthy relationships and reliable interactions between participants of business processes and their IT systems. The articles of this special issue, which have been accepted after a double-blind peer review, contribute to this view on interdis-ciplinary security engineering in regard to the stages of security and privacy requirements analysis, enforcement of resulting security requirements for an information exchange, testing with a privacy-preserving detection of policy violations , and knowledge management for the purpose of keeping business processes resilient. His Cabinet 2013). One aim of these initiatives is that systems, such as business networking applications, social infrastructures, and a society become resilient. Changes and vulnerabilities should be identified and analyzed to foresee and prevent incidents of any kind, as well as to react accordingly in acceptable time. Resilient ICT then supports the increased, sustainable welfare of a society. The key approach for gaining information on vulnerabil-ities and predicting incidents will be the ability to access and to analyze huge amounts of data in nearly real-time and to implement so-called Big Data analytics successfully. Service providers will collect, aggregate, and analyze data from various sources provided by";"0,00";"0,00";"0,00"
"TUW-231707";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\groundtruth\TUW-231707.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\result\result-TUW-231707-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\extracted\grobid\grobid-TUW-231707-xstream.xml"")";"The VRVis Research Center in Vienna is the largest technology transfer institution in the area of Visual Computing in Austria. The requirements of the funding body FFG include the publication of scientific research results in first class peer reviewed media, and the active cooperation with co-funding companies. As a consequence the requirements on the staff of VRVis are manifold: they have to communicate with real users, use real data, know about software and hardware, understand the market, do professional documentation, initiate new projects and write funding proposals for these, be part of the scientific community and publish and review papers, manage several projects in parallel and obey strict deadlines for their projects and some more. Such staff is barely available and must be trained on the job.";"The VRVis Research Center in Vienna is the largest technology transfer institution in the area of Visual Computing in Austria. The requirements of the funding body FFG include the publication of scientific research results in first class peer reviewed media, and the active cooperation with co-funding companies. As a consequence the requirements on the staff of VRVis are manifold: they have to communicate with real users, use real data, know about software and hardware, understand the market, do professional documentation, initiate new projects and write funding proposals for these, be part of the scientific community and publish and review papers, manage several projects in parallel and obey strict deadlines for their projects and some more. Such staff is barely available and must be trained on the job. 1 INTRODUCTION";"100,00";"100,00";"100,00"
"TUW-233317";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\groundtruth\TUW-233317.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\result\result-TUW-233317-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\extracted\grobid\grobid-TUW-233317-xstream.xml"")";"Visual Analytics prototypes increasingly support human sensemak-ing through providing Provenance information. For data analysts the challenge of knowledge generation starts with assessing the quality of a data set, but Provenance is not yet utilized to aid this task. This position paper aims at characterizing the complexity of Visual Analytics methods introducing Provenance in Data Quality by highlighting the challenges of (1) generating Provenance from Data Quality Control and (2) sensemaking based on Data Quality Provenance.";"Visual Analytics prototypes increasingly support human sensemak-ing through providing Provenance information. For data analysts the challenge of knowledge generation starts with assessing the quality of a data set, but Provenance is not yet utilized to aid this task. This position paper aims at characterizing the complexity of Visual Analytics methods introducing Provenance in Data Quality by highlighting the challenges of (In Data Quality (DQ) assessment one of the central questions is, 'Is the Data Quality good enough for analysis to produce meaningful results?' The quality of data analysis is highly dependent on the quality of the underlying data. Thus, a prerequisite of any data analysis, such as creating visualizations and performing analytical reasoning, is assessing and improving DQ. Data cleansing is an iterative task that requires user expertise and domain knowledge of the data provided [7]. DQ control can be understood as a combination of data quality assessment, the data cleansing process, as well as applying transformations to change a data set's structure. Kandel et al. [7] argue that integrating interactive and visual systems could facilitate these tasks as well as data verification. Yet, the analyst is left with the decision about when quality is sufficient to start analysis, or if the data is worth further manipulating at all. Sensemaking is an integral part of Visual Analytics (VA). During DQ assessment the analyst needs to take into account not only the actual data, but also implicit information, like how the data was created or its transformation history. A data set already might have been analyzed by someone else, generating a transformation";"0,00";"0,00";"0,00"
"TUW-233657";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\groundtruth\TUW-233657.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\result\result-TUW-233657-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\extracted\grobid\grobid-TUW-233657-xstream.xml"")";"Recent research in Visualization has focused mostly on data analysis systems for domain experts, but also considered presentation to external people in the form of storytelling. The established directions assume that the target audience has in inherent interest in the facts to be discovered, sometimes even to the point of them being willing to learn how to operate a complex visualization system and spend considerable time and effort. In reality, sometimes the opposite is true: people unwilling to face an inconvenient truth actively avert their eyes. As a solution, we propose the presentation of facts by experts who manage to gain a limited amount of attention by means of rapid and expressive visualization. Using conventional desktop systems, this method is hard to implement, but new visual channels will open up new possibilities.";"Recent research in Visualization has focused mostly on data analysis systems for domain experts, but also considered presentation to external people in the form of storytelling. The established directions assume that the target audience has in inherent interest in the facts to be discovered, sometimes even to the point of them being willing to learn how to operate a complex visualization system and spend considerable time and effort. In reality, sometimes the opposite is true: people unwilling to face an inconvenient truth actively avert their eyes. As a solution, we propose the presentation of facts by experts who manage to gain a limited amount of attention by means of rapid and expressive visualization. Using conventional desktop systems, this method is hard to implement, but new visual channels will open up new possibilities. In 1997, Edward Tufte argued that the catastrophic loss of the space shuttle Challenger can be traced back to the engineers who argued for postponing the launch, but failed to convince the officials at NASA [13]. Even though his argument was subject to criticism later [10], the critics refrain ""to say that the engineers presentation was not flawed or that even if conceptually correct, could not have been better done"" [10]. Both publications are several years old by now, and in the meantime , the science of Visual Analytics has emerged that might have allowed the engineers to gain a much better understanding of their data. Afterward, they might, at best, have made a screenshot from their Visual Analytics system and send that to NASA-active analysis by experts has improved much, but for convincing others, research has stalled. Basically, what Tufte does [13] is nothing else but storytelling [7] which has seen a considerable amount of practical research and is praised as a tool for reporters in writing articles. However, those visu-alizations are still aimed at people of have an active interest in getting information about a topic. The officials at NASA most likely had a high interest of launching the Challenger, and there was only little time to convince them with one or two highly impressive visualizations. In this workshop paper, our intent are to 1. analyze the task of convincing other people by a visualization rather than a lengthy discussion; 2. explain how the emergence of non-desktop (or rather console) computer usage helps in that task; and 3. lay out the prospects for the future. In Section 2, we show what has already been done. In Section 3 we describe three scenarios of a possible future. Finally, in Section 4, [13] E. R. Tufte and E. Weise Moeller. Visual explanations: images and quantities , evidence and narrative, volume 36.";"0,00";"0,00";"0,00"
"TUW-236063";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\groundtruth\TUW-236063.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\result\result-TUW-236063-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\extracted\grobid\grobid-TUW-236063-xstream.xml"")";"The purpose of the project VALCRI is to develop a new system prototype for information exploitation by intelligence analysts working in law enforcement agencies. Information visu-alisation will be a core element of the prototype. Such systems have to be designed to support the sensemaking and reasoning processes of the analysts. One of the goals of the project is, therefore, to get a more thorough understanding of sensemaking processes and to develop a set of recommendations for the design of intelligence analysis systems to help analysts in their work.";"The purpose of the project VALCRI is to develop a new system prototype for information exploitation by intelligence analysts working in law enforcement agencies. Information visu-alisation will be a core element of the prototype. Such systems have to be designed to support the sensemaking and reasoning processes of the analysts. One of the goals of the project is, therefore, to get a more thorough understanding of sensemaking processes and to develop a set of recommendations for the design of intelligence analysis systems to help analysts in their work.";"100,00";"100,00";"100,00"
"TUW-236120";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\groundtruth\TUW-236120.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\result\result-TUW-236120-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\extracted\grobid\grobid-TUW-236120-xstream.xml"")";"As part of the project ""Educational standards in vocational schools"" the responsible Federal Ministry compiled competence models, descriptors and didactic examples for Informatics in technical high schools (HTL). The implementation of this project implies a paradigm shift for the vocational school system , which requires a new pedagogic foundation. This paper presents a didacti-cal concept, that meets the requirements of the educational standards in general and competence orientation in particular. A proposal for the implementation of these educational standards in the competence area industrial information technology (INIT) is presented. The specification is based on selected teaching sessions for micro controller technique. Learner-oriented teaching methods are applied , along with procedures to promote the learners' intrinsic motivation and creativity in general. It became apparent that the competence area INIT conveys remarkably demanding learning outcomes and its implementation proved challenging on multiple levels.";"Austria ' s school system is in a flux, educational standards and competence orientation are the two key topics. The declared objectives in this process are a sustainable increase of quality and a better comparability of educational attainment. This change is already partly implemented: in 2011, the Federal Ministry of Education , Arts and Culture (bm:ukk) released new educational standards for the vocational school system [1] which are orientated towards the development of well-defined competences. To this end, the ministry compiled competence models, descriptors and didactic examples for Informatics in technical high schools (HTL) as part of the project ""educational standards in vocational schools"". These new curricula induce a twofold change: new contents and an orientation towards competence attainment. While the adaptation of the contents is caused by the dynamics of IT, the change towards competences bears challenges on multiple levels. The predominant method in the class rooms is still ex-cathedra teaching where the students assume a rather passive role. This is in conflict with the characteristics of competence orientation which calls for a paradigm shift in teaching. Therefore, new didactic concepts have to be developed that cater to the new contents and the demands of competence orientated teaching. This paper deals with the development and implementation of a didactic concept for the competence field ""Industrial Information Technology"" (INIT) within the vocational schools for information technology (HTL for IT) as a case study. The pursuit of this new concept entails the following central questions: Q1: In which way should teaching be designed as to ensure the educational standards for the competence field INIT? Q2: In which way can be ensured that individual learning goals are attained in combination with personal and social competencies? Q3: To what extent does a specific school influence the implementation of the new curricula with respect to activity-oriented teaching methods? Q4: In which way should teaching be designed as to foster creativity and intrinsic";"0,00";"0,00";"0,00"
"TUW-237297";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\groundtruth\TUW-237297.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\result\result-TUW-237297-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\extracted\grobid\grobid-TUW-237297-xstream.xml"")";"This paper is about the role of e-learning environments to support first year students of computer science. Our goal is to make their transition easier from high school to university learning as well as to introduce our students self-regulated learning step by step. Our results are based on our qualitative study with our students. We analyzed the interviews we carried out with them to identify challenges in the first semester program. In this paper we discussed some possible solutions with support of our e-learning environment we have already established. We know that we need to adapt our system to meet these challenges. We conclude our paper with our future work.";"This paper is about the role of e-learning environments to support first year students of computer science. Our goal is to make their transition easier from high school to university learning as well as to introduce our students self-regulated learning step by step. Our results are based on our qualitative study with our students. We analyzed the interviews we carried out with them to identify challenges in the first semester program. In this paper we discussed some possible solutions with support of our e-learning environment we have already established. We know that we need to adapt our system to meet these challenges. We conclude our paper with our future work. Key words: E-Learning, learners' requirements, higher education, self-regulated learning. INTRODUCTION Seen as a supporting electronic technology e-learning is well-established at several universities for several reasons: to support the lecturers to prepare their courses in a multimodal way, to support students to get all relevant information of a course asynchronously, to provide additional support to students by answering their questions and helping them in their assignments, to name a few. Additional features of creating time schedules, discussion groups and forums among participants, messaging mechanisms, grading and feedback possibilities, etc. make the teaching and learning easier to all stakeholders involved. Nevertheless the most e-learning systems do not provide enough support for learning from students' individual point of view and see learners as ""deindividualized and demoted noncritical homogenous users"" [6, p.273]. In this paper we show that the opposite is the case, especially when considering first semester students of higher education. If it comes to introduce self-regulated learning, it is not only about the material provided for the students [6] or using ICT to improve assessment processes [8], it is about processes like scheduling, planning, and managing the learning activities, or assessment of one's knowledge and preparation for exams, etc. These are the factors for what we show in this paper evidence from our field study. Considering learning as an active, self-regulated, constructive, and situated process [4] [1] e-learning systems need to support learners in management and organization of learning activities, especially when the study requirements are unfamiliar, high (at least higher than expected), and not much individualized. One of the main goals of our paper is to show how to accompany ""novice"" learners in the first semester of a computer science university study to ""advanced beginners"" [3]. Our focus is on e-learning support to self-regulated learning. Since there are several definitions of e-learning, we want to clarify that we refer to the following definition: ""… all forms of electronic supported learning and teaching, which are procedural in character and aim to effect the construction of knowledge with reference to individual experience, practice and knowledge of the learner. Information and communication systems, whether networked or not, serve as specific media (specific in the sense elaborated previously) to implement the learning process."" [6, p.274]. Self-regulation is based on ""students' self-generated thoughts and behaviors that are systematically oriented toward the attainment of their learning goals"" [5, p.59]. This also means that students contribute actively to their learning goals and procedure. The main research question we deal with in this paper is: How can we apply e-learning mechanisms and systems to help students to transit from familiar learning structures and habits at high school to autonomous self-organized learning at a university? This is followed by other questions like: How can we support a newcomer at a university at all? What do we need to consider in a first year computer science (CS)";"0,00";"0,00";"0,00"
"TUW-240858";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\groundtruth\TUW-240858.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\result\result-TUW-240858-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\extracted\grobid\grobid-TUW-240858-xstream.xml"")";"The chase procedure is considered as one of the most fundamental algorithmic tools in database theory. It has been successfully applied to different database problems such as data exchange, and query answering and containment under constraints, to name a few. One of the central problems regarding the chase procedure is all-instance termination, that is, given a set of tuple-generating dependencies (TGDs) (a.k.a. existential rules), decide whether the chase under that set terminates, for every input database. It is well-known that this problem is un-decidable, no matter which version of the chase we consider. The crucial question that comes up is whether existing restricted classes of TGDs, proposed in different contexts such as ontological reasoning, make the above problem decidable. In this work, we focus our attention on the oblivious and the semi-oblivious versions of the chase procedure, and we give a positive answer for classes of TGDs that are based on the notion of guardedness.";;"none extracted value";"0,00";"0,00"
"TUW-245336";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\groundtruth\TUW-245336.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\result\result-TUW-245336-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\extracted\grobid\grobid-TUW-245336-xstream.xml"")";"As technology designers, we often inadvertently inscribe values and concepts in systems beyond what we intended. Further, while we aim to work from a user-and use-centred perspective, we often miss the perspectives of other critical stakeholders and the broader context in which technology systems are to be used. Reflecting on issues of responsible innovation from the bottom up, this paper explores two exemplar cases: designing for older people and health care agendas, and designing for eco-behaviour change agendas. While very different in focus, both highlight the importance that different conceptualisations have on shaping possible solutions, and the challenge of identifying and negotiating competing agendas. This draws attention to the limits of both top-down and bottom-up perspectives and suggests instead a middle out approach to responsible design that considers both policy and practice aspects in the process. It also calls for us as responsible designers to be reflective practitioners aware of our power in inscribing possible futures.";"As technology designers, we often inadvertently inscribe values and concepts in systems beyond what we intended. Further, while we aim to work from a user-and use-centred perspective, we often miss the perspectives of other critical stakeholders and the broader context in which technology systems are to be used. Reflecting on issues of responsible innovation from the bottom up, this paper explores two exemplar cases: designing for older people and health care agendas, and designing for eco-behaviour change agendas. While very different in focus, both highlight the importance that different conceptualisations have on shaping possible solutions, and the challenge of identifying and negotiating competing agendas. This draws attention to the limits of both top-down and bottom-up perspectives and suggests instead a middle out approach to responsible design that considers both policy and practice aspects in the process. It also calls for us as responsible designers to be reflective practitioners aware of our power in inscribing possible futures. 1 Introduction Human Computer Interaction (HCI) as a field is concerned with the design of technologies from a user-and use-centred perspective, to ensure that technologies are not only easy to use but are also useful, meet real needs and fit into people's everyday lives and practices. This is especially important as we move from applications focused on the desktop and the workplace to considering technologies embedded in everyday life. Technology is everywhere now and touches on all spheres of life, not just for work and efficiency, but for how we play, create, relate, socialise, and so on. As noted by Sellen et al. (2009), technology in this new pervasive and mobile world is fundamentally shaping what it means to be human. As such, we are inscribing futures, intended and unintended, through our technology design. This places a huge responsibility on researchers and designers to design technologies in a responsible way; this is a quantum step away from just designing elements of an";"0,00";"0,00";"0,00"
"TUW-245799";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\groundtruth\TUW-245799.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\result\result-TUW-245799-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\extracted\grobid\grobid-TUW-245799-xstream.xml"")";"The diagnosis of failures in train traffic installations can be done in several ways: direct observations and measurements, software assisted diagnosis using specific software packages, process variable monitoring for electronically centralized installations. This work presents basic concepts for Model Based Diagnosis (MBD) that uses fuzzy logic to analyse the integrity of Centralized Traffic Control Installations (CTC) in train traffic. We define the diagnosis relations to be used and show how to apply them to train traffic security installations. Implementing these concepts into an expert system assists maintenance operators in quick failure diagnosis of the train traffic security installations.";"The diagnosis of failures in train traffic installations can be done in several ways: direct observations and measurements, software assisted diagnosis using specific software packages, process variable monitoring for electronically centralized installations. This work presents basic concepts for Model Based Diagnosis (MBD) that uses fuzzy logic to analyse the integrity of Centralized Traffic Control Installations (CTC) in train traffic. We define the diagnosis relations to be used and show how to apply them to train traffic security installations. Implementing these concepts into an expert system assists maintenance operators in quick failure diagnosis of the train traffic security installations. Keywords: (CTC), Model Based Diagnosis, logic relations, fuzzy logic 1. Introduction Railroad Centralized Traffic Control installations are very complex systems that command and control train traffic, being essential in train traffic security. Train stations where the CTC installation is electro-dynamically realised (ED-CTC) have more complex subsystems, each of them, again, having subcomponents that interact with each other. Each subsystem interacts with other subsystems, therefore the safety of the traffic passing an ED-CTC station is ensured when all subsystems function within normal parameter values. Fig. 1 shows the interconnection block schema for a railway station with CTC and ABS (Automatic Block Signalling) installations. All subsystems are controlled through the command console, any failure of the one or more subsystem causing an irregular behaviour of the whole system, behaviour which is usually signalled. Diagnosing the system or the subsystem is necessary in order to detect the failing component(s) that are the cause of the faulty behaviour. Currently, for these types of installations, the failure diagnosis is done by direct observations of the various signals and measurements of equipment states.";"0,00";"0,00";"0,00"
"TUW-247301";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\groundtruth\TUW-247301.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\result\result-TUW-247301-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\extracted\grobid\grobid-TUW-247301-xstream.xml"")";;;"none extracted value";"none expected";"NA"
"TUW-247741";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\groundtruth\TUW-247741.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\result\result-TUW-247741-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\extracted\grobid\grobid-TUW-247741-xstream.xml"")";"Evaluating the quality of segmentations is an important process in image processing, especially in the medical domain. Many evaluation metrics have been used in evaluating segmentation. There exists no formal way to choose the most suitable metric(s) for a particular segmentation task and/or particular data. In this paper we propose a formal method for choosing the most suitable metrics for evaluating the quality of segmenta-tions with respect to ground truth segmentations. The proposed method depends on measuring the bias of metrics towards/against the properties of the the seg-mentations being evaluated. We firstly demonstrate how metrics can have bias towards/against particular properties and then we propose a general method for ranking metrics according to their overall bias. We finally demonstrate for 3D medical image segmentations that ranking produced using metrics with low overall bias strongly correlate with manual rankings done by an expert.";"Evaluating the quality of segmentations is an important process in image processing, especially in the medical domain. Many evaluation metrics have been used in evaluating segmentation. There exists no formal way to choose the most suitable metric(s) for a particular segmentation task and/or particular data. In this paper we propose a formal method for choosing the most suitable metrics for evaluating the quality of segmenta-tions with respect to ground truth segmentations. The proposed method depends on measuring the bias of metrics towards/against the properties of the the seg-mentations being evaluated. We firstly demonstrate how metrics can have bias towards/against particular properties and then we propose a general method for ranking metrics according to their overall bias. We finally demonstrate for 3D medical image segmentations that ranking produced using metrics with low overall bias strongly correlate with manual rankings done by an expert. Index Terms-image segmentation; evaluation met-rics; selection 1 Introduction 1.1 The need to understand metrics: Many evaluation metrics for image segmentation have been introduced; most researchers choose the evaluation metrics arbitrarily or according to their popularity. Investigating met-rics would help researchers to better understand them and help companies and stakeholders to save effort and time reaching optimal systems [1]. A poorly defined metric may lead to inaccurate conclusions like selecting suboptimal models when comparing the performance of classifiers [2]. Many researchers have investigated the drawbacks of particular metrics given particular properties of the data being classified. As a special case of classification, image segmentation is also affected by these drawbacks. The following are some examples: Hausdorff distance is very sensitive to noise and least squares based evaluation methods are very sensitive to out-liers [3]. Mutual information doesn't utilize spatial information inherited in images because only voxel relationships are considered but not the neighborhoods [4]. Information theoretical measures have a non-convergent baseline which depends on the ratio between the number of data points and the number of classes. Therefore this class of measure needs chance correction [5]. Commonly used measures (precision, recall and F-measures) are biased and don't consider the level of chance [6]. Choosing evaluation metrics is very important and application-dependent; when evaluating imbalanced datasets, the metric choice is not obvious [2]. Metrics have different properties with respect to their correlation with user satisfaction criteria and their ease of interpretation [7]. Benhabiles et al. [8] validated 250 automatic segmentations against their corresponding ground truth segmentations using four different evaluation metrics. The results were then compared with manual ratings from 40 human observers. They found that the correlations between the ranking based on the manual ratings and the rankings based on the evaluation metrics vary between 30% and 80% depending on the used metric. Research in the last decades generally results in the relative system improvement achieved becoming smaller and smaller. As a result, sensitivity and fidelity of evaluation metrics become increasingly critical. When improvements are small, metrics with high sensitivity are needed to measure small but real improvements and also with high fidelity to distinguish";"0,00";"0,00";"0,00"
"TUW-247743";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\groundtruth\TUW-247743.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\result\result-TUW-247743-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\extracted\grobid\grobid-TUW-247743-xstream.xml"")";"The Self-Organizing Map (SOM) is a useful and strong tool for data analysis, especially for large data sets or data sets of high dimensionality. SOM visualizations map the data model dimensions to visual dimensions like color and position, thus they help exploring the SOM. Visualization can also involve the data itself so that it helps accessing information that are not available in the trained SOM, thereby enabling a deeper look inside the data. If the data comes with supervised class labels, these labels can be also involved in the visualization, thus enabling the user to have a clearer idea about the data and the structures learned by the SOM. In this work we propose a novel SOM visualization method, namely the SOM class coloring, which is based on the data class labels. This method finds a colored partitioning of the SOM lattice, that reflects the class distribution. SOM class coloring helps discovering class information such as class topology, class clusters, and class distribution. Furthermore class labels can be assigned to new data items by estimating the point on the lattice, that best represents the data item and then assigning the class of the partition that includes this point to the data item.";"Dimensionalität. SOM Visualisierungen bilden die Dimensionen des Datenmod-ells auf graphische Dimensionen wie Farbe und Position ab, so helfen sie der Navigation und dem Erforschen von dem SOM. SOM Visualisierungen können auch die Daten selbst einbeziehen, so dass der Zugri auf Informationen möglich wird, die in einem reinen SOM nicht verfügbar sind. Dadurch wird ein tieferer Einblick in die Daten möglich. Wenn die Daten mit klassen gekennzeichnet sind, können diese Klassen auch in der Visualisierung einbezogen werden, so dass, eine klarere Idee über die Klassinformation gewonnen wird. In dieser Arbeit schlagen wir eine neuartige SOM Visualisierungsmethode, nämlich die SOM Klassenfär-bung vor, welche auf den Datenklassen beruht. Diese Methode ndet eine farbige Partition des SOM-Gitters, die die Klassenstruktur widerspiegelt. Diese Visu-alisierung ermöglicht das entdecken von Klassinformation wie Klassenstruktur, Klassenverteilung und Klassenclusters. Auÿerdem können neue Daten Klassen zugeordnet werden und zwar indem der Punkt auf dem SOM-Gitter ermittelt wird, welcher das neue Datum (Messwert) am besten repräsentiert; das neue Datum wird dann jener Klasse zugeordnet, die die Partition repräsentiert, auf der sich der Punkt bendet. Abstract The Self-Organizing Map (SOM) is a useful and strong tool for data analysis, especially for large data sets or data sets of high dimensionality. SOM visualiza-tions map the data model dimensions to visual dimensions like color and position, thus they help exploring the SOM. Visualization can also involve the data itself so that it helps accessing information that are not available in the trained SOM, thereby enabling a deeper look inside the data. If the data comes with supervised class labels, these labels can be also involved in the visualization, thus enabling the user to have a clearer idea about the data and the structures learned by the SOM. In this work we propose a novel SOM visualization method, namely the ii colored partitioning of the SOM lattice, that reects the class distribution. SOM class coloring helps discovering class information such as class topology, class clusters, and class distribution. Furthermore class labels can be assigned to new data items by estimating the point on the lattice, that best represents the data item and then assigning the class of the partition that includes this point to the LIST OF FIGURES vi 2.7 Gabriel Graph Visualization [Aup03] (a)Notation for the three data qualities dened in this method, from top to bottom: the topology of the classes and the way they are connected and the density of the connections between the dierent components, where two classes are drawn with their densities and topology; the class and quality graph G5 provides the same of information as in G3 in addition to the visualization of border and isolated components. Toolbox overview An illustration of some features of the SOM Toolbox with banksearch data (A)The SOM lattice with units represented as pie charts (B)Zoomed details view with labels and pie charts (C)Pie chart representing the unit position and the class contribution (D)Smoothed data histogram vi-sualization as an example visualization (E)DMatrix visualization as another example (F)There are other visualizations that can be selected from the";"0,00";"0,00";"0,00"
"TUW-251544";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\groundtruth\TUW-251544.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\result\result-TUW-251544-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\extracted\grobid\grobid-TUW-251544-xstream.xml"")";"Die Wiener Zauberschule der Informatik (WIZIK) führt Kinder der Primarstufe an die Denkweise der Informatik heran und vermittelt ihnen erste informatische Kompetenzen. Die Kinder lernen spielerisch verschiedene Problemlösungsstrategien kennen und erhalten einen ersten Einblick in die Grundlagen logischen und prozessorientierten Denkens.";"heran und vermittelt ihnen erste informatische Kompetenzen. Die Kinder lernen spielerisch verschiedene Problemlösungsstrategien kennen und erhalten einen ersten Einblick in die Grundlagen logischen und prozessorientierten Denkens.";"0,00";"0,00";"0,00"
"TUW-252847";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\groundtruth\TUW-252847.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\result\result-TUW-252847-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\extracted\grobid\grobid-TUW-252847-xstream.xml"")";"This paper provides an overview of the Retrieving Diverse Social Images task that is organized as part of the MediaEval 2016 Benchmarking Initiative for Multimedia Evaluation. The task addresses the problem of result diversification in the context of social photo retrieval where images, meta-data, text information, user tagging profiles and content and text models are available for processing. We present the task challenges, the proposed data set and ground truth, the required participant runs and the evaluation metrics.";"This paper provides an overview of the Retrieving Diverse Social Images task that is organized as part of the MediaEval 2016 Benchmarking Initiative for Multimedia Evaluation. The task addresses the problem of result diversification in the context of social photo retrieval where images, meta-data, text information, user tagging profiles and content and text models are available for processing. We present the task challenges, the proposed data set and ground truth, the required participant runs and the evaluation metrics. 1. INTRODUCTION";"100,00";"100,00";"100,00"
"TUW-255712";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\groundtruth\TUW-255712.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\result\result-TUW-255712-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\extracted\grobid\grobid-TUW-255712-xstream.xml"")";"Abstract argumentation is a rich research subfield of AI and till today, numerous frameworks for it have been proposed. It is thus natural to ask whether one can translate between these structures, and what are the price and consequences of undergoing this process. Although every study explains how a given structure relates to the cornerstone of abstract argu-mentation-Dung's framework-there are less results available concerning the connections between more advanced formalisms. Moreover, the existing research is not particularly systematized or classified in a way that would clearly show us the properties of a given transformation. In our work, we address these issues by creating an in-depth compendium on the intertranslatability of argumentation frameworks, describing approximately eighty translations. Furthermore, we provide a system for analyzing a given transformation in terms of its functional, syntactical, semantical and computational properties and the underlying methodology.";"argumentation is a rich research subfield of AI and till today, numerous frameworks for it have been proposed. It is thus natural to ask whether one can translate between these structures, and what are the price and consequences of undergoing this process. Although every study explains how a given structure relates to the cornerstone of abstract argu-mentation-Dung's framework-there are less results available concerning the connections between more advanced formalisms. Moreover, the existing research is not particularly systematized or classified in a way that would clearly show us the properties of a given transformation. In our work, we address these issues by creating an in-depth compendium on the intertranslatability of argumentation frameworks, describing approximately eighty translations. Furthermore, we provide a system for analyzing a given transformation in terms of its functional, syntactical, semantical and computational properties and the underlying methodology. Over the last years, argumentation has become an influential subfield of artificial intelligence, with applications ranging from legal reasoning (Bench-Capon, Prakken, and Sartor 2009) to dialogues and persuasion (McBurney and Parsons 2009; Prakken 2009) to medicine (Fox et al. 2010; Hunter and Williams 2012) to eGovernment (Atkinson, Bench-Capon, and McBurney 2006). Within it, we can distinguish the abstract argumentation approaches, at the heart of which lies Dung's argumentation framework (Dung 1995). Since the structure itself was relatively limited, as it took into account only the conflict relation between the arguments, it inspired the search for more general models (Brewka, Pol-berg, and Woltran 2014). Throughout the years, many of its extensions were proposed, ranging from the ones employing various strengths and preferences to those that focus on researching new types of relations between arguments (Baroni et al. 2011; Cayrol and Lagasquie-Such an amount of frameworks should not come as a surprise. Argumentation is a wide area with numerous applications , in which one has to face different classes of problems. Frameworks of a given type can be seen as tools to model particular issues and concepts, which on one side gives us more insight into how to approach the problems, but on the other affects the framework's design. Nevertheless, with so";"0,00";"0,00";"0,00"
"TUW-256654";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\groundtruth\TUW-256654.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\result\result-TUW-256654-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\extracted\grobid\grobid-TUW-256654-xstream.xml"")";"A section is a contiguous region of memory, to which data or code can be appended (like the Forth dictionary). Assembly languages and linkers have supported multiple sections for a long time. This paper describes the benefits of supporting multiple sections in Forth, interfaces and implementation techniques.";"A section is a contiguous region of memory, to which data or code can be appended (like the Forth dictionary). Assembly languages and linkers have supported multiple sections for a long time. This paper describes the benefits of supporting multiple sections in Forth, interfaces and implementation techniques. • You put B in allocated memory. Unfortunately , that usually means that B does not survive a savesystem, and it's also cumbersome if B is a growable structure. 1 Introduction A section is a contiguous memory area, to which new data can be appended at the end; the Forth dictionary is a section. Assemblers and linkers have supported multiple sections or segments for many decades [Lev00]. In contrast, Forth traditionally has had only one section; some systems have had separated headers (another section), and cross-compilers have uninitialized memory for buffer:, but by and large, Forth systems have made do with just one section: the dictionary. With multiple sections, each section has it's own start, dictionary pointer (what here reads), and end (used in unused, but otherwise not used much). This paper presents various uses of sections and why they are better than the current workarounds (Section 2), presents a programming interface (Sec-tion 3), and discusses various implementation approaches (Section 4).";"0,00";"0,00";"0,00"
"TUW-257397";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\groundtruth\TUW-257397.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\result\result-TUW-257397-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\extracted\grobid\grobid-TUW-257397-xstream.xml"")";"Configuration files, command-line arguments and environment variables are the dominant tools for local configuration management today. When accessing such program execution environments, however, most applications do not take context, e.g. the system they run on, into account. The aim of this paper is to integrate unmodified applications into a coherent and context-aware system by instrumenting the getenv API. We propose a global database stored in configuration files that includes specifications for contextual interpretations and a novel matching algorithm. In a case study we analyze a complete Debian operating system where every getenv API call is intercepted. We evaluate usage patterns of 16 real-world applications and systems and report on limitations of unforeseen context changes. The results show that getenv is used extensively for variability. The tool has acceptable overhead and improves context-awareness of many applications.";"Configuration files, command-line arguments and environment variables are the dominant tools for local configuration management today. When accessing such program execution environments, however, most applications do not take context , e.g. the system they run on, into account. The aim of this paper is to integrate unmodified applications into a coherent and context-aware system by instrumenting the getenv API. We propose a global database stored in configuration files that includes specifications for contextual interpretations and a novel matching algorithm. In a case study we analyze a complete Debian operating system where every getenv API call is intercepted. We evaluate usage patterns of 16 real-world applications and systems and report on limitations of unforeseen context changes. The results show that getenv is used extensively for variability. The tool has acceptable overhead and improves context-awareness of many applications. 1 Introduction The goal of context-oriented programming (COP) is to avoid the tedious, time-consuming and error-prone task of implementing context awareness manually, and instead adapt the application's behavior using the concept of layers [1, 12]. Each layer represents one dimension of the context relevant to the application. Contextual values [27] act as variables whose values depend on layers. A program execution environment consists of the environment variables and key/value pairs retrieved from configuration files. A program execution environment can be tightly integrated with contextual values [21]. Context awareness [5] is a property of software and refers to its ability to correctly adapt to the current context. Our aim is to make applications context-aware that previously were not. For example, an important context for a browser is the network it uses. In a different network, different proxy settings are required to successfully retrieve a web page. We want the browser to automatically adapt itself to the network actually present, i.e., make it context-aware in respect to the network. Markus Raab Institute of Computer Languages, Vienna University of Technology e-mail: markus.raab@complang.tuwien.ac.at 2 Markus Raab Although COP eases the writing of new software, there remains a huge corpus of legacy software that cannot profit from context awareness. Our paper aims at intercepting the standard API getenv in a way that COP-techniques are applied to unmodified applications. We focus on getenv because we found that it is used extensively. Our interception technique, however, does not make any assumption on the API. We recommend to specify the values and the context of the program execution environments separately. This configuration specification contains place-holders, each representing a dimension of the context: [/phone/call/vibration] type=boolean context=/phone/call/%inpocket%/vibration In this example, vibration is a contextual value of type boolean and %inpocket% a placeholder to be substituted in contextual interpretations. Thus, the value of vibration changes whenever inpocket changes. E.g., when a context sensor measures body temperature only on one side of the gadget, it will change the value of %inpocket%. Thus, when the mobile phone is in the pocket, it will turn on vibration. When the mobile phone is lying on a table, it will turn off vibration to prevent falling down when someone calls. If needed, users can even specify further context. For example , some users dislike the context-dependent feature as described. Our approach inherently allows users to reconfigure every parameter in every context. To turn on vibration if the phone is not in the pocket, we configure our device differently: In this paper we analyze the popular getenv() API. The function getenv() is standardized by SVr4, POSIX.1-2001, 4.3BSD, C89, and C99. Because of this standardization and ease of use it is adopted virtually everywhere, even in core libraries such as libc. It allows developers to query the environment. Using standard getenv implementations developers have to act carefully: settings valid in the current context can differ from those received through getenv. To reduce the danger of assuming wrong context information we propose to use a context-aware implementation. We implement it in the whole system by intercepting every getenv API call.";"0,00";"0,00";"0,00"
"TUW-257870";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\groundtruth\TUW-257870.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\result\result-TUW-257870-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\extracted\grobid\grobid-TUW-257870-xstream.xml"")";"This position paper describes a critical incident from an early AAL project related to the design decisions made about which features to include. In order to give the older users of a sensor-based telecare monitoring system more tangible value, a number of non-sensor-based interactive services were incorporated into the system which was installed in a residential facility. These services were chosen based on recommendations and input from older people. In the end though, many services were not used and actually contributed to the system being removed from residences.";"This position paper describes a critical incident from an early AAL project related to the design decisions made about which features to include. In order to give the older users of a sensor-based telecare monitoring system more tangible value, a number of non-sensor-based interactive services were incorporated into the system which was installed in a residential facility. These services were chosen based on recommendations and input from older people. In the end though, many services were not used and actually contributed to the system being removed from residences. willing to invest money. They also received some national funding for later development steps.";"100,00";"100,00";"100,00"
