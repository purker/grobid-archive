<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Users\Angela\git\grobid\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.4.5-dummy" ident="GROBID" when="2017-12-29T00:16+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Retrieving Diverse Social Images at MediaEval 2016: Challenge, Dataset and Evaluation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bogdan</forename><surname>Ionescu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandru</forename><surname>Lucian</surname></persName>
						</author>
						<title level="a" type="main">Retrieving Diverse Social Images at MediaEval 2016: Challenge, Dataset and Evaluation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Gînsc ˘ a Maia Zaharieva * LAPI</term>
					<term>University Politehnica of Bucharest</term>
					<term>Romania CEA</term>
					<term>LIST</term>
					<term>France alexandruginsca@ceafr bionescu@alphaimagpubro University of Vienna &amp; Vienna University of Technology</term>
					<term>Austria maiazaharieva@univieacat Bogdan Boteanu Mihai Lupu Henning Müller LAPI</term>
					<term>University Politehnica of Bucharest</term>
					<term>Romania Vienna University of Technology</term>
					<term>Austria HES-SO</term>
					<term>University of Applied Sciences Western Switzerland bboteanu@alpha</term>
				</keywords>
			</textClass>
			<abstract>
				<p>This paper provides an overview of the Retrieving Diverse Social Images task that is organized as part of the MediaEval 2016 Benchmarking Initiative for Multimedia Evaluation. The task addresses the problem of result diversification in the context of social photo retrieval where images, meta-data, text information, user tagging profiles and content and text models are available for processing. We present the task challenges, the proposed data set and ground truth, the required participant runs and the evaluation metrics. 1. INTRODUCTION</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Search</head><p>1 ). Participants are required, given a ranked list of query-related photos retrieved from Flickr 2 , to refine the results by providing a set of images that are at the same time relevant to the query and to provide a diversified summary of it. Compared to the previous editions, this year's task includes complex and general-purpose multi-concept queries.</p><p>The requirements of the task are to refine these results by providing a ranked list of up to 50 photos that are both relevant and diverse representations of the query, according to the following definitions: Relevance: a photo is considered to be relevant for the query if it is a common photo representation of the query topics (all at once). Bad quality photos (e.g., severely blurred, out of focus, etc.) are not considered relevant in this sce- nario; Diversity: a set of photos is considered to be diverse if it depicts different visual characteristics of the query topics and subtopics with a certain degree of complementarity, i.e., most of the perceived visual information is different from one photo to another.</p><p>To carry out the refinement and diversification tasks, par- ticipants may use the social metadata associated with the images, the visual characteristics of the images, informa- tion related to user tagging credibility (an estimation of the global quality of tag-image content relationships for a user's contributions) or external resources (e.g., the Internet).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">DATASET</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">TASK DESCRIPTION</head><p>The task is built around the use case of a general ad-hoc image retrieval system, which provides the user with diverse representations of the queries (see for instance Google Image 3 * This task is partly supported by the Vienna Science and Technology Fund (WWTF) through project ICT12-010. multi-topic queries related to events and states associated with locations from the 2015 dataset <ref type="bibr">[14]</ref>), a user annotation credibility set (credibilityset) containing information for ca. 300 location-based queries and 685 users (different than the ones in devset and testset -updated version of the 2015 dataset <ref type="bibr">[14]</ref>), a set providing semantic vectors for general English terms computed on top of the English Wikipedia (wikiset), which could help the participants in developing advanced text models, and a test set (testset) containing 65</p><p>Copyright is held by the author/owner(s). description, photo id, tags, Creative Com- mon license type, the url link of the photo location from Flickr, the photo owner's name, user id, the number of times the photo has been displayed, etc), and ground truth for both relevance and diversity.</p><p>Apart from the metadata, to facilitate participation from various communities, we also provide the following content descriptors:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">GROUND TRUTH</head><p>-convolutional neural network based descriptors -generic CNN based on the reference convolutional neural network (CNN) model provided along with the Caffe framework</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5</head><p>Both relevance and diversity annotations were carried out by expert annotators. For relevance, annotators were asked to label each photo (one at a time) as being relevant <ref type="figure">(value  1), non-relevant (0) or with "don't know" (-1). For devset, 9</ref> annotators were involved, for credibilityset 9 and for testset 8. The data was partitioned among annotators such that in the end each image has been marked by 3 different an- notators. The final relevance ground truth was determined after a lenient majority voting scheme. For diversity, only the photos that were judged as relevant in the previous step were considered. For each query, annotators were provided with a thumbnail list of all relevant photos. After getting fa- miliar with their contents, they were asked to re-group the photos into clusters with similar visual appearance (up to 25). Devset and testset were annotated by 5 persons, each of them annotating distinct parts of the data (leading to only one annotation). An additional annotator acted as a master annotator and reviewed once more the final annotations.</p><p>(this model is learned with the 1,000 ImageNet classes used during the ImageNet challenge); and an adapted CNN based on a CNN model obtained with an identical architecture to that of the Caffe reference model. Adaptation is done only for the 2015 location-based multi-topic queries (35 queries from the devset), i.e., the model is learned with 1,000 tourist points of interest classes of which the images were automati- cally collected from the Web <ref type="bibr">[16]</ref>. For the other queries, the descriptor is computed as the generic one, because queries are diverse enough and do not require any adaptation;</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">RUN DESCRIPTION</head><p>-text information that consists as in the previous edition of term frequency information, document frequency informa- tion and their ratio, i.e., TF-IDF, which is computed on per image basis, per query basis and per user basis (see <ref type="bibr">[17]</ref>);</p><p>Participants were allowed to submit up to 5 runs. The first 3 are required runs: run1 -automated using visual information only; run2 -automated using text informa- tion only; and run3 -automated using text-visual fused without other resources than provided by the organizers. The last 2 runs are general runs: run4 and run5 -every- thing allowed, e.g., human-based or hybrid human-machine approaches, including using data from external sources (e.g., Internet). For generating run1 to run3 participants are al- lowed to use only information that can be extracted from the provided data (e.g., provided descriptors, descriptors of their own, etc).</p><p>-user annotation credibility descriptors that give an au- tomatic estimation of the quality of the users' tag-image content relationships. These descriptors are extracted by visual or textual content mining: visualScore (measure of user image relevance), faceProportion (the percentage of im- ages with faces), tagSpecificity (average specificity of a user's tags, where tag specificity is the percentage of users hav- ing annotated with that tag in a large Flickr corpus), lo- cationSimilarity (average similarity between a user's geo- tagged photos and a probabilistic model of a surrounding cell), photoCount (total number of images a user shared), uniqueTags (proportion of unique tags), uploadFrequency (average time between two consecutive uploads), bulkPro- portion (the proportion of bulk taggings in a user's stream, i.e., of tag sets that appear identical for at least two dis- tinct photos), meanPhotoViews (mean value of the number of times a user's image has been seen by other members of the community), meanTitleWordCounts (mean value of the number of words found in the titles associated with users' photos), meanTagsPerPhoto (mean value of the number of tags users put for their images), meanTagRank (mean rank of a user's tags in a list in which the tags are sorted in de- scending order according the the number of appearances in a large subsample of Flickr images), and meanImageTagClar- ity (adaptation of the Image Tag Clarity from <ref type="bibr">[18]</ref> using as individual tag language model a tf/idf language model).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">EVALUATION</head><p>Performance is assessed for both diversity and relevance. The following metrics are computed: Cluster Recall at X (CR@X) -a measure that assesses how many different clus- ters from the ground truth are represented among the top X results (only relevant images are considered), Precision at X (P@X) -measures the number of relevant photos among the top X results and F1-measure at X (F1@X) -the har- monic mean of the previous two. Various cut off points are to be considered, i.e. <ref type="bibr">, X=5, 10, 20, 30, 40, 50</ref>. Official rank- ing metric is the F1@20 which gives equal importance to diversity (via CR@20) and relevance (via P@20). This met- ric simulates the content of a single page of a typical Web image search engine and reflects user behavior, i.e., inspect- ing the first page of results with priority.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">CONCLUSIONS</head><p>4 http://creativecommons.org/. 5 http://caffe.berkeleyvision.org/.</p><p>The 2016 Retrieving Diverse Social Images task provides participants with a comparative and collaborative evalua- tion framework for social image retrieval techniques with explicit focus on result diversification. This year in particu- lar, the task explores the diversification in the context of a challenging, ad-hoc image retrieval system, which should be able to tackle complex and general-purpose multi-concept queries. Details on the methods and results of each indi- vidual participant team can be found in the working note papers of the MediaEval 2016 workshop proceedings. <ref type="bibr">[1]</ref> A.W.M. Smeulders, M. Worring, S. Santini, A. Gupta, R. Jain, "Content-based Image Retrieval at the End of the Early Years", IEEE Transactions on Pattern  <ref type="bibr">[3]</ref> R. Priyatharshini, S. Chitrakala, "Association Based Image Retrieval: A Survey", Mobile Communication and Power Engineering, Springer Communications in Computer and Information Science, 296, pp. <ref type="bibr">17-26, 2013. [4]</ref> R.H. van Leuken, L. Garcia, X. Olivares, R. van Zwol, "Visual Diversification of Image Search Results", ACM World Wide Web, pp. 341-350, 2009. <ref type="bibr">[5]</ref> M.L. Paramita, M. Sanderson, P. Clough, "Diversity in Photo Retrieval: Overview of the ImageCLEF Photo Task 2009", ImageCLEF 2009. <ref type="bibr">[6]</ref> B. Taneva, M. Kacimi, G. Weikum, "Gathering and Ranking Photos of Named Entities with High Precision, High Recall, and Diversity", ACM Web Search and Data Mining, pp. 431-440, 2010. <ref type="bibr">[7]</ref> S. Rudinac, A. Hanjalic, M.A. Larson, "Generating Visual Summaries of Geographic Areas Using Community-Contributed Images", IEEE  <ref type="bibr">[8]</ref> R. Agrawal, S. Gollapudi, A. Halverson, S. Ieong, "Diversifying Search Results", ACM International Conference on Web Search and Data Mining, pp. <ref type="bibr">5-14, 2009. [9]</ref> Y. Zhu, Y. Lan, J. Guo, X. Cheng, S. Niu, "Learning for Search Result Diversification", ACM SIGIR Conference on Research and Development in Information Retrieval, pp. <ref type="bibr">293-302, 2014. [10]</ref> H.-T. Yu, F. Ren, "Search Result Diversification via Filling up Multiple Knapsacks", ACM International Conference on Conference on Information and Knowledge Management, pp. <ref type="bibr">609-618, 2014. [11]</ref> D.-T. Dang-Nguyen, L. Piras, G. Giacinto, G. Boato, F.G.B. De Natale, "A Hybrid Approach for Retrieving Diverse Social Images of Landmarks", IEEE International Conference on Multimedia and Expo, pp.  <ref type="bibr">[13] B. Ionescu, A. Popescu, M. Lupu, A.L. Gˆınsc˘Gˆınsc˘ a, B.</ref> Boteanu, H. Müller, "Div150Cred: A Social Image Retrieval Result Diversification with User Tagging Credibility Dataset", ACM MMSys, Portland, Oregon, USA, 2015. <ref type="bibr">[14]</ref> B. Ionescu, A.L. Gˆınsc˘Gˆınsc˘ a, B. Boteanu, M. Lupu, A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">REFERENCES</head><p>Popescu, H. Müller, "Div150Multi: A Social Image Retrieval Result Diversification Dataset with Multi-topic Queries", ACM MMSys, Klagenfurt, Austria, 2016. <ref type="bibr">[15]</ref> B. Ionescu, A. Popescu, A.-L. Radu, H. Müller, "Result Diversification in Social Image Retrieval: A Benchmarking Framework", Multimedia Tools and Applications, 2014. <ref type="bibr">[16]</ref> E. Spyromitros-Xioufis, S. Papadopoulos, A. Gˆınsc˘Gˆınsc˘ a, A. Popescu, I. Kompatsiaris, I. Vlahavas, "Improving Diversity in Image Search via Supervised Relevance Scoring", ACM International Conference on Multimedia Retrieval, ACM, Shanghai, China, 2015. <ref type="bibr">[17]</ref> B. Ionescu, A. Popescu, M. Lupu, A.L. Gˆınsc˘Gˆınsc˘ a, H.</p><p>Müller, "Retrieving Diverse Social Images at  <ref type="bibr">[18]</ref> A. Sun, S.S. Bhowmick, "Image Tag Clarity: in Search of Visual-Representative Tags for Social Images", SIGMM workshop on Social media, 2009.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>MediaEval 2016 Workshop, October 20-21, 2016, Hilversum,</head><label>MediaEval 2016</label><figDesc></figDesc><table>1 https://images.google.com/. 
2 https://www.flickr.com/. 
3 https://en.wikipedia.org/. queries (19,017 Flickr photos). 
Each query is provided with the following information: 
query text formulation (the actual query formulation used 
on Flickr to retrieve all the data), a ranked list of up to 300 
photos in jpeg format retrieved from Flickr using Flickr's 
default "relevance" algorithm (all photos are Creative Com-
mons licensed allowing redistribution 
4 ), an xml file contain-
ing metadata from Flickr for all the retrieved photos (e.g., 
photo title, photo </table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl/>
			</div>
		</back>
	</text>
</TEI>
