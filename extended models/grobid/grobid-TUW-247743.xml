<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Users\Angela\git\grobid\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.4.5-dummy" ident="GROBID" when="2017-12-29T00:16+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main"></title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>Die Selbstorganisierende Karte (SOM) ist ein nützliches und starkes Werkzeug für die Datenanalyse, besonders für groÿe Datensätze oder Datensätze von hoher</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<textClass>
				<keywords>MASTERARBEIT Coloring of the Self-Organising Maps based on class labels ausgeführt am Institut für Softwaretechnik der Technischen Universität Wien unter Anleitung von Ao Univ Prof Andreas Rauber durch Taha Abdel Aziz Quellenstrasse 24B/13/13 A1100 Wien Wien, am 3 Jänner 2016 Unterschrift i Kurzfassung</keywords>
			</textClass>
			<abstract>
				<p>Dimensionalität. SOM Visualisierungen bilden die Dimensionen des Datenmod-ells auf graphische Dimensionen wie Farbe und Position ab, so helfen sie der Navigation und dem Erforschen von dem SOM. SOM Visualisierungen können auch die Daten selbst einbeziehen, so dass der Zugri auf Informationen möglich wird, die in einem reinen SOM nicht verfügbar sind. Dadurch wird ein tieferer Einblick in die Daten möglich. Wenn die Daten mit klassen gekennzeichnet sind, können diese Klassen auch in der Visualisierung einbezogen werden, so dass, eine klarere Idee über die Klassinformation gewonnen wird. In dieser Arbeit schlagen wir eine neuartige SOM Visualisierungsmethode, nämlich die SOM Klassenfär-bung vor, welche auf den Datenklassen beruht. Diese Methode ndet eine farbige Partition des SOM-Gitters, die die Klassenstruktur widerspiegelt. Diese Visu-alisierung ermöglicht das entdecken von Klassinformation wie Klassenstruktur, Klassenverteilung und Klassenclusters. Auÿerdem können neue Daten Klassen zugeordnet werden und zwar indem der Punkt auf dem SOM-Gitter ermittelt wird, welcher das neue Datum (Messwert) am besten repräsentiert; das neue Datum wird dann jener Klasse zugeordnet, die die Partition repräsentiert, auf der sich der Punkt bendet. Abstract The Self-Organizing Map (SOM) is a useful and strong tool for data analysis, especially for large data sets or data sets of high dimensionality. SOM visualiza-tions map the data model dimensions to visual dimensions like color and position, thus they help exploring the SOM. Visualization can also involve the data itself so that it helps accessing information that are not available in the trained SOM, thereby enabling a deeper look inside the data. If the data comes with supervised class labels, these labels can be also involved in the visualization, thus enabling the user to have a clearer idea about the data and the structures learned by the SOM. In this work we propose a novel SOM visualization method, namely the ii colored partitioning of the SOM lattice, that reects the class distribution. SOM class coloring helps discovering class information such as class topology, class clusters, and class distribution. Furthermore class labels can be assigned to new data items by estimating the point on the lattice, that best represents the data item and then assigning the class of the partition that includes this point to the LIST OF FIGURES vi 2.7 Gabriel Graph Visualization [Aup03] (a)Notation for the three data qualities dened in this method, from top to bottom: the topology of the classes and the way they are connected and the density of the connections between the dierent components, where two classes are drawn with their densities and topology; the class and quality graph G5 provides the same of information as in G3 in addition to the visualization of border and isolated components. Toolbox overview An illustration of some features of the SOM Toolbox with banksearch data (A)The SOM lattice with units represented as pie charts (B)Zoomed details view with labels and pie charts (C)Pie chart representing the unit position and the class contribution (D)Smoothed data histogram vi-sualization as an example visualization (E)DMatrix visualization as another example (F)There are other visualizations that can be selected from the</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>LIST OF FIGURES</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>vii</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.6</head><p>Areas in Voronoi diagram (A) are similar to these produced by color ooding <ref type="bibr">(B). . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32 3.7</ref> Relation between Voronoi diagram and Delaunay triangulation(DT). (A)a set of point (sites) in a 2-D space(B). (B)Voronoi faces (thin lines) and Delaunay triangulation (thick lines) of the sites in A. (C)The circumsphere of every triangle in DT contains no sites inside. <ref type="bibr">. . . . . . . . . . . . . . . . . . 33 3.8</ref> Sweepline algorithm. The slides 1-9 animate the algorithm while nding the Voronoi diagram of the three sites in slide 1. In slide 2 a parabola for the upper site arises because the sweepline passes this site. In the next two slides the parabola grows while the sweepline moves. In slide 5 two parabolas for the two lower points arise because the sweepline passes these points. In the rest slides all parabolas grow while the line sweeping. The intersections of the parabolas form the edges of the Voronoi diagram as in slide 9 (red lines). . . 35</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.9</head><p>Bowyer-Watson algorithm when adding new site. (A)The faces to be deleted are all faces whose circumsphere or itself contains the new added site. (B)Faces to delete are colored, and new faces are marked in dashed lines. (C)The resulting Delaunay triangulation after insertion and update. <ref type="bibr">. . . . . . . . 36 3.10 Region substitution. . . . . . . . . . . . . . . . . . . . . . . . . . . . 38 3.11 Sector partitioning. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39</ref> 3.12 (A)Notations used int this section(Note that we use the colors as class names for simplifying). (B)Connection lines between regions that have same classes. 40 from r4 that are nearest to L5 from the other side, are colored in blue. that gives the eect of an area extending over two regions In regions r2 and r3 the same thing is done with L2 and L3, because L2 and L3 have a common point the resulting colored area appears as a thin area, that extends over two regions In r4 the line L4 has the length 0 (is equivalent to a point), thus the resulting colored area has a circular shape. . . . . . . . . . . . . . . . . 41</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.13</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>viii</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.15</head><p>Finding the lines segments (A)At rst each region is colored with the dominant class color. The class red is isolated because no neighbor region have this class; aline segment s with length 0 is added and the attractor function is applied.</p><p>In the nished coloring we see the class red as a circular area within the region.</p><p>(B)The class yellow in the region r remains after painting the dominant colors.</p><p>There is one neighbor, that have the same class which is r1, thus we add a segment s from the site to the middle of the common edge. The same thing happens when painting r1, from the view of r1 there is a neighbor region r that have the same class yellow, thus adding the line segment s1. After painting r and r1 we get continuous yellow area extending over the two regions. (C)In this case the two neighbor regions r1 and r2 are self neighbors and shares r with the vertex at s because all of the three regions have the class cyan, we add the line segment s with length 0 at the common vertex, so that the resulting class cluster is continuous for all of the three regions as it is seen in the nished coloring. (D)Because r have two neighbors of r1 and r2, that have the same class yellow, but they are not self neighbors, we add two segments s1 and s2 each one from the site location to the middle of the edge of the corresponding region. After nishing, we have a yellow stripe, that connect r1 and r2 crossing r. <ref type="bibr">. . . . . . . . . . . . . . . . . . . . . . . . . . 46</ref> 3.16 Smoothing of the border by using weighted line segments. (A)The border at the common edge has a zigzag form because the areas are not equal. (B)The line segments are weighted by using the contribution value as weights for the face points and the average for the common point. (C) Smooth partitioning as a result of the weighted sorting. . . . . . . . . . . . . . . . . . . . . 48</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.17</head><p>The minimum visible class parameter. (A) Visualization with parameter set to 0. (B) The same data visualized with the parameter set to 50% and <ref type="bibr">(C)</ref> set the parameter set to 100%. <ref type="bibr">. . . . . . . . . . . . . . . . . . . . . . 49</ref> 3.18 Chessboard Visualization vs. smooth partition visualization. (A)the smooth partition visualization, (B)Chessboard visualization. In each case with and without voronoi borders. . . . . . . . . . . . . . . . . . . . . . . . . . 50</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4.1</head><p>Class coloring of the iris data. Class visualization overview of the banksearch data set. (A)Pie chart visual- ization. (B)Smoothed class coloring of the same data set. For the categories and themes see Table 4.1. . . . . . . . . . . . . . . . . . . . . . . . . . 60</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4.3</head><p>Class Coloring with dierent values of the parameter minimum visible class.</p><p>(A)the parameter is set to 100%. (B)the parameter is set to 50%. (C)the parameter is set to 0%. . . . . . . . . . . . . . . . . . . . . . . . . . . 61</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4.4</head><p>Class coloring with Voronoi cell border. (A) to (F) are signicant samples showing some strengths and drawbacks of the visualization. . . . . . . . . . 62</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4.5</head><p>Chessboard visualization of the banksearch dataset. . . . . . . . . . . . . 63</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4.6</head><p>Radio station data set. (A) Pie chart visualization. (B) Class coloring of the data set. The text labels describe the main categories of the genres. See <ref type="bibr">[LR06]</ref>. 64</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4.7</head><p>Class coloring of the radio search data set. The parameter minimum visi- ble class is set to dierent values (A) 0% (B) 30% (C) 60% (D)100%. The dominant classes in (C) do not agree with these seen by the human eye in (A). 65</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4.8</head><p>Class coloring of the radio search data set after the correction of the algorithm.</p><p>The parameter minimum visible class is set to dierent values (A) 0% (B) 30% (C) 60% (D)100%. The dominant classes in (C) agree with these seen by the human eye in (A). . . . . . . . . . . . . . . . . . . . . . . . . . . . . 66</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4.9</head><p>Class coloring of the banksearch data set after the correction of the algorithm.</p><p>Some of the drawbacks that was found in the least section have disappeared after the correction. For example the area marked with a circle is to be compared with Figure 4.4-(C). . . . . . . . . . . . . . . . . . . . . . . 67</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Chapter 1 Introduction</head><p>Analyzing data usually needs more than getting pure statistical properties of the data set. There are many methods for quickly producing overall summaries of a data set. For example, ve-number summary consisting of some statistical values (greatest, median, lower quartile and upper quartile) provides a help understand- ing simple data sets of low dimensionality and limited volume <ref type="bibr">[Kas97]</ref>. Data structure and topology such as nding the clusters and class distribution of the data is very essential. The more data there is available the more dicult it is to understand this data set. Also, the dimensionality of the feature space represent- ing the data is a factor. For this problem there is an essential need of methods for data exploration and especially the methods, that can discover and illustrate eectively the structure of the data. Data mining is one of the elds where such exploratory methods are needed in the form of tools in Knowledge discovery in database (KDD), whose purpose is to nd new knowledge from databases where dimensionality, complexity, or amount of data is too large or complex for pure human observation to be studied. Data mining is now an important area of re- search, responding to the presence of large databases in commerce, industry, and research. Methods of data exploration vary depending on the nature of the data and the goal of study: clustering methods are the more conventional ones; they aim to organize the data patterns into groups, so that patterns in one group are more similar to each other than to those in another group. In other words clus- tering tries to reduce the amount of data items by grouping them. Clustering is useful in several areas such as pattern analysis, grouping, and machine-learning,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2</head><p>document retrieval, image segmentation, and many other applications <ref type="bibr">[AJ99]</ref>.</p><p>Projection methods try to reduce the dimensionality of the data: they map the multidimensional data features to low dimensional space (usually 2D or 3D) that represents the data and can be easier visualized and studied. Of course this map- ping should preserve the original data topology as much as possible. If the goal of projection is to visualize the data, then low output dimensionality should be cho- sen in order to achieve meaningful visualizations. Visualization methods aim to map the dimensions of the data to visual dimensions such as position, color, etc. in a way that the observer can get a deeper insight in the structure of the data.</p><p>Of course the visual dimensions are limited, so if the data is of high dimensional- ity, direct visualization will be dicult, not helpful or impossible <ref type="bibr">[Ves99]</ref>. In such a case visualization should make use of the two previous methods (Clustering and projection) as a pre-processing step to prepare the data in a form that it can be eciently visualized. Self-Organizing Maps (SOMs) <ref type="bibr">[Koh97]</ref> are ecient tools that implicitly combine all of the previous methods. A SOM is a neural network consisting of low dimensional grid (mostly 2D) of nodes that are trained with high-dimensional data. The nodes on the trained map represent the original data in a manner that similar data points in the feature space are mapped to nodes which are close to each other on the map. SOMs are strong tools used in the data exploration. Some properties that distinguish SOM from the other data mining tools are that it is numerical instead of symbolic, nonparametric, and capable of learning without supervision <ref type="bibr">[Kas97]</ref>; Many variants and extensions of the standard SOM were proposed which try to cover some drawbacks of the standard SOM. In practical applications the process of deciding which method to use is essential: this depends on the nature of the data and the goal of ex- ploration; the following questions are in any case to be resolved: What kind of structure the method can extract?; How does it illustrate this structure?; Does the method reduce the data dimensionality and/or the number of data items?;</p><p>Which and if so which type of data pre-processing is necessary?. The focus of this work is visualization of the SOM, in particular SOMs that are trained with labeled data. It is meaningful here to distinguish between two things that are of great importance in this work: SOM is based on an unsupervised learning process</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3</head><p>because the learning is data-driven. Regardless of this fact, class information can be used in the form of labels which can be then used for exploration and visualization purposes; such labels can be collected in a pre-processing step, or even as a post-processing step. These labels do not aect the training process and the resulting structure or topology, but they help in generating visualizations displayed on top of the nished map. In particular we propose in this work a visualization method of SOM which is based on such class labels namely a SOM coloring that depends on these labels.</p><p>The remainder of this thesis is structured as follows: In Chapter 2 we will dis- cuss some related concepts and methods such as the concept of the Self-Organizing Map, some clustering and classication methods and methods for projection and visualization of the trained SOM. In Chapter 3 we present our novel method for coloring the SOM based on class labels. In Chapter 4 we test and evaluate our method with some collected data sets. Finally we present our conclusions in Chapter 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Chapter 2 Related Work</head><p>In this chapter we will introduce some of the concepts and worksljubíme that have been proposed which are closely related with the underlying issue, coloring of the SOM. In Section 2.1 the general issue of the SOM will be introduced.</p><p>In Section 2.2 and 2.3 we will give some idea about the two processes, which are closely related to visualization: these are clustering and projection. In Sec- tion 2.4 visualization will be discussed briey especially in respect to the SOM,</p><p>Some SOM-based clustering, projection and visualization methods will be also discussed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Self organizing map</head><p>The Self-Organizing Map (SOM) in the basic form is an articial neural network model. The nodes of this network are trained to various input patterns from the feature space. SOMs use an unsupervised learning process: no a priori clas- sication for the input is needed. The result of the learning process is a map lattice (in this work we assume a 2 dimensional lattice, which is usually the case ), that map the high-dimensional feature space of the input in a way, so that similar inputs in the feature space have associated nodes that are close to each other on the map lattice, thus reducing the high-dimensionality of the feature space. This projection is capable (a) to help clustering the data and (b) to ap- proximately preserve the original data topology of the input. SOMs are therefore especially useful for data visualization and exploration purposes. I a classic SOM,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5</head><p>each neuron represents an n-dimensional column vector , where n depends on the data space dimensionality (input vectors dimensionality). The reason for using one-and two-dimensional grids is that space structures of higher dimensionality cause problems with data display. Neurons are usually located in the nodes of the two-dimensional grid with rectangular or hexagonal cells. The number of neurons in the lattice determines the resulting resolution and the granularity of data presentation.</p><p>During the training not only the winning neuron is modied but its neighbors as well, although the strength of the adaption may depend on their distance from the winner neuron. This allows considering the SOM to be a method of projecting the data nonlinearly onto a lower-dimensional display. When using this algorithm the vectors similar in the initial space get close to each other on the nal map SOMs <ref type="bibr">[Koh97]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1">The map initialization</head><p>If the map contains tens of thousands of neurons then it usually takes too much time to train the system for the practical tasks. Thus making choice of the nodes quantity requires a reasonable trade-o. Before training the map it is necessary to initialize the weight coecients of the neurons. The learning rate factor can be signicantly increased by an appropriate initialization method, thus bringing better results. In general, there are three methods of weights initialization:</p><p>• Initialization with random values. All the weights are assigned small ran- dom values.</p><p>• Initialization with patterns. Initial values are set to randomly chosen pat- terns of the training sample.</p><p>• PCA-based initialization. The weights are initialized by values of the vec- tors ordered along a two-dimensional subspace spanned by the two principal</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>6</head><p>Figure 2.1: Adapting the best-matching unit and its neighbors weights The input vector coordinates are marked with a cross, coordinates of the map nodes before modication are shown as full circles and after modication as empty circles.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.2">The training process</head><p>The learning process consists of sequential corrections of the vectors representing neurons. On every step of the learning process a random vector is chosen from the initial data set and then the best-matching neuron coecient vector (up now BMU or best-matching unit) is identied. This is the most similar unit to the input vector. The word 'similarity' in this task means the distance between the vectors. There are various distance metrics that can be used for this purpose.</p><p>The most common one is the Euclidean metric, where the the winner unit must meet the following relation: − W c = min i − W i <ref type="bibr">(2.1)</ref> Where W c is the winner vector at index c, X is the input vector and W i is any Unit vector in the SOM. After the BMU is found the neural network weights are adapted. The winning unit and its neighbors adapt to represent the input by modifying their reference vectors towards the current input.  <ref type="bibr">(2.5)</ref> Where σ(t) is a diminishing function of time. This value is often called the radius of the neighborhood. At the beginning of the learning procedure it is fairly large, but it is made to gradually shrink during learning. Towards the end a single winning neuron is trained. Most frequently the linear decreasing function of time is used.</p><p>Let us proceed to the learning rate function α(t). where A and B are constants. There are two main phases in the learning pro- cess. At the beginning the learning rate and the neighborhood radius are fairly large, which allows ordering the neurons vectors according to the sample patterns distribution. Then the weights are accurately adjusted with the learning rate pa- rameters much smaller than they initially were. In case of using PCA-based initialization the rst step of rough adjustment can be skipped. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Clustering and classication</head><p>Cluster analysis is the organization of a collection of patterns into clusters based on similarity among these patterns. That is patterns belonging to a cluster are (more) similar to each other to than those in another clusters. sentation of a data set. In the clustering context, a typical data abstraction is a compact description of each cluster, usually in terms of cluster proto-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>types.</head><p>To have optimal clustering, there are many parameters to be tuned.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">Hierarchical algorithms</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2">Partitive algorithms</head><p>In this approach the algorithm identies the partition by minimizing some error function. The number of clusters is either already predened or determined by the algorithm by trying various numbers of clusters. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.</head><p>The most common partitive clustering algorithm is the k-means algorithm, which partitions N data points into K disjoint subsets S j containing N j data points. where x n is a vector representing the nth data point and c j is the geometric centroid of the data points in S j . clusters by applying a suitable clustering algorithm. The number of the cluster prototypes (nodes) must be much larger than the expected number of clusters but much smaller than the number of the data patterns. The computational</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.3">Two</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>12</head><p>is because the SOM algorithm can be applied to large data sets and the compu- tational complexity is linearly proportional with the number of input patterns, but on the other hand, the complexity scales quadratively with the number of map nodes (prototypes). However this overhead can be solved with special op- timization techniques. The problem of noise and outliers is also solved because the prototypes are the local averages of the original data produced by training the SOM und thus less sensible to outliers and nodes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Projection</head><p>The structure of the data cloud formed by prototype vectors is the key for un- derstanding the data by giving answers to questions like what and where are the clusters, which shape has the data cloud, etc. Projection methods are very useful in answering such questions. Projecting data on to its underlying subspace can detect its real structures, facilitate functional analysis, and help making a judg- ment. Principally there are two types of projection methods: linear methods such as principal component analysis (PCA), and nonlinear such as multidimensional scaling (MDS) and SOM.</p><p>PCA is a linear transformation that transforms the data to a new coordinate system such that the greatest variance by any projection of the data comes to lie on the rst coordinate (called the rst principal component), the second greatest variance on the second coordinate, and so on. PCA can be used for dimensionality reduction in a dataset while retaining those characteristics of the dataset that contribute most to its variance, by keeping lower-order principal components and ignoring higher-order ones. PCA is an optimal linear projection algorithms, because it has the minimum mean-square-error values between the original data vectors and the points in the projection, thereby it achieves the equation 2 Where X = [x 1 , x 2 , .., x n ] T is the n-dimensional input vector. {q j , j = 1, 2, ...m, m ≤ n} are orthogonal vectors representing principal directions. The term q T j repre- sents the projection of x onto the j-th principal dimension.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>13</head><p>has limited power for capturing nonlinear relationships in practical data. For this reason extensions to nonlinear PCA are used such as Generalized PCA, Kernel PCA, and principal curves and surfaces <ref type="bibr">[Yin03]</ref>.</p><p>Multidimensional scaling (MDS) is a nonlinear projection method, that tries to project points of data onto a two-dimensional plot, thereby preserving as close as possible the inter-point metrics. For this purpose arbitrary optimization algorithms can be used, MDS does not specify that a certain algorithm to be used. In the optimization process local minima and divergence problems may occur. Furthermore, this process incurs high commotional costs. These costs depend on the selected algorithm and the starting conguration. The overall structure of the data is maintained thereby.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.1">Sammon's Mapping</head><p>Sammon's mapping is a common example of MDS projection. This algorithm aims to minimize the dierence between inter-point distances in the original data and in the projection. The Sammon's mapping algorithm minimizes the stress function 2.9.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2</head><p>S sammon =</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Σ</head><p>[d ij − D ij ] <ref type="bibr">(2.9)</ref> i&lt;j d ij d ij where X i and X j are two points in the input space and Y i and Y j are their projection points respectively, d ij is the Euclidean distances between X i and X j , D ij is the Euclidean distance between Y i and Y j , and n is the number of patterns.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>n</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Σ</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>n i&lt;j</head><p>It performs a recursive learning algorithm using the Newton optimization to achieve the optimal conguration <ref type="bibr">[Sam69]</ref>. Although this algorithm converges relatively fast, it has the drawback of high computational costs and higher risk of sub-optimal solution due to local minima. Furthermore Sammon mapping is a point-to-point mapping which means that all the inter-point distances must be explicitly calculated for every data set. In the broader since this means that there is no explicit mapping function. Accommodation of new data point needs recalculation of all points, which is a serious problem for applications that deal</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>14</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Visualizations</head><p>Data visualization is an important process in data mining. Generally, to visual- ize multi dimensional data, two steps are needed: the rst is vector quantization,</p><p>where the original data set is reduced to a smaller one, that still represents the original, suppresses noise and is easier to work with than the original set. The second is a vector projection to a low dimensionality (usually up to 3D). Of course in our context this projection must still represent the original vector set in terms of distances between vectors or at least topological ordering. Visualization is the process of projection the real dimensions to visual dimensions. Typical dimen- sions in this sense include positions, size, color and text labels. The number of visual dimensions available is essential: If there are many features involved in the visualization, it may be impossible to show them on one map and this leads to the issue of multiple visualization maps. It is an issue to visualize the data in a form, that it is easy to recognize corresponding areas on dierent maps. This process is called linking. Linking should be obvious to the observer. For example in case of SOM the linking dimension is the position. The same object in dierent map visualizations correspondents to the same position on the map. There are various algorithms to do these steps. In this and the following subsections a selection of visualization methods will be presented. A special case of visualization is the visualization achieved with SOM. SOM combine both of the two steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2.4.1</head><p>Visualization of SOM with Unlabeled data Visualization of complex data with high dimensions and large number of items is one of the most important applications of SOM. Visualization with SOM is a special case of visualization in term of implicitly combining vector quantization and projection, while in other methods projection is completely subordinate to quantization. Another deference is that the projection grid of the SOM is regu- larly shaped: this makes it very easy to compare or link dierent visualizations.</p><p>The map resulting from a trained SOM can itself help as an exploration tool; thereby similar data points are mapped to the same unit or to points that are placed near to each other; the labels mapped to these units on the map can be</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>15</head><p>tion, there are three categories of visualization methods: the rst is getting an abstract idea about the data including the overall shape and cluster structure.</p><p>The second category aims at the analysis of prototype vectors and characteristic of clusters. The third one has the purpose of examining and classication of new data. There also two categories of visualizations according to the source of information used: visualizations that shows the map in relation to the data set such as hit histograms and smoothed hit histograms, P-Matrix, etc., and visu- alizations that are derived from the model vectors which aim at showing cluster structure and boundaries such as U-Matrix, component planes visualizations and clustering of the SOM. These and other visualization methods will be discussed in the following subsections <ref type="bibr">[PDR]</ref>.</p><p>U-Matrix, P-Matrix and U*-Matrix U-Matrix <ref type="bibr">[Ultsch1992]</ref> is a distance-based visualization method that aims to vi- sualize the SOM by displaying the local distance structure on top of the SOM lattice; at each unit of the map the local distance to its neighbors is visualized as a height. This height is dened as the sum of distances to all immediate neighbors normalized by the largest height. This results in mountain ranges as boundaries for data clusters. This method produces good visualization if the data has clearly separated clusters, but problems can occur if the data has overlapping clusters or even slowly changing densities. P-Matrix <ref type="bibr">[Ult03a, Ult05]</ref> is a density-based visualization method in which the density distribution of the data is measured.</p><p>The value of local density for each unit is sampled and displayed with the help of the Pareto Density Estimation (PDE) method proposed in <ref type="bibr">[Ult03b]</ref>, because it is impossible to calculate the values for all possible neighbours. The U*-Matrix visualization combines both of the methods, thereby meeting the advantages and avoiding the drawbacks of them: the local distances produced by the U-Matrix result in a desired eect in areas with low data density because these areas should present cluster boundaries; on the other site local distances result undesired eect in dense regions because such distances could disrupt the cluster characteristic in these regions, therefore U*-Matrix aims to dampened the local distances of the U-Matrix in dense regions and to emphasize them in loose regions; in regions of</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>16</head><p>Figure 2.3: U-Matrix, PMatrix, and U*Matrix <ref type="bibr">[Ult05]</ref>. (A)Dataset consisting of a mixture of two Gaussians with 2048 points each. (B)The U-Matrix of the dataset, darker colors correspond to large distances. (C)The P-Matrix of the dataset, darker colors correspond to larger dinsities. (D)The U*-Matrix of the dataset, which shows clearly the two Gaussians.</p><p>demonstrates the visualizations. It is taken from <ref type="bibr">[Ult05]</ref>: Histograms Visualizing of the SOM</p><p>Hit histogram is a graphical representation of a SOM that shows the number of hits, i.e. mappings of data items, occurred per SOM node. Typically, while</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>17</head><p>Figure 2.4: Hit Histogram visualization methods <ref type="bibr">[Ves99]</ref> (a)The size of the black hexagon is proportional to the value of the histogram in the corresponding unit. (b)The height of the bar tells the same thing as in (a).</p><p>When SOM is already trained, these hit values are visualized on top of the SOM lattice. There are many methods to visualize these values. Figure 2.4 shows some of these methods. Hit histogram <ref type="bibr">[PRM02, Ves99]</ref> visualization is simple and can be achieved with linear costs. Yet, it has some drawbacks: data samples match normally to many SOM nodes in dierent rates assigning such samples only to the BMU results inaccurate visualization; assigning this sample to all matching hits also results in same inaccuracy because no information is given about the degree of match.</p><p>This problem is solved by using smoothed data histograms(SDH) proposed in <ref type="bibr">[PRM02]</ref>. SDH aims to visualize the clusters on the SOM by taking into account the probability density of the high-dimensional data. The SOM is used as a basis for the visualization. Instead of assigning a data sample to a specic unit, the SDH estimates a membership degree of every data sample to each unit. This membership is calculated based on distances between the data sample and all other units. The eect of these distances in the membership is controlled with a smoothing parameter s. Figure 2.5 shows a data set and its SDH visualization</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>18</head><p>Figure 2.5: SDH visualization with dierent values of the smoothing parameter s <ref type="bibr">[PRM02]</ref>. The Data set consists of 5000 samples, that are randomly drawn from a probability distribution, that is a mixture of 5 Gaussians. The SOM is of 10X10 units arranged on a rectangular grid.</p><p>more details are visualized but not the overall cluster topology. By increasing of the value of s, the general characteristics of the data become more visible.</p><p>Vector eld visualization A method of visualizing the cluster structure based on vectors is proposed in <ref type="bibr">[PDR]</ref>. The vectors are drawn on top of the SOM lattice in the form of arrows having one vector per unit. The length and direction of the vectors are determined so that they give information about the cluster structure. The directions of arrows point to the location of cluster centers and the lengths visualize the location of the unit inside the cluster in term of being a boundary unit or not: long arrows indicates that the unit is located closely to the cluster boundary while short arrows means in general that the unit is located closely to the cluster center or between two clusters to which the unit have equal similarity. In particular, the length of the arrow demonstrates the ratio between the dissimilarity of the unit to the area the arrow is pointing to and the dissimilarity of the unit to the  <ref type="bibr">[PDR]</ref> visualization but it has the some other advantages especially that the cluster centers visualized by the arrows are computed with the consideration of global areas. The visualization provides an overview of the cluster structure of the data, the locations of cluster centers and the interpolating units, and the similarity of units to the surrounding areas. The inuence of the neighborhood kernel on the visualization is controlled with a parameter σ. A large value of σ emphasizes the global cluster structure by enlarging the surrounding area that is taken into account while a smaller value emphasizes the ne details of clusters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>19</head><p>Figure 2.6 shows an example that illustrates this visualization. The example is taken from <ref type="bibr">[PDR]</ref> and shows vector eld visualizations with dierent values of σ of a 30X40 SOM that is trained with the Phonetic data set. Low values of σ lead to very granular visualizations, where only direct neighbors are taken into account for the computation of each arrow and thus only local gradients can be observed, as visualized in Figure 2.6 -(a) with σ = 1. By increasing this value, the clustering structure revealed shifts gradually from local towards global. Figure 2.6 -(b) provides a far better overview on the clustering structure, and individual regions can be distinguished with σ = 5. In All of the visualizations presented in the previous sections either only make use of the SOM units to visualize the data set or access the data vectors to make calculations that help by visualizing the data topology. In this subsection, we</p><p>will present two visualization methods that assume the availability of labeled data and uses these (class) labels to produce a visualization, which shows the class topology of the data.</p><p>Graph-based class visualization A dierent visualization method is proposed in <ref type="bibr">[Aup03]</ref>, which is based on graph analysis. This method assumes a labeled data set; that is the class label for each datum is known. This means in case of a trained SOM the class for each data vector is known and the problem is to extract information about the topology of the classes by making use of graph analysis, in particular Gabriel graph. Gabriel graph GG is a sub graph of Delaunay triangulation <ref type="bibr">[For97]</ref> in which every edge has a forbidden region of a diameter with length equal to the edge length, Figure 2.7 - c; that is for every edge eij in GG the open ball with diameter |eij| contains no other edges other than eij. This method denes three qualities of data, that can occur in the visualization, Figure 2.7 -a, which are:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1</head><p>. isolated data units for which all neighbours through the graph have a class dierent from its own.</p><p>2. border for which there is at least one of its neighbours, which has the same class as its own.</p><p>3. normal for which all neighbours have the same class as its own.</p><p>After applying the algorithm described in <ref type="bibr">[Aup03]</ref> visualizations like this in Fig- ure 2.7 -G3 and G5 are obtained.</p><p>Pie-chart class visualization</p><p>In this section we will illustrate the pie chart visualization described in <ref type="bibr">[RPM03]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CHAPTER 2. RELATED WORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>21</head><p>each unit is represented with a pie chart, that is located in the position of the unit. A pie chart has a number of sectors equal to the number of classes assigned to the represented unit. Each one of these sectors represents one of the classes and has an arc length which is proportional to the contribution fraction of this class.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Summary</head><p>Exploring complex data sets, such as data sets with large vector dimensionality or/and large number of vector needs technics that help dealing with them, look- ing deeper inside them and understanding the relations inside them. The most important ones of these technics are clustering, projection and visualization.</p><p>Cluster analysis is the organization of a collection of patterns into clusters based on similarity among these patterns. There are types of clustering algo- rithms: Hierarchical algorithms, which can be divided into agglomerative and di- visive algorithms, corresponding to bottom-up and top-down strategies, to build a hierarchical clustering tree. Agglomerative algorithms begin with each element in its own cluster and the clusters are merged together iteratively according to the similarity between them. The process continue until all elements are in one clus- ter. The second type of clustering algorithms is the Partitive algorithm, which identies the partition by minimizing some error function. Visualization methods can have some of many goals such as visualizing the overall topology of the data, showing the local details inside clusters, showing labels and class or helping to assign new data items. Two steps are needed for visualization: The rst is vector quantization, i.e. reducing data set to a smaller one, that still represents the original data. The second is a vector projection to a low dimensionality. Visualization is the process of projection the real dimensions </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>22</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Chapter 3 Methods</head><p>In this work a novel method for visualization of the SOM will be proposed. The aim of this method is to provide a visualization by coloring the SOM lattice. In particular, a class visualization based on class labels is provided. We will call this method SOM Class Coloring and we will use the shortcut SCC to denote it. It is clear that this method assumes (1)a trained SOM, and (2) a data set consisting of labeled data items. These labels are taken as the classes of the data set and used to color the SOM lattice. SOM Class Coloring produces a coloring on top of the SOM grid having the lattice of a trained SOM as a visualization ground.</p><p>The purpose is to obtain a visualization which 1. reect the class topology of the data set.</p><p>2. help assigning new data items and labeling them with one or more classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>.</head><p>The rst step in the SOM Class Coloring is assigning the labels to the SOM nodes; this is a straightforward step. In this work we assume that this assignment is given and we don't take this step in more details.</p><p>This chapter is structured as follows: In Section 3.1 the SOM Toolbox is de- scribed in more details, which provides the base conguration for the SOM Class Coloring. Section 3.2 describes the problem formally. In Section 3.3 we will describe one of the tries to achieve the coloring by letting color point ood in all directions. Section 3.4 introduces the use of graphs for coloring the SOM by</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>26</head><p>discussing some theoretical issues like voronoi diagrams and Delaunay triangula- tions which are important for our coloring method and we will then introduce the attractor functions which provide the key algorithms for achieving the smoothed class coloring.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">SOM Toolbox</head><p>Although the trained SOM itself can be used to explore the data, there are infor- mation that cannot be visualized with the pure trained SOM; some information is always lost through dimension mapping which cannot hold the exact topology and all data relations. Many interfaces and tools were proposed which visual- ize the SOM and try to cover the drawback caused by lost information through mapping.</p><p>In this chapter the SOM Toolbox described in <ref type="bibr">[RPM03]</ref> will be illustrated having the focus on the visualization aspects. It comprises visualization tools which use the SOM as a visualization start point, it provides then the possibility to make various visualizations on top of the SOM lattice. As input the tool takes a vector le with the raw data, a class info le with the involved class names, and template le with information about the data such as the dimensionality. The tool performs the SOM training process and provides a ground visualization of the trained SOM as a SOM grid with the units represented as pie charts, that show the class contribution. Depending on the details level set by the user, labels can also be displayed on the grid, see Figure 3.1.</p><p>As we can see the SOM Toolbox provides a basis on which we can build various visualizations. In the next sections we will propose SOM Class Coloring</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>27</head><p>Figure 3.1: SOM Toolbox overview An illustration of some features of the SOM Toolbox with banksearch data (A)The SOM lattice with units represented as pie charts (B)Zoomed details view with labels and pie charts (C)Pie chart representing the unit position and the class contribution (D)Smoothed data histogram visualization as an example visualization (E)D- Matrix visualization as another example (F)There are other visualizations that can be selected</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>28</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Problem formulation</head><p>In this section we will give a formal description of the problem.</p><p>Let C = {c 1 , c 2 , ..c n |n ≥ 0} be the set of classes involved in the data set and U = {u 1 , u 2 , ...u m |m ≥ 0} the set of units in the SOM. We assume that the assignment relation A is known which is a function f : U → R n that assigns the classes of the data to the unit in which these data items are represented by, as well as the fractions of contribution for each class, that is f (u) = {a 1 , a 2 , ...a n |a ∈ R} where a i is a real number giving the contribution of class c i in the unit u the other classes.</p><p>We assume that the units are located on a SOM grid of the dimensions p × q where p is number of rows and q is the number of columns which implies a number of units m = p · q.</p><p>The problem is to color the grid plane in such a way that the class topology of the data is visualized and that, according to this coloring, a new data item can be assigned to a class. Figure 3.2 illustrates this base conguration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">SOM coloring by color ooding</head><p>The rst try to color the SOM was to let colored points ood, that is if we imagine the units as colored points and that the colors spread at the same time, we will get a colored map.</p><p>At this point we will try to dene and determine the process of ooding the color; there are several approaches how the ooding of a set of color points can be simulated: the rst one is that in each iteration a point grows to a ring more. The second is that in each iteration a point grows to one graphic grid (for example a pixel) more in this case the question arises which pixel: we can take the next one at the boundary or a randomly selected pixel from the boundary; this process continues as long as the reached areas are not occupied by other ooding points.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>29</head><p>Figure 3.2: Base conguration of the SOM Class Coloring (A)The SOM units located on the grid with their class contributions, that is given by function f. (B)Representation of A as it is in SOM Toolbox, where f is represented with pie chart.</p><p>smoothly and we will get easily a coloring, that represents the class topology.</p><p>Figure 3.3 illustrate the process of color ooding. As we can see, a reasonable coloring was produced by simple color ooding.</p><p>In fact this was too simple case: the main diculty in this issue is that a unit can contain data of many classes, that is it can have many colors. In such cases a direct color ooding can not produce a coloring that meets our expectation. let them ood as in the last example. We note here that this concept is not complete because it doesn't take into account the values of contribution of the classes, but it considers as if all classes were equally involved. That means if this concept works well for the rst step, it must be then extended to consider the contribution values.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>30</head><p>Figure 3.3: Coloring by Simple Flooding assuming one class per unit. (A)Colored points representing the dominant classes of the SOM units, (B to D)The process of color ooding animated in 3 steps. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>31</head><p>Figure 3.5: Instability eect in the color ooding. (A)While growing, the two blue points close the way and prevent the red point from growing to the left side. (B)After making a slight change in the position of one of the blue points, the red point can grow to the left side, which changes the end result dramatically.</p><p>works with critical drawbacks especially the low accuracy and stability: a slice change in the locations of the substitution points could result a dramatic change in the resulting coloring. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Using graphs to color the SOM</head><p>Our next try was to use graphs segmentation for the visualization instead of color ooding. In this case the decision on Voronoi diagrams is straightforward, because it can produce a similar result like that of ooding the colors in Figure 3.3-(B).</p><p>This similarity is clear in Figure 3.6. Voronoi diagrams are discussed in details However we will face the same situation as in color ooding: After the voronoi areas are found, a solution must be found which considers units with many classes (colors) that have dierent ratios. Before we come to this issue, the Voronoi</p><p>Diagrams are discussed in the following subsection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.4.1</head><p>Voronoi diagrams and Delaunay triangulations Voronoi diagram <ref type="bibr">[For97]</ref> of a set of points (n points here called sites) located on a plane is the diagram that partitions this plane into exactly n regions, so that each Voronoi region is assigned to a site and consists of all points on the plane that are closer to this site than to any other site. More formally: Given a set of sites S = {s 1 , s 2 , ..s n } on a plane, then the Voronoi region R of a site s i is R(s i ) = {x : |x − s i | &lt;= |x − s j |, ∀j = i} where x is any point on the given plane, and the Voronoi diagram V of the Voronoi diagrams are generally dened in n − dimentional space. In this work we will consider only 2 − dimensional spaces because a two dimensional visualization underlies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>33</head><p>There is a close relation between Voronoi diagrams and Delaunay triangula- tion. Delaunay triangulation <ref type="bibr">[For97]</ref> of a set of sites is the unique triangulation so that the circumsphere of every triangle contains no sites inside. voronoi diagram and Delaunay triangulation have a duality relation:</p><p>• Each vertex of Voronoi diagram is a center of a circumsphere of a triangle in the DT.</p><p>• Every line segment connecting two sites in two neighboring voronoi regions (two regions with a common edge) is an Edge in the DT. R(s i ), R(s j ) ∈ V (S) and have a common edge e ⇔ s i s j is an edge in DT (S) tained by a balanced search structure called the sweeping line. It is used in many application for solving problems such as nding Delaunay triangulation, voronoi diagrams and other problems like sorting, tree management, etc.</p><p>In this algorithm a sweep line crosses the space and produces events when passing the underlying sites. These events control the computing see Figure 3.8.</p><p>Actually the sweep line splits the problem domain into two regions, an explored region and an unexplored region. It applies an ordering to the problem because we reason about the explored area based on what we have seen so far and ignore the unexplored area, that is only the points crossing the sweep line aect the current computation since all other points are too far away. be very costly. Instead, only instances are calculated when the beach line changes topology for example when the sweepline passes a new site. More Information animate the algorithm while nding the Voronoi diagram of the three sites in slide 1. In slide 2 a parabola for the upper site arises because the sweepline passes this site. In the next two slides the parabola grows while the sweepline moves. In slide 5 two parabolas for the two lower points arise because the sweepline passes these points. In the rest slides all parabolas grow while the line sweeping. Bowyer-Watson algorithm when adding new site. (A)The faces to be deleted are all faces whose circumsphere or itself contains the new added site. (B)Faces to delete are colored, and new faces are marked in dashed lines. (C)The resulting Delaunay triangulation after insertion and update.</p><p>The randomized-incremental algorithm (Bowyer-Watson algorithm)</p><p>The randomized-incremental (RE) <ref type="bibr">[For86]</ref> algorithm in general adds the sites se- quentially to the Delaunay triangulation updating the triangulation after each addition; for this update three steps are needed, as in whose circumspheres contains the added site; the second step is deleting all tri- angles discovered; the third step is the rebuilding of the empty region as a result of the deletion taking into account the new site, so that every new triangle has the new site as a vertex. The eciency of the algorithm depends on the eciency of the used algorithm for discovering faces to be deleted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.2">Voronoi cell coloring</head><p>In the case of single-class nodes, each voronoi cell can be assigned the according class color. As already said in previous chapters the diculty of our problem is that a SOM unit could have to many classes assigned. If the units were assigned to single classes, then coloring the voronoi regions with the corresponding colors should be the optimal solution of the problem.</p><p>In this section we will at rst describe some of the solutions for this problem, which we tried but have failed because of some reasons. Then we will intro-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>37</head><p>expectations.</p><p>We started by a solution that is based on node substitution similar to the substitution described in 4. The sites are arranged by rotation about the node center, so that colors assigned to sites in a region as similar as possible to colors in the neighboring regions.</p><p>Suppose V is the Voronoi diagram before the substitution and V S is the Voronoi diagram for the substitution sites.</p><p>Because the substitution site are located (almost) in the same place as the unit itself, then we expect that the substitution (splitting of the units) makes only local changes in the voronoi regions of V in the meaning that the region boundaries of V exists also in V S and each of them is divided into further partitions. Furthermore, because the neighboring sites are taken into account in the arrangement, we expect to have continuous color regions, that go over the region boundaries of V to the other regions. See <ref type="table" target="#tab_33">Figure 3.10</ref> This method has several problems:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1.</head><p>The class sharing value is not reected, because the areas of the subregions depend on the positions of the sites and the shape of the region. If these areas are to be controlled, the positions of the substitution sites must be moved but this requires that the whole Voronoi diagram to be changed, in the meaning that the region boundaries of V are also changed. For example in Figure 3.10-B, the region R consists of two classes c1 and c2 and thus divided into two areas; if we imagine, that class c1 contributes with only 1%, then we must move the node very close to the neighbor region to get aects the borders of the Voronoi cell itself. This means, that the whole voronoi diagram must be changed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>38</head><p>2. Only moving the sites is not sucient to meet the contribution grads (shar- ing fraction), also the rotation about the node is to be controlled. But this rotation could produce a conict with the aim to tune the own classes with the neighboring classes, as mentioned previously. In other words it could be impossible to consider both of having right areas and right directions for many classes by only positioning the node, because the produced areas depend also on the shape of the Voronoi cell.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.3">Angular segmentation of Voronoi cells</head><p>We gave up the substitution and tried another method, namely to divide the voronoi region into sectors similar to a pie chart. Each of them is dened with an angle. The angles are to be calculated depending on the sharing value of the class and on the neighboring classes as depicted in Figure 3.11. The method solves the drawback in the substitution method which causes a global topology change if the sites are moved. But there is another problem or shortage with this solution: we consider the case in Figure 3.11, for input like in (A) the described method would produce a coloring like in (B). If we let a class have more than one sector, then the resulting coloring could be like (C). Actually, in this case the cells. However what we would like to have is something like the coloring in (D).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>39</head><p>Also in case of an isolated class, we would like to have it as an isolated area in the middle of the Voronoi cell and not as a sector. Such a smoothed coloring can not be reached any using the sectoring method.</p><p>So we stopped our investing in this direction and searched for other methods.</p><p>We found two methods, that were satisfying: the rst is assigning every grid unit (pixel) to a class separately with the help of a distance algorithm, that takes into account the sharing value and the neighborhood and the second method is by coloring the dierent classes on a chessboard like grid with the areas according to their relative frequencies. Both of the two methods are described in details in the following subsections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.4">Smooth partitioning of Voronoi cells</head><p>In this section we will introduce a method for producing a smoothed partitioning of the voronoi regions into class (color) areas.</p><p>At rst let us dene some notations to help expressing the problem and our algorithms to solve them.</p><p>• R = {r 1 , r 2 , ..r n |n ≥ 0} is the set of voronoi regions which is drawn on top of the SOM lattice as described in Section 3.4.1 with the units as faces.</p><p>• C = {c 1 , c 2 , ..c m |m ≥ 0} is the set of all classes that exist in the data set.</p><p>• N (r i ) = {r i 1 , r i 2 , ...} is the set of the neighbor regions of region r i . These Notations used int this section(Note that we use the colors as class names for simplifying). (B)Connection lines between regions that have same classes.</p><p>• C(r i ) = {c i 1 , c i 2 , ...} is the set of classes share in the unit located at the face of r i .</p><p>• S(r i , c j ) ∈ R + is the value of the sharing of the class c j in the region r i .</p><p>• Furthermore we dene the term connection line which is an imaginary line that connects the site of a region r i with the middle of the edge of a neighboring region r n if there is a class c where c ∈ C(r i ) and c ∈ C(r n ). In other words this line denotes that a region has a class that also the neighboring region, to which the line is connected, has, as shown in A key issue in this method is a function for assigning pixels to classes according to a distance function we will call it attractor f unction because it produces an eect which is similar to the magnet eect, this function is applied on a region, a line segment, a class and a number n that denote the number of pixels to be assigned.The function works as follows:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1.</head><p>At rst all pixels (or a predened number of pixel blocks in case lower</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>41</head><p>Figure 3.13: The work of attractor function: In region r1, at rst the 34 grids, that are most close to L1, are colored in red then the next nearest 45 grids are colored in yellow. The 41 grids most nearest to L5 are colored in blue, also 14 grids from r4 that are nearest to L5 from the other side, are colored in blue. that gives the eect of an area extending over two regions In regions r2 and r3 the same thing is done with L2 and L3, because L2 and L3 have a common point the resulting colored area appears as a thin area, that extends over two regions In r4 the line L4 has the length 0 (is equivalent to a point), thus the resulting colored area has a circular shape.</p><p>region are sorted according to their distance to the line segment. The distance here is dened to be the distance to the closest point that lies on the line segment(Note that this distance is dierent from the commonly known distance which measures the length of the normal).</p><p>2. After all pixels are sorted, the rst n pixels (the nearest ones to the line seg- ment), that are not yet assigned, are assigned to the given class. <ref type="table" target="#tab_33">Figure 3.13</ref> shows this functionality.</p><p>The eect of this function is that line segments (we will call them attractors)</p><p>attracts the pixels of a specic color toward them. Depending on the position of this line segments in the Voronoi cell, the length, and direction of the segment,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>42</head><p>• Edge of the Voronoi cell: applying the function on an edge of the region lets the color go to the border, now if the function is also applied to the same edge but from the other side (in the neighboring region), then the resulting area reects a cluster that extends over two regions. see the attractor L5 in Figure 3.13 applied in r1 and r3; this can be applied if two neighboring regions share the same class.</p><p>• Line segment connecting the site with the middle of an edge: • Point attractor: It is a line of length 0. It produces a circular area inside the region and is suitable for coloring isolated classes. See L4 in Figure 3.13.</p><p>In the remainder of this section we will describe the algorithm for the attractor function and then we will describe some rules for nding the ref erence lines de- pending on the classes that are present in a region and in it neighboring regions, so that the resulting coloring reects the class topology of the data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Distance Function</head><p>The underlying distance function measures the distance between a point (grid center) and a line segment. In particular it measures the distance d from a point P to the closest point on the line segment P 0 P 1 . distance is the shorter of the distances between the point and the segment</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>43</head><p>Figure 3.14: Distance between a point and a line segment <ref type="bibr">[Sof]</ref>.</p><p>An easy and sucient way to nd the distance without measuring unnecessary distances is to consider the angles between the segment P 0 P 1 and the vectors P 0 P and P 1 P from the segment endpoints to P . See Figure 3.13. If either of these angles is 90 o , then the corresponding endpoint is the perpendicular base P (b).</p><p>Otherwise, if the angle is not 90</p><p>• , then the base lies to one side or the other of the endpoint according to whether the angle is acute or obtuse. Note that the two tests can be done just using the two dot products W 0 · V and V · V which are also the numerator and denominator of the formula to nd the parametric base of the perpendicular from P to the extended line L of the segment S </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>} Coloring algorithm</head><p>The underlying algorithm for coloring assumes that there is a SOM lattice with units labeled with classes and and their contribution fractions. A voronoi dia- gram is rst found with the units as sites. Then the resulting voronoi regions are colored each separately taking into account all of the neighbor regions. The coloring consists of the following steps:</p><p>• Finding a dominant class for each region and coloring the region with its color. The dominant class is not the class with largest fraction, but the class having the most number of neighbor regions, which have the same class. If the number is equal for several classes, then the dominant class is selected randomly from these.</p><p>• For the remaining classes, line segments are found depending on the classes in the region and in the neighborhood. The attractor function is then ap- plied on these line segments, thereby overwriting the dominant class color.</p><p>The algorithm for the rst step is straight forward: a simple iteration through the neighbor regions and counting the occurrence of the underlying class. If this is done for all classes in the region, then the one with the most hits is selected.</p><p>In the second step, the main issue is how to nd the best suitable line segments</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>45</head><p>class topology as faithfully as possible. In the rest of this subsection, we will describe and improve an algorithm for solving this problem.</p><p>We consider four cases, that can occur concerning the relation between the class c being processed in the region r with the sharing fraction s and between the classes c i in the neighboring regions r n . We also assume that the region r contains p pixels. The following attractor types can be applied:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1.</head><p>The class c is isolated: there is no neighboring region r ∈ r n that contains data of the class c. This is the simplest case because we do not need to consider a cluster that extends to other regions for this class. Thus we use the point attractor by adding a segment with length 0 (a point) in the site location and assign p * s pixels for the class c by applying the attractor function for this class and this line segment. See Figure 3.15 -A 2. There is only one neighboring region r n that contains the class c: in this case we try to attract the colored pixels at the region border close to r n .</p><p>Thus we add a segment line from the site location to the middle point of the common edge e (the edge between r and r n ) and apply the attractor function on this segment with p * s pixels. See Figure 3.15 -B 3. There are two neighbor regions r 1 and r 2 that have the class c and that are themselves neighbors to each others. This implies that r,r 1 ,r 2 all shares a vertex v. In this case we use the point attractor by add a line segment of length 0 to the point v (the common vertex). We do that hoping that when all of the three regions concentrate the color at this point, then we have a colored cloud extending over the three regions. <ref type="figure">See Figure 3.15 -C   4</ref>. There are more than one neighbor region {r 1 , .., r n |n &gt; 1} that have the class c but none of them is a neighbor of the other: we treat each of these regions similar as in case 2 with the dierence that the number of pixels assigned is (p * s)/n that is we add n line segments, one for each region from the site to the middle point of the corresponding common edge. See </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>46</head><p>Figure 3.15: Finding the lines segments (A)At rst each region is colored with the dominant class color. The class red is isolated because no neighbor region have this class; aline segment s with length 0 is added and the attractor function is applied. In the nished coloring we see the class red as a circular area within the region. (B)The class yellow in the region r remains after painting the dominant colors. There is one neighbor, that have the same class which is r1, thus we add a segment s from the site to the middle of the common edge. The same thing happens when painting r1, from the view of r1 there is a neighbor region r that have the same class yellow, thus adding the line segment s1. After painting r and r1 we get continuous yellow area extending over the two regions. (C)In this case the two neighbor regions r1 and r2 are self neighbors and shares r with the vertex at s because all of the three regions have the class cyan, we add the line segment s with length 0 at the common vertex, so that the resulting class cluster is continuous for all of the three regions as it is seen in the nished coloring. (D)Because r have two neighbors of r1 and r2, that have the same class yellow, but they are not self neighbors, we add two segments s1 and s2 each one from the site location to the middle of the edge of the corresponding region. After nishing, we have a yellow stripe, that connect r1 and r2 crossing</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>47</head><p>boring regions have the same color but they do not have smooth bordering. where d(p, p 1 , p 2 ) the normal (not weighted) distance and w 1 , w 2 the weights for the line segment ends p 1 and p 2 respectively. The weighted distance results the eect that pixels tends to be concentrated more around the end point with the larger weight. Now if we have two neighboring regions having the same class like Figure 3.16 -A, where the contribution fractions for the class c are f 1 for region r 1 and f 2 for region r 2 , then we assign the points of the line segments connecting the two regions weights as follows: the face points take the contribution value as a weight w1 = f 1 , w 2 = f 2 , and the common point is assigned a weight of the value w 1 +w 2 2 . See Figure 3.16 -B. This implies that the pixel concentration on both sides of the common edge is the same for both regions regardless of the values of the contribution fractions. The minimum visible class parameter There are Nodes that have classes assigned with a low contribution. Some of these are outliers. If one is interested to have a visualization of a certain abstraction level for example if only the main class clusters are important, the such classes could make an undesired eect, because the are unimportant details.</p><p>For this reason we dened a parameter, the minimum visible class, which can be</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>48</head><p>Figure 3.16: Smoothing of the border by using weighted line segments. (A)The border at the common edge has a zigzag form because the areas are not equal. (B)The line segments are weighted by using the contribution value as weights for the face points and the average for the common point. (C) Smooth partitioning as a result of the weighted sorting. contribution, that a class must have in order to be painted. Setting the parameter to 0 will cause that all details are painted; while setting the parameter to 100% will cause that only the dominant classes are painted. setting the value to any other value will cause that only these classes are painted, that have a contribution equal or greater than this parameter. Note that the eect of this parameter is applied only on classes other than the dominant class, because the dominant class is painted in all cases as a background, which forms a base for painting the other classes. Figure 3.17 shows a comparison between three visualizations, with the parameter set to 0 in the rst and to 50% in the second, and to 100% in the third.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Chessboard visualization</head><p>In this section we describe another alternative for visualizing the regions with multiclass contribution. In this visualization every region r is divided into grids.</p><p>For every class c with contribution f in r, a number m = n * f grids is calculated, where n is the total number of the grids in the region. Now the pixels are are assigned to the classes by using a uniform distribution function. when each pixel is then colored with the corresponding class color we get a visualization like this</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>49</head><p>Figure 3.17: The minimum visible class parameter. (A) Visualization with parameter set to 0. (B) The same data visualized with the parameter set to 50% and (C) set the parameter set to 100%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Summary</head><p>To visualize a trained SOM with labeled data, we propose a method for coloring the SOM lattice according to the class labels. We assume that every unit in the trained SOM has one ore more classes assigned to it and every class is assigned a fraction which gives the contribution of the class in the unit. The Coloring method consists of the following steps:</p><p>• Partitioning the SOM lattice with a Voronoi diagram having the units as sites. That is every unit is the site of a Voronoi cell.</p><p>• Using the attractor function to partition the Voronoi cells, each one sepa- rately taking into account the classes in the unit and in the neighborhood.</p><p>The attractor function is an algorithm, that attracts the colored pixels to a line segment. To partition a Voronoi Cell with the attractor function, we select line segments with suitable lengths, directions, start and end points, taking into ac- count the classes in the unit and the classes in the neighboring units. Finally we apply the attractor function to these line segments and the corresponding classes.</p><p>Visible classes can be ltered according their contribution fractions with the help of the minimum visible class parameter, which can be set interactively by the user.</p><p>There is an alternative visualization which color the Voronoi cells with a chess</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>50</head><p>Figure 3.18: Chessboard Visualization vs. smooth partition visualization. (A)the smooth partition visualization, (B)Chessboard visualization. In each case with and without voronoi borders.</p><p>according a uniform distribution function taking into account the contribution fractions for each class.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Chapter 4</head><p>Experements and evaluation In this chapter we will test and evaluate our work with three data sets, selected from three dierent practical areas: the rst one the iris data set, which is a simple data set consisting of 150 sample of owers. The second data set is the banksearch data set, a standard data set for testing clustering and classication method in the web elds; it consists of 11000 web documents. The third one is an audio data set: it consists of audio samples taken from the broadcast of 8 radio station.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Iris data</head><p>The Iris benchmark database is a data set with 150 random samples of owers with 4 attributes (sepal and petal length and width, of Iris plants). The iris data is a simple data set in relation of class distribution: as it is clear from the visualization there are no SOM nodes with multiple classes, each node has a single class assigned. Even so we can see the benet of the SOM coloring when it is compared with the pure pie chart visualization: with the help of the</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>52</head><p>SOM coloring we can directly see the class topology of the data with sharp class cluster boundaries. In fact the main class clusters can also be seen in the pie chart visualization, but the absence of exact borders make it dicult to decide for data points that are close to the cluster borders and those points that lie near outliers or noise nodes. See the regions marked with circles in Figure 4.1.</p><p>The main benet of the coloring in case of such simple data is the ability to label new data: after we obtain such a coloring from a labeled data set, we can assign a new unlabeled data by nding the best matching unit for this data and then nd in which colored area this unit is located. Note that the best matching unit can be eventually a new node, if so, we can see the benet in comparison with the pure pie chart visualization, where we must estimate and choose one of the nodes in the neighborhood to assign the class.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Text data</head><p>In this section we will use the Banksearch data set described in <ref type="bibr">[SC02]</ref> to test and evaluate our visualization method. Banksearch data set was designed as a standard data set for general use in web document clustering and similar experi- ments and in particular for research in the eld of unsupervised clustering of web documents, so that clustering methods can be tested with a common data set, and thus be easily and objectively compared. It was aimed to use a relatively large number of categories, but with sucient of each category to allow a wide variety of sensible experiments, each using clearly dened subsets of the data set.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>53</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset Id Dataset Category Associated Theme</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Commercial Banks</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Banking &amp; Finance</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Building Societies</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Banking &amp; Finance</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Insurance Agencies</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Banking &amp; Finance</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Java</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Programming languages</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E C/C++ Programming Languages</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F Visual Basic</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Programming languages</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G Astronomy Science</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H Biology Science</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I Soccer Sport</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>J Motor Sport Sport</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>K Sport Sport</head><p>by comparing the pie chart visualization with the smoothed coloring in Fig- ure 4.2 we see that the class clusters are easier to recognize in the smooth coloring, especially those clusters formed by nodes with multiple classes. With pure pie chart visualization, it is dicult to recognize such class clusters. Smooth coloring provides sharp boundaries between class clusters. These boundaries are helpful in the process of assigning new data. cells could be split int two or more parts, which is also not optimal.</p><p>The sample in Figure 4.4 -(D) shows an optimal partitioning in our opinion:</p><p>The class colored in brown form a continuous cluster that extends over many Voronoi cells. The cluster borders are smoothed without zigzag boundaries. Iso-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>55</head><p>class in one Voronoi cell, they are painted as rings in the middle of the Voronoi cell. In fact this good quality holds for most of the cases.</p><p>The sample in Figure 4.4 -(E) shows a typical example of isolated classes.</p><p>Isolated classes are painted as a circle in the middle of the voronoi cell. If there are more than one isolated class, then the they are painted as rings about the circle. The order of painting (which class to which rings) is randomly selected. This visualization is meaningful especially in the areas, where the classes are interleaved, because in this case the smoothed partitioning makes no sense.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Audio data</head><p>In this subsection we will test and evaluate our visualization method by using the data set presented in <ref type="bibr">[LR06]</ref>, where a method for proling radio stations is described. broadcast classic music. The same thing holds also for the genre schlager and the radio station Bgld: Bgld Radio station broadcast very often schlager music. The area represents the genre Pop music (upper part of the gure) is very interleaved. This is meaningful because because Pop music is very popular and is broadcast by almost all radio stations. The same thing holds exactly for the area Speech especially those regions representing news genre, which is also expected, because all radio stations broadcast news.</p><p>In the areas with heavily interleaved classes we face the problem of ambiguous- ness: if the classes are uniformly (or nearly uniformly) distributed, the coloring algorithm fails to nd a denite order to color the areas; namely to nd the dom- inant class, which is painted as a background for the voronoi region, on which</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>57</head><p>the class distribution in the neighborhood. In case of heavily interleaved areas and, the dominant class may be selected randomly. This means that more than one coloring are possible. Figure 4.6 -E shows such a case. On the contrary class distribution that meet the reality with a certain accuracy level, which varies depending on many factors; in such a situation we can ignore the error and the ambiguousness that result from the heavy interleaved class distribution.</p><p>Figure 4.7 shows the class coloring of the data set with dierent values of the parameter minimum visible class. As we see in the gure the visible details decrease as the parameter value increase. After comparing the visualizations areas marked with circles in (A), we see a dominant colors which are brown in the upper area and red in lower one. After setting the parameter to 100% the elementary classes should be eliminated and only the dominant class should be visible. If we look at the visualization in (D) we see that the corresponding areas (areas marked in circles) have a dierent classes than these dominant classes we see with our eyes; namely blue/cyan in the upper area and green in the lower one. This means that there is an error in the coloring algorithm. the actual unit, it counts the regions in the neighborhood which contain this class, then it selects the class with the maximum number of such regions. The algorithm does not take into account the contributions for these classes. This means that if most of the surrounding regions have a class then it is considered the dominant class regardless the contribution fraction of this class. So the marked area in <ref type="bibr">(A)</ref> right/buttom have the class green in most of the cells but in a low contribution, nevertheless it is considered as a dominant class and this is the error.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>58</head><p>the occurrence of the class in the surrounding regions in nding the dominant class; That is, the dominant class is the class for which the sum of the fractions in the surrounding neighborhood is the maximum.</p><p>The same test after the correction produced a satisfying result, which matches the expectation and agree with the dominant class seen by the human eye. The result after correction is illustrated in Figure 4.8.</p><p>Also, we tested the banksearch data set after the correction and we found that some eects, which we considered as sub-optimal, disappeared. For example the eect in Figure 4.4-(C). The class coloring of the banksearch data set after the correction of the error is illustrated in Figure 4.9</p><p>Figure 4.10 shows the chessboard visualization of the radio search data set. In the visualization we can see the main clusters and the class topology and clouds of color mixture in the areas with interleaved class distribution. This coloring is meaningful because if you concentrates only at the full painted areas you see the dominant classes and thus the general class topology. Whereas if you concentrate</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>59</head><p>Figure 4.1: Class coloring of the iris data. The areas marked with circles are examples of</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>60</head><p>Figure 4.2: Class visualization overview of the banksearch data set. (A)Pie chart visual- ization. (B)Smoothed class coloring of the same data set. For the categories and themes see</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>61</head><p>Figure 4.3: Class Coloring with dierent values of the parameter minimum visible class. (A)the parameter is set to 100%. (B)the parameter is set to 50%. (C)the parameter is set to </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>62</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="63">64</head><p>Figure 4.6: Radio station data set. (A) Pie chart visualization. (B) Class coloring of the</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>65</head><p>Figure 4.7: Class coloring of the radio search data set. The parameter minimum visible class is set to dierent values (A) 0% (B) 30% (C) 60% (D)100%. The dominant classes in (C) do</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>66</head><p>Figure 4.8: Class coloring of the radio search data set after the correction of the algo- rithm. The parameter minimum visible class is set to dierent values (A) 0% (B) 30% (C) 60%</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>67</head><p>Figure 4.9: Class coloring of the banksearch data set after the correction of the algorithm. Some of the drawbacks that was found in the least section have disappeared after the correction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>68</head><p>Figure 4.10: Chessboard visualization of the radio search data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Chapter 5 Conclusions</head><p>Self-Organizing Map is a strong and useful tool for analyzing large and/or complex data sets especially those of high dimensionality. If the SOM is designed to be a tool for human use, then a SOM visualization is useful or sometimes necessary.</p><p>SOM Visualizations are divided into two main categories: Visualization methods for unlabeled data, which are either visualizations that show the map in relation to the data set such as hit histograms or visualizations that are derived from the model vectors which aim at showing cluster structure and boundaries such as U-</p><p>Matrix. The second category includes visualizations that assume the availability of labeled data and uses these (class) labels to produce a visualization, which shows the class topology of the data such as the pie chart visualization.</p><p>In this work a novel visualization method for visualization of labeled data is proposed namely the SOM class coloring method. This method aims to produce a colored partitioning of the SOM lattice depending on the class distribution in the units. The coloring process has two main steps: The rst step is partitioning the SOM lattice by nding the Voronoi diagram having the unit positions as sites. The second step is partitioning each Voronoi cell separately depending on the classes in the corresponding unit and the classes in the neighboring cells. To achieve the second step, attractor functions are used: The attractor function is an algorithm, that attracts the colored pixels to a line segment. For partitioning a Voronoi cell with the attractor function, line segments with suitable lengths, directions, start and end points are selected, taking into account the classes in the unit and the classes in the neighboring units; nally the attractor function is</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CHAPTER 5. CONCLUSIONS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>70</head><p>applied to these line segments and the corresponding classes.</p><p>An alternative method for coloring the cells is the chess board coloring, in which the Voronoi cell is divided into squires, which are colored according a uniform distribution function taking into account the contribution fractions for each class.</p><p>The class coloring method was tested with three dierent data sets, namely the iris data set, banksearch data set and an audio data set consisting of the broadcast of eight radio stations. The most of test showed satisfying results.</p><p>Some drawbacks were found such as the problem of ambiguousness; that is more than one coloring is possible. The problem occurs if the class distribution is very Geometry algorithms.  <ref type="bibr">[Ult03a]</ref> Alfred Ultsch. Maps for the visualization of high dimensional data spaces. In Proceedings Workshop on Self organizing Maps(WSOM'03),</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 . 4 :</head><label>34</label><figDesc>Figure 3.4: Unit substitution.(A) Multi class units represented by pie charts,(B)Substitution points that can be used by color ooding.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>in subsection 3 . 4 . 1</head><label>341</label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>dened:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>See Figure 3 . 7 .</head><label>37</label><figDesc>More formally:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>35 Figure 3 . 8 :</head><label>3538</label><figDesc>Figure 3.8: Sweepline algorithm. The slides 1-9 animate the algorithm while nding the Voronoi diagram of the three sites in slide 1. In slide 2 a parabola for the upper site arises because the sweepline passes this site. In the next two slides the parabola grows while the sweepline moves. In slide 5 two parabolas for the two lower points arise because the sweepline passes these points. In the rest slides all parabolas grow while the line sweeping. The intersections of the parabolas form the edges of the Voronoi diagram as in slide 9 (red lines).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>40 Figure 3 . 12 :</head><label>40312</label><figDesc>Figure 3.12: (A)Notations used int this section(Note that we use the colors as class names for simplifying). (B)Connection lines between regions that have same classes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>The areas marked with circles are examples of</head><label></label><figDesc></figDesc><table>LIST OF FIGURES 

ix 

4.2 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>The previous training algorithm, which was described formally is now sum- marized in the following steps:</head><label>The previous</label><figDesc></figDesc><table>CHAPTER 2. RELATED WORK 

8 

2. Chose a sample vector randomly from the set of training data. 

3. Every node is examined to nd the best-matching unit. 

4. The radius of the neighborhood of the BMU is now calculated. This is 

a value that starts large, typically set to the 'radius' of the lattice, but 

diminishes each time-step. Any nodes found within this radius are deemed 

to be inside the BMU's neighborhood. 

5. Each of neighboring node's weights are adjusted to make them more like 

the input vector. The closer a node is to the BMU, the more its weights 

get altered. 

6. Iterate from step 2 for N iterations. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>features are in form that is not suitable for clustering, one or more transformation could be used to put the features in an appropriate form to</head><label></label><figDesc></figDesc><table>There are 

many clustering methods, however it is useful at this point to distinguish between 

supervised and unsupervised methods: in the case of supervised classication 

(discriminant analysis) a collection of already classied patterns is provided and 

the problem is to classify a new pattern. In the case of unsupervised classication 

(clustering) a collection of unclassied pattern is provided and the problem is to 

group this collection into meaningful clusters, but this process of labeling is data 

driven and not as according to a given class structure as in the case of supervised 

classication. The clustering process typically consist of the following steps: 

Pattern representation, pattern measure denition, grouping, data abstraction. 

1. Pattern representation: this step involves preparing the input data and 

nding important information such as the number of available patterns 

and the number, type, and scale of the features involved in the clustering. 

Furthermore a subset of features may be selected to be the most important CHAPTER 2. RELATED WORK 

9 

If the start clustering. 

2. Pattern proximity measure denition: this is the distance function, that 

is applied on pairs of patterns to measure the similarity between the two 

patterns. There is a variety of distance measure functions. The simplest 

one may be the Euclidean distance. 

3. Grouping: this is the process of partitioning of the date into groups. The 

partitions can be hard, where a pattern can belong to only one partition 

and it can be fuzzy, where a pattern can have membership degrees in many 

clusters. Furthermore there are two conceptual clustering algorithms: hier-

archical algorithms, that produce many nested partitions based on similar-

ity according to a criterion for merging or splitting clusters and partitive 

clustering algorithms, that identify the partition that optimizes a clustering 

criterion. Clustering algorithms will be discussed in the next sub sections 

in more details. 

4. Data abstraction: is the process of extracting a simple and compact repre-

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10" validated="false"><head>-level clustering approach / Clustering of the SOM</head><label></label><figDesc></figDesc><table>There are two main problems in the standard clustering algorithms mentioned 

in the previous sub sections above: the rst problem is the high computational 

costs when applying the algorithm directly on the original data, especially if a 

large collection of patterns underlies. The second problem is the sensibility to 

noise: applying the algorithm to original data makes it sensible to the occurrence 

of noise and outlier because this could make clusters overlap or have inaccurate 

boundaries. Both of these problems can be solved by using the two-level clus-

tering approach: The clustering algorithm is not applied directly on the original 

data but rather on cluster prototypes, that are generated by a previously trained 

SOM. SOM nodes are considered as cluster prototypes and are grouped into 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17" validated="false"><head>; for example</head><label></label><figDesc></figDesc><table>if we assume 5 classes and f (u) = {0.0, 0.25, 0.5, 0.25, 0.0} that means that the 

following data classes and their contribution fractions are assigned to the unit u: 
25% from class c2, 50% from class c3, 25% from class c3 and 0.0 (or no data) of 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18" validated="false"><head></head><label></label><figDesc>To solve this problem we suggest the following concept: a unit with more than one class is substituted by a number of points equal to the number of classes, and these points are located closely to the original unit; then they are rotated round the original point so that they are arranged according to the classes at the neighboring units, that is we thereby aim to have neighboring classes as similar as possible as it is in Figure 3.4 illustrated. When all units are substituted, we</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19" validated="false"><head></head><label></label><figDesc>The random positioning could produce a dierent result for every run. We analyzed the issue and found that it is caused by a starvation eect which occurs as the following: in one case the ow of an isolated color is stopped by the ow of the surrounding ones; in the other case if a slice change is done, the ow of the isolated color nds a way outwards and grows dramatically more eventually in a dieren region. See Figure 3.5. This eect can also happen without making changes in the positions, but only by changing the order in which the pixels are occupied(for example by randomly selection). The drawbacks seemed for us to be in the nature of the concept and thus we stopped to search 32 Figure 3.6: Areas in Voronoi diagram (A) are similar to these produced by color ooding (B).</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_24" validated="false"><head></head><label></label><figDesc>Figure 3.9: the rst step is discovering all triangles that are aected by the addition, which are the triangles</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_33" validated="false"><head>Figure 3 .15 -D</head><label>Figure 3</label><figDesc></figDesc><table>Border smoothing by weighting the line segments </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_34" validated="false"><head>In particular , this occurs if a class has dierent contributions in the two neighboring</head><label>In particular</label><figDesc></figDesc><table>regions or if the regions are of dierent areas, so that the the border lines at the 

common edge have a zigzag form. See Figure 3.16 -A. To solve this problem 

we use a weighted comparator for the sorting process needed in the attractor 

algorithm described in 3.4.4. Recall that the pixels are sorted according their 

distances to a line segment and then they are assigned to the class in this order. 

The weighted comparator uses a weighted distance metric for nding the the 

distance to the line segment. That is, the line segment is assigned two weights 

for the two ends and the weights are taken in account in measuring the distance: 

If p is the point to be measured and p 1 p 2 is the line segment then the weighted 

distance is: 

d w (p, p 1 , p 2 ) = d(p, p 1 , p 2 ) + |pp 1 |(1/w 1 ) 
2 + |pp 2 |(1/w 2 ) 

2 
(3.1) 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_38" validated="false"><head>Table 4 .1: Categories and their associated themes in the banksearch dataset.</head><label>4</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_43" validated="false"><head>we can not assume that the trained SOM holds and reects the data exactly, but only the general topology and details up to a certain grad are hold; this means that the coloring algorithm deals with a</head><label></label><figDesc></figDesc><table>Figure 4.6 -F shows areas with classes, that are not very interleaved. These areas 

have a clear class structure, because the dominant classes and coloring order is 

algorithmically clear dened. In fact the ambiguousness in the coloring of heavy 

interleaved classes can be ignored: </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_46" validated="false"><head>Figure 4 .4: Class coloring with Voronoi cell border. (A) to (F) are signicant samples showingCHAPTER 4. EXPEREMENTS AND EVALUATION</head><label>Figure 4</label><figDesc></figDesc><table></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Data clustering: a review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">N Murty</forename><surname>Jain</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
			<publisher>ACM Computing Surveys</publisher>
			<biblScope unit="volume">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Franz Aurenhammer and Otfried Schwarzkopf. A simple on-line randomized incremental algorithm for computing higher order voronoi diagrams</title>
	</analytic>
	<monogr>
		<title level="m">Annual Symposium on Computational Geometry, Proceedings of the seventh annual symposium on Computational geometry, pages 363381</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Michael Aupetit. High-dimensional labeled data analysis with gabriel graphs</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings Intl. European Symp. on Articial Neural Networks (ESANN&apos;03)</title>
		<meeting>Intl. European Symp. on Articial Neural Networks (ESANN&apos;03)<address><addrLine>Bruges, Belgium</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
	<note>Dside publications</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Steven Fortune. A sweepline algorithm for voronoi diagrams</title>
	</analytic>
	<monogr>
		<title level="m">SCG 86: Proceedings of the second annual symposium on Computational geometry, pages 313322</title>
		<meeting><address><addrLine>New York, NY, USA; New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="1986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Voronoi diagrams and delaunay triangulations. pages 377388</title>
	</analytic>
	<monogr>
		<title level="m">Steven Fortune</title>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<title level="m">Samuel Kaski. Data exploration using self-organizing maps. Acta Polytechnica Scandinavica, Mathematics, Computing and Management in Engineering Series No. 82</title>
		<imprint>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Exploration of very large databases by self-organizing maps</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICNN&apos;97, International Conference on Neural Networks, pages PL1PL6</title>
		<meeting>ICNN&apos;97, International Conference on Neural Networks, pages PL1PL6<address><addrLine>IEEE Service Center, Piscataway, NJ</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
	<note>Teuvo Kohonen</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Visually proling radio stations</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th International Conference on Music Information Retrieval (ISMIR</title>
		<meeting>the 7th International Conference on Music Information Retrieval (ISMIR</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="10" to="2006" />
		</imprint>
	</monogr>
	<note>Thomas Lidy and Andreas Rauber</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Michael Dittenbach, and Andreas Rauber. Advanced visualization of self-organizingmaps with vector elds</title>
	</analytic>
	<monogr>
		<title level="m">Georg Pölzlbauer</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Using smoothed data histograms for cluster visualization in self-organizing maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pampalk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rauber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Merkl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Artical Neural Networks (ICANN&apos;02</title>
		<meeting>the International Conference on Artical Neural Networks (ICANN&apos;02<address><addrLine>Madrid, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
	<note>Springer Lecture Notes in Computer Science</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">The som-enhanced jukebox organization and visualization of music collections based on perceptual models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rauber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pampalk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Merkl</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
	<note>New Music Research</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">John Sammon. A nonlinear mapping for data structure analysis</title>
	</analytic>
	<monogr>
		<title level="m">IEEE Trans. Computer, volume c-18, pages 401409</title>
		<imprint>
			<date type="published" when="1969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Mark Sinka and David Corne. A large benchmark dataset for web document clustering</title>
	</analytic>
	<monogr>
		<title level="m">Soft Computing Systems: Design, Management and Applications, Volume 87 of Frontiers in Articial Intelligence and Applications, pages 881890</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<title level="m">Softsurfer.com. BIBLIOGRAPHY 73</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Pareto density estimation: Probability density estimation for knowledge discovery</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings Conf. Soc. for Information and Classication</title>
		<meeting>Conf. Soc. for Information and Classication<address><addrLine>Cottbus, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Alfred Ultsch</publisher>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<title level="m">Alfred Ultsch. Esom-maps: tools for clustering, visualization, and classication with emergent som. In Technical Report 46, Dept. of Mathematics and Computer Science, D-35032 Marburg</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
<note type="report_type">Germany</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Juha Vesanto and E. Alhoniemi. Clustering of the self-organizing map</title>
	</analytic>
	<monogr>
		<title level="m">586</title>
		<imprint>
			<publisher>IEEE-NN</publisher>
			<date type="published" when="2000" />
			<biblScope unit="volume">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<title level="m">Juha Vesanto. SOM-based data visualization methods. IntelligentData-Analysis, 3:11126</title>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Nonlinear multidimensional data projection and visualisation</title>
	</analytic>
	<monogr>
		<title level="m">Lecture Notes in Computer Science 2690</title>
		<meeting><address><addrLine>Manchester, M60 1QD, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
	<note>Hujun Yin</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
