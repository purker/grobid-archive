"path";"PDF";"original";"extracted";"original value";"extracted value";"Precision";"Recall";"F1"
"TUW-137078";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\groundtruth\TUW-137078.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\result\result-TUW-137078-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\extracted\grobid\grobid-TUW-137078-xstream.xml"")";"The aim of the ECIC project is the dissemination of participative methods of organizational development originating from Scandinavian countries to other European countries. To support this process we developed a hypertextual learning system. Because of the heterogeneous target group and the ill-structured domains of ECIC detailed usability testing is necessary. First tentative results indicate that graphical overview maps play an important role and interactive examples are very motivating.";"The aim of the ECIC project is the dissemination of participative methods of organizational development originating from Scandinavian countries to other European countries. To support this process we developed a hypertextual learning system. Because of the heterogeneous target group and the ill-structured domains of ECIC detailed usability testing is necessary. First tentative results indicate that graphical overview maps play an important role and interactive examples are very motivating.";"100,00";"100,00";"100,00"
"TUW-138011";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\groundtruth\TUW-138011.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\result\result-TUW-138011-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\extracted\grobid\grobid-TUW-138011-xstream.xml"")";"The role of medical guidelines is becoming more and more important in the medical field. Within the Protocure project it has been shown that the quality of medical guidelines can be improved by formalisation. However formal-isation turns out to be a very time-consuming task, resulting in a formal guideline that is much more complex than the original version and the relation with this original guideline is often unclear. This paper presents a case study where the relation between the informal medical guideline and its formal counterpart is investigated. This has been used to determine the gaps between the formal and informal guidelines and the cause of the size explosion of the formal guidelines.";"The role of medical guidelines is becoming more and more important in the medical field. Within the Protocure project it has been shown that the quality of medical guidelines can be improved by formalisation. However formal-isation turns out to be a very time-consuming task, resulting in a formal guideline that is much more complex than the original version and the relation with this original guideline is often unclear. This paper presents a case study where the relation between the informal medical guideline and its formal counterpart is investigated. This has been used to determine the gaps between the formal and informal guidelines and the cause of the size explosion of the formal guidelines.";"100,00";"100,00";"100,00"
"TUW-138447";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\groundtruth\TUW-138447.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\result\result-TUW-138447-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\extracted\grobid\grobid-TUW-138447-xstream.xml"")";"A key problem in multimedia systems is the faithful reproduction of color. One of the main reasons why this is a complicated issue are the different color reproduction technologies used by the various devices; displays use easily modeled additive color mixing, while printers use a subtractive process, the characterization of which is much more complex than that of self-luminous displays. In order to resolve these problems several processing steps are necessary, one of which is accurate device characterization. Our study examines different learning algorithms for one particular neural network technique which already has been found to be useful in related contexts – namely radial basis function network models – and proposes a modified learning algorithm which improves the colorimetric characterization process of printers. In particular our results show that is possible to obtain good performance by using a learning algorithm that is trained on only small sets of color samples, and use it to generate a larger look–up table (LUT) through use of multiple polynomial regression or an interpolation algorithm. We deem our findings to be a good start point for further studies on learning algorithms used in conjunction with this problem.";"A key problem in multimedia systems is the faithful reproduction of color. One of the main reasons why this is a complicated issue are the different color reproduction technologies used by the various devices; displays use easily modeled additive color mixing, while printers use a subtractive process, the characterization of which is much more complex than that of self-luminous displays. In order to resolve these problems several processing steps are necessary, one of which is accurate device characterization. Our study examines different learning algorithms for one particular neural network technique which already has been found to be useful in related contexts-namely radial basis function network models-and proposes a modified learning algorithm which improves the colorimetric characterization process of printers. In particular our results show that is possible to obtain good performance by using a learning algorithm that is trained on only small sets of color samples, and use it to generate a larger look-up table (LUT) through use of multiple polynomial regression or an interpolation algorithm. We deem our findings to be a good start point for further studies on learning algorithms used in conjunction with this problem.";"100,00";"100,00";"100,00"
"TUW-138544";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\groundtruth\TUW-138544.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\result\result-TUW-138544-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\extracted\grobid\grobid-TUW-138544-xstream.xml"")";"If visual information retrieval should make further progress, it will be necessary to identify new ways to derive visual properties from higher levels of understanding than the pixel level (e.g. from low-level features). The paper outlines the implementation of modelling of feature hierarchies in the visual information retrieval framework VizIR (free under GPL). The approach allows for the derivation of high-level features from low-level features by aggregation and localisa-tion as well as semantic enrichment with additional knowledge. The technical implementation is based on the MPEG-7 structures for aggregation and specialisation.";"If visual information retrieval should make further progress, it will be necessary to identify new ways to derive visual properties from higher levels of understanding than the pixel level (e.g. from low-level features). The paper outlines the implementation of modelling of feature hierarchies in the visual information retrieval framework VizIR (free under GPL). The approach allows for the derivation of high-level features from low-level features by aggregation and localisa-tion as well as semantic enrichment with additional knowledge. The technical implementation is based on the MPEG-7 structures for aggregation and specialisation.";"100,00";"100,00";"100,00"
"TUW-138547";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\groundtruth\TUW-138547.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\result\result-TUW-138547-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\extracted\grobid\grobid-TUW-138547-xstream.xml"")";"This paper describes solutions for parallel video processing based on LAN-connected PC-like workstations. We outline application scenarios for the processing of video with broadcast TV resolution and promising areas for future research. Additionally, we describe a prototype system implemented in our workgroup. This Network-of-Workstations is based on Gigabit Ethernet, free Java software and standard network protocols. Video data is streamed to and from processing hosts using IP multicast and the Real-time Transfer Protocol. Control mechanisms are based on network file sharing. In comparison to related approaches, our system makes use of high-performance network hardware and open source software. In consequence, our system is easier to implement, cheaper and more flexible than, for example, highly integrated commercial solutions.";"This paper describes solutions for parallel video processing based on LAN-connected PC-like workstations. We outline application scenarios for the processing of video with broadcast TV resolution and promising areas for future research. Additionally, we describe a prototype system implemented in our workgroup. This Network-of-Workstations is based on Gigabit Ethernet, free Java software and standard network protocols. Video data is streamed to and from processing hosts using IP multicast and the Real-time Transfer Protocol. Control mechanisms are based on network file sharing. In comparison to related approaches, our system makes use of high-performance network hardware and open source software. In consequence, our system is easier to implement, cheaper and more flexible than, for example, highly integrated commercial solutions.";"100,00";"100,00";"100,00"
"TUW-139299";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\groundtruth\TUW-139299.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\result\result-TUW-139299-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\extracted\grobid\grobid-TUW-139299-xstream.xml"")";"This thesis presents methods to support protocol-based care in medicine. Time-oriented treatment plans and patient data are represented visually providing various interaction possibilities to aid execution and analysis of medical therapy plans formulated in the representation language Asbru. We introduce a two-view approach consisting of a Logical View and a Temporal View. The Logical View depicts therapy plans using a flow-chart like representation based on ""clinical algorithm maps"". The Temporal View on the other hand depicts plans as well as patient data in form of parameters and variables over time. The plan visualization method within the Temporal View is based on the idea of LifeLines. For being able to depict hierarchical structures and temporal uncertainties, we extended this concept and a novel glyph called PlanningLine has been developed. The development is embedded into a 3-step evaluation process including a user study with eight domain experts (physicians) at the beginning to acquire users' needs, a design evaluation, and an evaluation of our software prototype at the end of the thesis project.";"This thesis presents methods to support protocol-based care in medicine. Time-oriented treatment plans and patient data are represented visually providing various interaction possibilities to aid execution and analysis of medical therapy plans formulated in the representation language Asbru. We introduce a two-view approach consisting of a Logical View and a Temporal View. The Logical View depicts therapy plans using a flow-chart like representation based on ""clinical algorithm maps"". The Temporal View on the other hand depicts plans as well as patient data in form of parameters and variables over time. The plan visualization method within the Temporal View is based on the idea of LifeLines. For being able to depict hierarchical structures and temporal uncertainties, we extended this concept and a novel glyph called PlanningLine has been developed. The development is embedded into a 3-step evaluation process including a user study with eight domain experts (physicians) at the beginning to acquire users' needs, a design evaluation, and an evaluation of our software prototype at the end of the thesis project.";"100,00";"100,00";"100,00"
"TUW-139761";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\groundtruth\TUW-139761.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\result\result-TUW-139761-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\extracted\grobid\grobid-TUW-139761-xstream.xml"")";"This master thesis describes how to price options by means of Genetic Programming. The underlying model is the Generalized Autoregressive Conditional Heteroskedastic (GARCH) asset return process. The goal of this master thesis is to nd a closed-form solution for the price of European call options where the underlying securities follow a GARCH process. The data are simulated over a wide range to cover a lot of existing options in one single equation.
Genetic Programming is used to generate the pricing function from the data. Genetic Programming is a method of producing programs just by defining a problemdependent fitness function. The resulting equation is found via a heuristic algorithm inspired by natural evolution. Three different methods of bloat control are used. Additionally Automatic Defined Functions (ADFs) and a hybrid approach are tested, too. To ensure that a good configuration setting is used, preliminary testing of many different settings has been done, suggesting that simpler configurations are more successful in this environment.
The resulting equation can be used to calculate the price of an option in the given range with minimal errors. This equation is well behaved and can be used in standard spread sheet programs. It offers a wider range of utilization or a higher accuracy, respectively than other existing approaches.";"This master thesis describes how to price options by means of Genetic Programming. The underlying model is the Generalized Autoregressive Conditional Heteroskedastic (GARCH) asset return process. The goal of this master thesis is to find a closed-form solution for the price of European call options where the underlying securities follow a GARCH process. The data are simulated over a wide range to cover a lot of existing options in one single equation. Genetic Programming is used to generate the pricing function from the data. Genetic Programming is a method of producing programs just by defining a problem-dependent fitness function. The resulting equation is found via a heuristic algorithm inspired by natural evolution. Three different methods of bloat control are used. Additionally Automatic Defined Functions (ADFs) and a hybrid approach are tested, too. To ensure that a good configuration setting is used, preliminary testing of many different settings has been done, suggesting that simpler configurations are more successful in this environment. The resulting equation can be used to calculate the price of an option in the given range with minimal errors. This equation is well behaved and can be used in standard spread sheet programs. It offers a wider range of utilization or a higher accuracy, respectively than other existing approaches. Zusammenfassung Diese Diplomarbeit beschreibt, wie Optionen mit Hilfe Genetischer Programmierung bewertet werden können. Das zugrunde liegende Modell nennt sich GARCH (Gen-eralized Autoregressive Conditional Heteroskedastic) Renditeprozess. Das Ziel dieser Diplomarbeit ist eine geschlossene Formel, die als Ergebnis den Preis einer europäischen Kaufoption liefert, dessen dahinter liegende Wertpapier einem GARCH Prozess folgt. Die Daten werden innerhalb eines breiten Wertebereiches simuliert, um die meisten existierenden Optionen mit einer Formel bewerten zu können. Die Formel wird mittels Genetischer Programmierung aus den Daten generiert. Genetische Programmierung ist eine Methode, bei der nur durch Definition einer zum Problem passenden Bewertungsfunktion vollständige Programme produziert werden können. Die Ergebnisgleichung wird schließlich mittels eines der EvolutionähnlichenEvolutionähnlichen Algorithmus gefunden. Drei verschiedene Methoden zum Bloat Control wurden ver-wendet. Zusätzlich wurden auch Automatisch Definierte Funktionen sowie ein hybrider Ansatz untersucht. Um sicherzustellen, dass eine gute Konfiguration gewählt wird, gibt es Vortests vieler verschiedener Konfigurationen. Es zeigt sich, dass in diesem Umfeld einfachere Konfigurationen erfolgreicher sind. Die Ergebnisgleichung kann schließlich zur Errechnung der Optionspreise mit min-imalem Fehler verwendet werden. Diese Gleichung verhält sich gut und kann auch in Standardtabellenkalkulationen verwendet werden. Im Vergleich mit anderen existieren-den Ansätzen, bietet diese Gleichung eine weitere Verwendbarkeit beziehungsweise eine höhere Genauigkeit. Acknowledgements I have to thank my family, my professors and all my friends. Special thanks to Dr. Hanke, who has helped me to find this interesting topic of research and to Dr. Raidl, who has showed me how to write a good master thesis. All the brave programmers who have made libraries I have used, are mentioned here too. Magister Katarina Kocian has read my thesis very often to find even the last mistake. Without these people it would not have been possible to write this thesis.";"0,00";"0,00";"0,00"
"TUW-139769";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\groundtruth\TUW-139769.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\result\result-TUW-139769-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\extracted\grobid\grobid-TUW-139769-xstream.xml"")";"This thesis deals with local branching, a local search algorithm applied on top of a Branch and Cut algorithm for mixed integer programming problems. Local branching defines custom sized neighborhoods around given feasible solutions and solves them partially or completely before exploring the rest of the search space. Its goal is to improve the heuristic behavior of a given exact integer programming solver, i.e. to focus on finding good solutions early in the computation. Local branching is implemented as an extension to the open source Branch and Cut solver COIN/BCP. The framework's main goal is to provide a generic implementation of local branching for integer programming problems. IP problems are optimization problems where some or all variables are integer values and must satisfy one or more (linear) constraints. Several extensions to the standard local branching algorithm were added to the framework. Pseudo-concurrent exploration of multiple local trees, aborting local trees and a variable fixing heuristic allow the user to implement sophisticated search metaheuristics that adjust the local branching parameters adaptively during the computation. A major design goal was to provide a clean encapsulation of the local branching algorithm to facilitate embedding of the framework in other, higher-level search algorithms, for example in evolutionary algorithms. As an example application, a solver for the multidimensional knapsack problem is implemented. A custom local branching metaheuristic imposes node limits on local subtrees and adaptively tightens the search space by fixing variables and reducing the size of the neighborhood. Test results show that local branching can offer significant advantages to standard Branch and Cut algorithms and eventually proves optimality in shorter time. Especially for large, complex test instances exploring the local neighborhood of a good feasible solution often yields better short-term results than the unguided standard Branch and Cut algorithm. Improving the solutions found early in the computation also helps to remove additional parts of the search tree, potentially leading to better solutions in longer runs. ";"This thesis deals with local branching, a local search algorithm applied on top of a Branch and Cut algorithm for mixed integer programming problems. Local branching defines custom sized neighborhoods around given feasible solutions and solves them partially or completely before exploring the rest of the search space. Its goal is to improve the heuristic behavior of a given exact integer programming solver, i.e. to focus on finding good solutions early in the computation. Local branching is implemented as an extension to the open source Branch and Cut solver COIN/BCP. The framework's main goal is to provide a generic implementation of local branching for integer programming problems. IP problems are optimization problems where some or all variables are integer values and must satisfy one or more (linear) constraints. Several extensions to the standard local branching algorithm were added to the framework. Pseudo-concurrent exploration of multiple local trees, aborting local trees and a variable fixing heuristic allow the user to implement sophisticated search metaheuristics that adjust the local branching parameters adaptively during the computation. A major design goal was to provide a clean encapsulation of the local branching algorithm to facilitate embedding of the framework in other, higher-level search algorithms, for example in evolutionary algorithms. As an example application, a solver for the multidimensional knapsack problem is implemented. A custom local branching metaheuristic imposes node limits on local subtrees and adaptively tightens the search space by fixing variables and reducing the size of the neighborhood. Test results show that local branching can offer significant advantages to standard Branch and Cut algorithms and eventually proves optimality in shorter time. Especially for large, complex test instances exploring the local neighborhood of a good feasible solution often yields better short-term results than the unguided standard Branch and Cut algorithm. Improving the solutions found early in the computation also helps to remove additional parts of the search tree, potentially leading to better solutions in longer runs. Zusammenfassung Diese Diplomarbeit beschäftigt sich mit Local Branching, einem lokalen Suchalgorithmus, der auf einem Branch and Cut Algorithmus für ganzzahlige Optimierungsprobleme aufsetzt. Local Branching definiert beliebig große Nachbarschaften um gegebene gültige Lösungen und löst diese teilweise oder komplett, bevor der Rest des Lösungsraums durchsucht wird. Das Ziel ist eine Verbesserung des heuristischen Verhaltens des gegebenen Solvers für ganzzahlige Optimierungsprobleme, d.h. sich auf das möglichst frühe Finden guter Lösungen zu konzentrieren. Local Branching ist als Erweiterung des Open Source Branch and Cut Solvers COIN/BCP implementiert. Das Hauptziel des Frameworks ist eine generische Implementierung von Local Branching für ganzzahlige Optimierungsprobleme, also Probleme, bei denen alle oder einige Variablen ganzzahlig sein müssen, und zusätzlich eine oder mehrere (lineare) Bedingungen in Form von Ungleichungen erfüllen müssen. Es wurden mehrere Erweiterungen zum Framework hinzugefügt: die pseudo-parallele Abarbeitung mehrerer lokaler Suchbäume, das vorzeitige Terminieren lokaler Suchbäume sowie eine unabhängige Variablen-Fixing-Heuristik. Durch diese Erweiterungen können die Parameter für Local Branching im Laufe der Berechnung beliebig verändert werden. Ein wesentliches Ziel beim Entwurf des Frameworks war eine klare Kapselung des Local Branching Algorithmus, um die Einbettung in andere, höhere Suchalgorithmen zu ermöglichen, etwa in evolutionäre Algorithmen. Als Beispielapplikation wurde ein Solver für das mehrdimensionale Rucksackproblem implementiert. Eine eigene Local Branching Metaheuristik beschränkt die Größe lokaler Bäume durch Knotenlimits und kann den Suchraum durch Anwendung der Variablen-Fixing-Heuristik weiter einschränken. Die Testergebnisse zeigen signifikante Vorteile für Local Branching im Vergleich zum normalen Branch and Cut Algorithmus. Vor allem für große, komplexe Testinstanzen liefert die Suche in lokalen Bäumen oft bessere Resultate am Anfang der Berechnung. Dadurch wird auch die Zeit zum Finden (und Beweisen) der optimalen Lösung potentiell verringert, da dadurch früher zusätzliche Teile des Suchbaums weggeschnitten werden können.";"0,00";"0,00";"0,00"
"TUW-139781";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\groundtruth\TUW-139781.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\result\result-TUW-139781-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\extracted\grobid\grobid-TUW-139781-xstream.xml"")";;;"none extracted value";"none expected";"NA"
"TUW-139785";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\groundtruth\TUW-139785.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\result\result-TUW-139785-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\extracted\grobid\grobid-TUW-139785-xstream.xml"")";"In this master thesis a generic libray of efficient metaheuristics for combinatorial optimization is presented. In the version at hand classes that feature local search, simulated annealing, tabu search, guided local search and greedy randomized adaptive search procedure were implemeted. Most notably a generic implementation features the advantage that the problem dependent classes and methods only need to be realized once without targeting a specific algorithm because these parts of the sourcecode are shared among all present algorithms contained in EAlib. This main advantage is then exemplarily demonstrated with the quadratic assignment problem. The sourcecode of the QAP example can also be used as an commented reference for future problems. Concluding the experimental results of the individual metaheuristics reached with the presented implementation are presented.";"In this master thesis a generic libray of efficient metaheuristics for combinatorial optimization is presented. In the version at hand classes that feature local search, simulated annealing, tabu search, guided local search and greedy randomized adap-tive search procedure were implemeted. Most notably a generic implementation features the advantage that the problem dependent classes and methods only need to be realized once without targeting a specific algorithm because these parts of the sourcecode are shared among all present algorithms contained in EAlib. This main advantage is then exemplarily demonstrated with the quadratic assignment problem. The sourcecode of the QAP example can also be used as an commented reference for future problems. Concluding the experimental results of the individual metaheuristics reached with the presented implementation are presented. Kurzfassung In dieser Diplomarbeit wird eine generische Bibliothek von effizienten Metaheuris-tiken für kombinatorische Optimierungsprobleme vorgestellt. In der vorliegenden Eine generische Implementierung bietet vorallem den Vorteil das bei einem neuen zu lösendem Problem nur einige bestimmte problemabhängige Klassen und Metho-den realisiert werden müssen ohne sich schon im Vorhinein einen speziellen Algorith-mus festzulegen, da diese Klassen und Methoden von allen in der EAlib vorhanden Metaheuristiken verwendet werden. Die Vorteile dieser Bibliothek werden anschließend anhand des Quadratic Assignment Problems ausführlich dargestellt. Dieses Beispiel dient zusätzlich auch noch als kommentierte Referenz für zukünftige Problemimplentierungen. Abschließend werden die Resulate der Experimente mit den verschiedenen Meta-heuristiken präsentiert. 1 Danksagung An dieser stelle möchte ich mich bei allen Menschen bedanken die zum Gelingen dieser Diplomarbeit beigetragen haben. Dieser Dank gilt meinem Betreuer Prof. Raidl, der mich mit großer Geduld am Weg zum Abschluß begleitet hat und mit mir in den vielen Treffen oft nützliche Ideen entwickelt hat. Meinen Eltern und meinem Bruder Ronald danke ich für ein sorgloses Studium und die moralische Unterstützung wenn die Motivation einmal nicht so groß war. Bei meinen Studienkollegen, besonders bei Harry und Zamb, bedanke ich mich für die Freundschaft, den Spaß und die gegenseitige Unterstützung. Last but not least möchte ich mich auch bei meinen Mitbewohnern Sic0 und Leo bedanken, die mir während meiner Arbeit die nötige Ruhe zukommen ließen, aber natürlich auch ab und zu für willkommene Ablenkung gesorgt haben. Natascha danke ich für die schöne gemeinsame Zeit. 2";"0,00";"0,00";"0,00"
"TUW-140047";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\groundtruth\TUW-140047.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\result\result-TUW-140047-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\extracted\grobid\grobid-TUW-140047-xstream.xml"")";"This paper describes our submission to the MIREX 2006 Audio Music Similarity and Retrieval task. The task was to submit an audio feature extraction algorithm, to compute music similarity measures and to return a distance matrix from an audio collection consisting of 5000 pieces, which was subsequently evaluated through human listening tests as well as objective statistics. We submitted a new implementation of the Statistical Spectrum Descriptor (SSD) audio feature extractor and computed the distance matrix directly from feature space. Results from the human evaluation show that our approach is among the top 5 algorithms which furthermore show no statistically significant performance differences. The evaluation of a number of objective statistics ranked our algorithm 3rd in most of the cases. Our submission was one of the two fastest in terms of total runtime, having the shortest distance computation time. The approach has also been evaluated on Audio Cover Song Identification, where it was the bestperforming ""Audio Music Similarity and Retrieval"" submission, outperformed, however, by 4 submissions which were specifically designed for the cover identification task.";"This paper describes our submission to the MIREX 2006 Audio Music Similarity and Retrieval task. The task was to submit an audio feature extraction algorithm, to compute music similarity measures and to return a distance matrix from an audio collection consisting of 5000 pieces, which was subsequently evaluated through human listening tests as well as objective statistics. We submitted a new implementation of the Statistical Spectrum Descriptor (SSD) audio feature extractor and computed the distance matrix directly from feature space. Results from the human evaluation show that our approach is among the top 5 algorithms which furthermore show no statistically significant performance differences. The evaluation of a number of objective statistics ranked our algorithm 3rd in most of the cases. Our submission was one of the two fastest in terms of total run-time, having the shortest distance computation time. The approach has also been evaluated on Audio Cover Song Identification, where it was the best-performing ""Au-dio Music Similarity and Retrieval"" submission, outper-formed, however, by 4 submissions which were specifically designed for the cover identification task.";"100,00";"100,00";"100,00"
"TUW-140048";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\groundtruth\TUW-140048.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\result\result-TUW-140048-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\extracted\grobid\grobid-TUW-140048-xstream.xml"")";"UN/CEFACT's Modeling Methodology (UMM) is a well accepted and formal notation to analyze and design B2B business processes. In a service oriented architecture (SOA) environment process specification languages like the Business Process Specification Schema (BPSS) are used to configure B2B information systems. However , mappings from UMM models to BPSS process specifications currently exist just on a conceptual level. This results in a gap between defined B2B processes and BPSS configurable e-commerce systems. Thus, a model driven code generation of BPSS descriptions is required. In this paper we present a technical implementation of a transformation engine that generates BPSS process specifications from a UMM model represented in the XML Metadata Interchange (XMI) language. This implementation bridges the gap mentioned above. It has been used in the EU project GILDAnet to generate BPSS descriptions from logistic processes modeled in UMM.";"UN/CEFACT's Modeling Methodology (UMM) is a well accepted and formal notation to analyze and design B2B business processes. In a service oriented architecture (SOA) environment process specification languages like the Business Process Specification Schema (BPSS) are used to configure B2B information systems. However , mappings from UMM models to BPSS process specifications currently exist just on a conceptual level. This results in a gap between defined B2B processes and BPSS configurable e-commerce systems. Thus, a model driven code generation of BPSS descriptions is required. In this paper we present a technical implementation of a transformation engine that generates BPSS process specifications from a UMM model represented in the XML Metadata Interchange (XMI) language. This implementation bridges the gap mentioned above. It has been used in the EU project GILDAnet to generate BPSS descriptions from logistic processes modeled in UMM.";"100,00";"100,00";"100,00"
"TUW-140229";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\groundtruth\TUW-140229.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\result\result-TUW-140229-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\extracted\grobid\grobid-TUW-140229-xstream.xml"")";"Detecting the needs of learners is a challenging but essential task to be able to provide adaptivity. In this paper we present a tool that enables learning management systems (LMS) to detect learning styles based on the behavior of learners during an online course. By calculating the learning styles and filling the student model of LMS with such personal data, a basis for adaptivity is provided.";"Detecting the needs of learners is a challenging but essential task to be able to provide adaptivity. In this paper we present a tool that enables learning management systems (LMS) to detect learning styles based on the behavior of learners during an online course. By calculating the learning styles and filling the student model of LMS with such personal data, a basis for adaptivity is provided. MASPLANG [2] and CS383 [3]. But these systems use a questionnaire for detecting learning styles. Garcia et al. [4] investigated the use of Bayesian networks to detect learning styles based on the behavior of learners in a web-based educational system. While their work is focused on the use of Bayesian networks, our approach sums up indications of preferences based on patterns, equally to the approach of learning style questionnaires. Moreover, we propose a tool for LMS in general rather than for one specific system.";"0,00";"0,00";"0,00"
"TUW-140253";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\groundtruth\TUW-140253.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\result\result-TUW-140253-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\extracted\grobid\grobid-TUW-140253-xstream.xml"")";"Process types – a kind of behavioral types – specify constraints on message acceptance for the purpose of synchronization and to determine object usage and component behavior in object-oriented languages. So far process types have been regarded as a purely static concept for Actor languages incompatible with inherently dynamic programming techniques. We propose solutions of related problems causing the approach to become useable in more conventional dynamic and concurrent languagues. The proposed approach can ensure message acceptability and support local and static checking of race-free programs.";"Process types-a kind of behavioral types-specify constraints on message acceptance for the purpose of synchronization and to determine object usage and component behavior in object-oriented languages. So far process types have been regarded as a purely static concept for Actor languages incompatible with inherently dynamic programming techniques. We propose solutions of related problems causing the approach to become useable in more conventional dynamic and concurrent languagues. The proposed approach can ensure message acceptability and support local and static checking of race-free programs.";"100,00";"100,00";"100,00"
"TUW-140308";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\groundtruth\TUW-140308.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\result\result-TUW-140308-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\extracted\grobid\grobid-TUW-140308-xstream.xml"")";"We show that algorithmic thinking is a key ability in informatics that can be developed independently from learning programming. For this purpose we use problems that are not easy to solve but have an easily understandable problem definition. A proper visualization of these problems can help to understand the basic concepts connected with algorithms: correctness, termination , efficiency, determinism, parallelism, etc. The presented examples were used by the author in a pre-university course, they may also be used in secondary schools to help understanding some concepts of computer science.";"We show that algorithmic thinking is a key ability in informatics that can be developed independently from learning programming. For this purpose we use problems that are not easy to solve but have an easily understandable problem definition. A proper visualization of these problems can help to understand the basic concepts connected with algorithms: correctness, termination , efficiency, determinism, parallelism, etc. The presented examples were used by the author in a pre-university course, they may also be used in secondary schools to help understanding some concepts of computer science.";"100,00";"100,00";"100,00"
"TUW-140533";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\groundtruth\TUW-140533.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\result\result-TUW-140533-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\extracted\grobid\grobid-TUW-140533-xstream.xml"")";"Formal specification and verification of software have made small but continuous advances throughout its long history, and have reached a point where commercial tools became available for verifying programs semi-automatically or automatically. The aim of the master thesis is to evaluate commercial and academic verification tools with respect to their usability in developing software and in teaching formal methods. The thesis will explain the theoretical foundation and compare the capabilities and characteristics of selected commercial and academic tools on concrete examples. The theoretical foundations deal on the one hand with the general ideas and principles of formal software verification, on the other hand present some internals of the selected tools to give a comprehensive understanding. The discussed tools are the Frege Program Prover, KeY, Perfect Developer, and the Prototype Verification System. The examples encompass simple standard computer science problems. The evaluation of these tools concentrates on the whole development process of specification and verification, not just on the verification results.";;"none extracted value";"0,00";"0,00"
"TUW-140867";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\groundtruth\TUW-140867.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\result\result-TUW-140867-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\extracted\grobid\grobid-TUW-140867-xstream.xml"")";"We present the status of formal methods at our university, and describe our course on formal software verification in more detail. We report our experiences in using Perfect Developer for the course assignments.";"We present the status of formal methods at our university, and describe our course on formal software verification in more detail. We report our experiences in using Perfect Developer for the course assignments.";"100,00";"100,00";"100,00"
"TUW-140895";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\groundtruth\TUW-140895.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\result\result-TUW-140895-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\extracted\grobid\grobid-TUW-140895-xstream.xml"")";"This master thesis presents an ant colony optimisation algorithm for the bounded diameter minimum spanning tree problem, a N P-hard combinatorial optimisation problem with various application fields, e.g. when considering certain aspects of quality in communication network design. The algorithm is extended with local optimisation in terms of a variable neighbourhood descent algorithm based on four different neighbourhood structures. These neighbourhood structures have been designed in a way to enable a fast identification of the best neighbouring solution. The proposed algorithm is empirically compared to various evolutionary algorithms and a variable neighbourhood search implementation on Euclidean instances based on complete graphs with up to 1000 nodes considering either solution quality as well as computation time. It turns out that the ant colony optimisation algorithm performs best among these heuristics with respect to quality of solution, but cannot reach the results of the variable neighbourhood search implementation concerning computation time.";"ZUSAMMENFASSUNG Im Rahmen dieser Magisterarbeit wurde ein Ant Colony Optimisation Algorithmus für das durchmesserbeschränkte minimale Spannbaum Problem erarbeitet. Bei diesem Problem handelt es sich um ein N P-schweres kombinatorisches Optimierungsproblem mit zahl-reichen praktischen Anwendungsgebieten, zum Beispiel im Netzwerkdesign. Der Algorith-mus wurde mit einer lokalen Optimierungsheuristik, nämlich einem Variable Neighbourhood Descent, erweitert. Diese lokale Optimierungsheuristik arbeitet auf vier verschiedenen Nachbarschaftsstrukturen, bei deren Entwicklung besonders auf eine effiziente Evaluierung der Nachbarschaft einer Lösung Wert gelegt wurde. Vergleiche mit verschiedenen evolu-tionären Algorithmen und einer variablen Nachbarschaftssuche auf euklidischen Instanzen bis zu 1000 Knoten hinsichtlich Lösungsqualität als auch Berechnungszeit zeigen, dass der Ant Colony Optimisation Algorithmus bei ausreichend Zeit die bisher besten bekan-nten Ergebnisse zum Teil deutlichübertreffendeutlich¨deutlichübertreffen kann, hingegen bei Testläufen mit starker Zeitbeschränkung nicht die Lösungsqualität der variablen Nachbarschaftssuche erreichen kann. ABSTRACT This master thesis presents an ant colony optimisation algorithm for the bounded diameter minimum spanning tree problem, a N P-hard combinatorial optimisation problem with various application fields, e.g. when considering certain aspects of quality in communication network design. The algorithm is extended with local optimisation in terms of a variable neighbourhood descent algorithm based on four different neighbourhood structures. These neighbourhood structures have been designed in a way to enable a fast identification of the best neighbouring solution. The proposed algorithm is empirically compared to various evolutionary algorithms and a variable neighbourhood search implementation on Euclidean instances based on complete graphs with up to 1000 nodes considering either solution quality as well as computation time. It turns out that the ant colony optimisation algorithm performs best among these heuristics with respect to quality of solution, but cannot reach the results of the variable neighbourhood search implementation concerning computation time.";"0,00";"0,00";"0,00"
"TUW-140983";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\groundtruth\TUW-140983.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\result\result-TUW-140983-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\extracted\grobid\grobid-TUW-140983-xstream.xml"")";"In this paper we construct adaptive user profiles from tagging data. We present and evaluate an algorithm for creating such profiles, characterizing its behavior through statistical analysis.";"In this paper we construct adaptive user profiles from tagging data. We present and evaluate an algorithm for creating such profiles, characterizing its behavior through statistical analysis.";"100,00";"100,00";"100,00"
"TUW-141024";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\groundtruth\TUW-141024.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\result\result-TUW-141024-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\extracted\grobid\grobid-TUW-141024-xstream.xml"")";"Clinical practice guidelines are widely used to support medical staff in treatment planning and decision-making, whereas, the classification of different recommendations in the CPGs are one of the most important information sources to use. However, there is a lack of consensus amongst guideline developers, regarding those classification schemes. To address this problem, we mapped the different graded and ungraded evidence information used by different guideline developing organizations into our meta schema. In this paper we describe how guideline representation languages, such as Asbru and PROforma can be extended to model our meta schema.";"Clinical practice guidelines are widely used to support medical staff in treatment planning and decision-making, whereas, the classification of different recommendations in the CPGs are one of the most important information sources to use. However, there is a lack of consensus amongst guideline developers, regarding those classification schemes. To address this problem, we mapped the different graded and ungraded evidence information used by different guideline developing organizations into our meta schema. In this paper we describe how guideline representation languages, such as Asbru and PROforma can be extended to model our meta schema.";"100,00";"100,00";"100,00"
"TUW-141065";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\groundtruth\TUW-141065.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\result\result-TUW-141065-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\extracted\grobid\grobid-TUW-141065-xstream.xml"")";"The need to provide more holistic adaptivity to students has brought us to investigate the relationship between learning styles and working memory capacity (WMC). The aim of this investigation is to study the relationship between learning styles and WMC in order to get additional information about the students. This information can be used to make more holistic adaptivity possible by improving the student modelling process of both learning styles and WMC. An experiment with 297 participants was conducted. Findings suggest that relationships from WMC to the active/reflective, the sensing/intuitive, and the visual/verbal learning styles exist, whereas the suggested relationship from WMC to sequential/global learning styles could not be found.";"* The need to provide more holistic adaptivity to students has brought us to investigate the relationship between learning styles and working memory capacity (WMC). The aim of this investigation is to study the relationship between learning styles and WMC in order to get additional information about the students. This information can be used to make more holistic adaptivity possible by improving the student modelling process of both learning styles and WMC. An experiment with 297 participants was conducted. Findings suggest that relationships from WMC to the active/reflective, the sensing/intuitive, and the visual/verbal learning styles exist, whereas the suggested relationship from WMC to sequential/global learning styles could not be found.";"100,00";"100,00";"100,00"
"TUW-141121";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\groundtruth\TUW-141121.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\result\result-TUW-141121-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\extracted\grobid\grobid-TUW-141121-xstream.xml"")";"Multimedia content can be described in versatile ways as its essence is not limited to one view. For music data these multiple views could be a song's audio features as well as its lyrics. Both of these modalities have their advantages as text may be easier to search in and could cover more of the 'content semantics' of a song, while omitting other types of semantic categorisation. (Psycho)acoustic feature sets, on the other hand, provide the means to identify tracks that 'sound similar' while less supporting other kinds of semantic categorisation. Those discerning characteristics of different feature sets meet users' differing information needs. We will explain the nature of text and audio feature sets which describe the same audio tracks. Moreover, we will propose the use of textual data on top of low level audio features for music genre classification. Further, we will show the impact of different combinations of audio features and textual features based on content words.";"Multimedia content can be described in versatile ways as its essence is not limited to one view. For music data these multiple views could be a song's audio features as well as its lyrics. Both of these modalities have their advantages as text may be easier to search in and could cover more of the 'content semantics' of a song, while omitting other types of semantic categorisation. (Psycho)acoustic feature sets, on the other hand, provide the means to identify tracks that 'sound simi-lar' while less supporting other kinds of semantic categorisation. Those discerning characteristics of different feature sets meet users' differing information needs. We will explain the nature of text and audio feature sets which describe the same audio tracks. Moreover, we will propose the use of textual data on top of low level audio features for music genre classification. Further, we will show the impact of different combinations of audio features and textual features based on content words.";"100,00";"100,00";"100,00"
"TUW-141140";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\groundtruth\TUW-141140.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\result\result-TUW-141140-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\extracted\grobid\grobid-TUW-141140-xstream.xml"")";"Current model-driven Web Engineering approaches (such as OO-H, UWE or WebML) provide a set of methods and supporting tools for a systematic design and development of Web applications. Each method addresses different concerns using separate models (content, navigation, presentation, business logic, etc.), and provide model compilers that produce most of the logic and Web pages of the application from these models. However, these proposals also have some limitations, especially for exchanging models or representing further modeling concerns, such as architectural styles, technology independence , or distribution. A possible solution to these issues is provided by making model-driven Web Engineering proposals interoperate, being able to complement each other, and to exchange models between the different tools. MDWEnet is a recent initiative started by a small group of researchers working on model-driven Web Engineering (MDWE). Its goal is to improve current practices and tools for the model-driven development of Web applications for better interoperability. The proposal is based on the strengths of current model-driven Web Engineering methods, and the existing experience and knowledge in the field. This paper presents the background, motivation, scope, and objectives of MDWEnet. Furthermore, it reports on the MDWEnet results and achievements so far, and its future plan of actions.";"Current model-driven Web Engineering approaches (such as OO-H, UWE or WebML) provide a set of methods and supporting tools for a systematic design and development of Web applications. Each method addresses different concerns using separate models (content, navigation, presentation, business logic, etc.), and provide model compilers that produce most of the logic and Web pages of the application from these models. However, these proposals also have some limitations, especially for exchanging models or representing further modeling concerns, such as architectural styles, technology independence , or distribution. A possible solution to these issues is provided by making model-driven Web Engineering proposals interoperate, being able to complement each other, and to exchange models between the different tools. MDWEnet is a recent initiative started by a small group of researchers working on model-driven Web Engineering (MDWE). Its goal is to improve current practices and tools for the model-driven development of Web applications for better interoperability. The proposal is based on the strengths of current model-driven Web Engineering methods, and the existing experience and knowledge in the field. This paper presents the background, motivation, scope, and objectives of MDWEnet. Furthermore, it reports on the MDWEnet results and achievements so far, and its future plan of actions.";"100,00";"100,00";"100,00"
"TUW-141336";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\groundtruth\TUW-141336.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\result\result-TUW-141336-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\extracted\grobid\grobid-TUW-141336-xstream.xml"")";"The QXOR-SAT problem is the quantified version of the satisfiability problem XOR-SAT in which the connective exclusive-or is used instead of the usual or. We study the phase transition associated with random QXOR-SAT instances. We give a description of this phase transition in the case of one alternation of quantifiers, thus performing an advanced practical and theoretical study on the phase transition of a quantified problem.";"The QXOR-SAT problem is the quantified version of the satisfiability problem XOR-SAT in which the connective exclusive-or is used instead of the usual or. We study the phase transition associated with random QXOR-SAT instances. We give a description of this phase transition in the case of one alternation of quantifiers, thus performing an advanced practical and theoretical study on the phase transition of a quantified problem.";"100,00";"100,00";"100,00"
"TUW-141618";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\groundtruth\TUW-141618.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\result\result-TUW-141618-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\extracted\grobid\grobid-TUW-141618-xstream.xml"")";"On-line analytical processing (OLAP) is a powerful data analysis technology. However, current implementations bypass the 3-layer model of databases for performance reasons and create a redundant database
	
The clear separation between external views, conceptional model and internal physical representations in the three layer database architecture constitutes one of the key factors to the success of database technology for on-line transaction processing (OLTP). However, experience has shown that it is not feasible to construct an on-line analytical processing (OLAP) view onto the data within this model. As a compromise, materialized views were introduced. Yet, these limit the applicability of OLAP to fairly standard applications. Some application domains such as the analysis of stock market data or telecommunication systems with a diverse structure and unpredictable aggregation and query patterns can not be timely handled using pre-materialization. As a result, the benefits of OLAP are not available when building analysis and visualization tool for this kind of applications.

We envision OLAP-on-demand as a foundation for the re-unification of the transaction processing and analysis in databases. Instead of pre-materializing expected queries, analysts can integrate the available data sources on the fly where the analysis of information is not delayed by certain``certain``update windows''. Although current computing hardware does not provide the necessary data throughput, we expect future generation systems to cope with the high demands of this approach. In the meantime, parallel and distributed query processing can be used to achieve the required response times.

As an ultimate goal the realisation of Codd's formulaic model is envisioned.";"On-line analytical processing (OLAP) is a powerful data analysis technology. However, current implementations bypass the 3-layer model of databases for performance reasons and create a redundant database The clear separation between external views, conceptional model and internal physical representations in the three layer database architecture constitutes one of the key factors to the success of database technology for on-line transaction processing (OLTP). However, experience has shown that it is not feasible to construct an on-line analytical processing (OLAP) view onto the data within this model. As a compromise, materialized views were introduced. Yet, these limit the applicability of OLAP to fairly standard applications. Some application domains such as the analysis of stock market data or telecommunication systems with a diverse structure and unpredictable aggregation and query patterns can not be timely handled using pre-materialization. As a result, the benefits of OLAP are not available when building analysis and visualization tool for this kind of applications. We envision OLAP-on-demand as a foundation for the re-unification of the transaction processing and analysis in databases. Instead of pre-materializing expected queries, analysts can integrate the available data sources on the fly where the analysis of information is not delayed by certain``certain``update windows''. Although current computing hardware does not provide the necessary data throughput, we expect future generation systems to cope with the high demands of this approach. In the meantime, parallel and distributed query processing can be used to achieve the required response times. As an ultimate goal the realisation of Codd's formulaic model is envisioned. ____________________________ 1)";"100,00";"100,00";"100,00"
"TUW-141758";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\groundtruth\TUW-141758.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\result\result-TUW-141758-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\extracted\grobid\grobid-TUW-141758-xstream.xml"")";"Medical information is often stored in a narrative way, which makes the automated processing a difficult and time-consuming task. Persons responsible for the authoring of medical documents do not take care of a further processing with automated systems. So, information stored in medical writings is not directly usable for the processing with computers. Due to this, efforts have been made to transfer these narrative documents in a format easier processable with computers. This matter of fact also applies to clinical practice guidelines (CPGs). As many medical documents , CPGs are written in a narrative speech as well, without regards to a computer-assisted processing. For the implementation of CPGs in medical facilities an automated processing is therefore desirable. An important fact is that a lot of information in CPGs is provided in a negated form, expressing that certain circumstances in patients or treatments are not available, existing or applicable. Although negated, this information is nevertheless very useful, since it can express the absence of certain conditions or diseases in patients. Moreover, negations can describe which treatment options should not be taken into account for a given patient, helping a practising physician or nurse in his/her decision process for the assortment of a proper treatment. Thus, a proper Negation Detection in CPGs is an important task for the automated processing of this type of medical documents. It helps to accelerate the decision making process and can support medical staff in their care for patients. We developed algorithms capable of Negation Detection in CPGs. We use syntactical methods provided by the English language to achieve a precise detection of occuring negations. According to our results we are convinced that the involvement of syntactical methods can improve Negation Detection, not only in medical writings but also in arbitrary narrative texts.";"Medical information is often stored in a narrative way, which makes the automated processing a difficult and time-consuming task. Persons responsible for the authoring of medical documents do not take care of a further processing with automated systems. So, information stored in medical writings is not directly usable for the processing with computers. Due to this, efforts have been made to transfer these narrative documents in a format easier processable with computers. This matter of fact also applies to clinical practice guidelines (CPGs). As many medical documents , CPGs are written in a narrative speech as well, without regards to a computer-assisted processing. For the implementation of CPGs in medical facilities an automated processing is therefore desirable. An important fact is that a lot of information in CPGs is provided in a negated form, expressing that certain circumstances in patients or treatments are not available, existing or applicable. Although negated, this information is nevertheless very useful, since it can express the absence of certain conditions or diseases in patients. Moreover, negations can describe which treatment options should not be taken into account for a given patient, helping a practising physician or nurse in his/her decision process for the assortment of a proper treatment. Thus, a proper Negation Detection in CPGs is an important task for the automated processing of this type of medical documents. It helps to accelerate the decision making process and can support medical staff in their care for patients. We developed algorithms capable of Negation Detection in CPGs. We use syntactical methods provided by the English language to achieve a precise detection of occuring negations. According to our results we are convinced that the involvement of syntactical methods can improve Negation Detection, not only in medical writings but also in arbitrary narrative texts. ii";"100,00";"100,00";"100,00"
"TUW-168222";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\groundtruth\TUW-168222.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\result\result-TUW-168222-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\extracted\grobid\grobid-TUW-168222-xstream.xml"")";"Recent research results on communication science mention a relation between the inter-locutor's perceptional preferences, his or her mental representations and his or her choice of expressions. However, there is not any tool for analyzing such. In my research I combined lexical with computational linguistic methods to develop a software prototype that is able to analyze text on the usage of perceptional expressions. This analyzing tool can help to identify the interlocutor's perceptional preference for easier meeting his or her way of thinking and thereby facilitate the understanding.";"Recent research results on communication science mention a relation between the inter-locutor's perceptional preferences, his or her mental representations and his or her choice of expressions. However, there is not any tool for analyzing such. In my research I combined lexical with computational linguistic methods to develop a software prototype that is able to analyze text on the usage of perceptional expressions. This analyzing tool can help to identify the interlocutor's perceptional preference for easier meeting his or her way of thinking and thereby facilitate the understanding. Still, for the German language, not much research on the relation between an individual's perceptional preference and his or her language use has been done yet. To be able to go deeper into that idea, (1) a lexical corpus of perceptional expressions and (2) a software tool that automatically filters those expressions from a text are needed. Those two steps shall be explained in this abstract.";"0,00";"0,00";"0,00"
"TUW-168482";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\groundtruth\TUW-168482.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\result\result-TUW-168482-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\extracted\grobid\grobid-TUW-168482-xstream.xml"")";"Formal dialogue games are a traditional approach to characterize the semantics of logics. In the 1970s Robin Giles attempted to provide an operational foundation for formal reasoning in physical theories by dialogue games based on atomic experiments that may show dispersion. This thesis motivates, describes and analyzes his approach and the connection to t-norm based fuzzy logics. We give a short introduction into t-norms and many-valued logics based on t-norms. In particular we focus on three fundamental t-norm based fuzzy logics: Lukasiewicz Logic, Gödel Logic, and Product Logic. We present and discuss several approaches for extending the game rules of Giles's Game in order to make it adequate for Gödel Logic and Product Logic. Moreover, we give hints at a strong correspondence between winning strategies in the game and derivations in an analytic proof system based on relational hypersequents. Another type of dialogue games are truth comparison games. This type is suitable for Gödel Logic and relates more to the degree based semantics of that logic than Giles's Game. We present the game and discuss winning strategies for both players indicating the validity or refutability of a formula. Additionally, several utilities implemented in the context of this thesis are presented. Amongst these is a web-based application which allows for the interactive exploration of Giles's Game and its extensions.";"Zusammenfassung Formale Dialogspiele werden schon seit langem dazu verwendet, die Semantik verschiedener Logiken zu charakterisieren. In den 70er Jahren präsentierte Robin Giles seinen Versuch, eine operationale Grundlage für formales Schließen zu definieren, basierend auf atomaren Exper-imenten, welche Dispersion aufweisen können. Diese Masterarbeit motiviert und beschreibt seinen Ansatz und die Verbindung zu t-Norm-basierten Fuzzy-Logiken. Wir geben eine kurze Einführung in t-Normen und mehrwertige Fuzzy-Logiken, die auf diese Bezug nehmen. Im Speziellen liegt der Schwerpunkt auf drei solchen fundamen-talen Fuzzy-Logiken: Lukasiewicz-Logik, Gödel-Logik und Produkt-Logik. Verschiedene Möglichkeiten, die Spielregeln von Giles' Spiel zü andern, um dieses adäquat für Gödel-und Produkt-Logik zu machen, werden präsentiert und diskutiert. Darüber hinaus beschreiben wir die starke Verbindung zwischen Gewinnstrategien im Spiel und Ableitungen in einem analytischen Kalkül, der auf relationalen Hypersequenten basiert. Eine andere Art von Dialogspielen sind sogenannte ""Truth Comparison Games"". Diese sind besonders geeignet für Gödel-Logik, da sie der gradbasierten Semantik der Gödel-Logik mehr entsprechen als Giles' Spiel. Wir präsentieren das Spiel und diskutieren Gewinnstrate-gien für beide Spieler, welche als Beweis für die Gültigkeit oder Widerlegbarkeit einer Formel gesehen werden können. Zusätzlich werden mehrere Hilfsprogramme vorgestellt, die im Kontext dieser Master-arbeit entwickelt wurden. Darunter befindet sich auch eine webbasierte Anwendung zur interaktiven Exploration von Giles' Spiel und dessen Erweiterungen. i Abstract Formal dialogue games are a traditional approach to characterize the semantics of logics. In the 1970s Robin Giles attempted to provide an operational foundation for formal reasoning in physical theories by dialogue games based on atomic experiments that may show dispersion. This thesis motivates, describes and analyzes his approach and the connection to t-norm based fuzzy logics. We give a short introduction into t-norms and many-valued logics based on t-norms. In particular we focus on three fundamental t-norm based fuzzy logics: Lukasiewicz Logic, Gödel Logic, and Product Logic. We present and discuss several approaches for extending the game rules of Giles's Game in order to make it adequate for Gödel Logic and Product Logic. Moreover, we give hints at a strong correspondence between winning strategies in the game and derivations in an analytic proof system based on relational hypersequents. Another type of dialogue games are truth comparison games. This type is suitable for Gödel Logic and relates more to the degree based semantics of that logic than Giles's Game. We present the game and discuss winning strategies for both players indicating the validity or refutability of a formula. Additionally, several utilities implemented in the context of this thesis are presented. Amongst these is a web-based application which allows for the interactive exploration of Giles's Game and its extensions. ii";"0,00";"0,00";"0,00"
"TUW-169511";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\groundtruth\TUW-169511.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\result\result-TUW-169511-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\extracted\grobid\grobid-TUW-169511-xstream.xml"")";;;"none extracted value";"none expected";"NA"
"TUW-172697";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\groundtruth\TUW-172697.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\result\result-TUW-172697-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\extracted\grobid\grobid-TUW-172697-xstream.xml"")";"Creating a concrete plan for preserving an institution's collection of digital objects requires the evaluation of available solutions against clearly defined and measurable criteria. Preservation planning aids in this decision making process to find the best preservation strategy considering the institution's requirements, the planning context and possible actions applicable to the objects contained in the repository. Performed manually, this evaluation of possible solutions against requirements takes a good deal of time and effort. In this demonstration, we present Plato, an interactive software tool aimed at creating preservation plans.";"Creating a concrete plan for preserving an institution's collection of digital objects requires the evaluation of available solutions against clearly defined and measurable criteria. Preservation planning aids in this decision making process to find the best preservation strategy considering the institution's requirements, the planning context and possible actions applicable to the objects contained in the repository. Performed manually, this evaluation of possible solutions against requirements takes a good deal of time and effort. In this demonstration, we present Plato, an interactive software tool aimed at creating preservation plans. Plato Plato 1 implements the PLANETS 2 Preservation Planning approach [2] which provides a solid way of making informed and accountable decisions on which solution to put into practise in order to optimally preserve digital objects for a given purpose. The tool is integrated into the PLANETS Interop-erability Framework based on open J2EE and web technologies. Through this environment it integrates registries and services for preservation action and characterisation through flexible discovery and invocation. Characterisation services such as DROID 3 and JHove 4 are used for format identification and property extraction; based on this information, applicable action services such as emula-tion tools or the migration services provided by CRiB[3] are discovered through available registries. Comparison and validation of objects as an essential feature of the system maps the specified requirements such as essential object characteristics to measurable criteria that can be compared automatically. It thus considerably improves the repeatability, documentation and automation of preservation planning. Plato has been developed with very close attention to the web user interface to not impose any technical restrictions on the user when determining the requirements. Figure 1 shows the requirements for a preservation endeavour of a web archive laid out in a tree structure. The tool offers a fully flexible way to enable the specification of a wide range of measurement scales. As the definition of requirements in a tree structure is often done in a workshop setting, Plato also supports tree import from mind-mapping software 5. The applicability and usefulness of the tool has been validated in a series of workshops and case studies which involved various institutions [2, 1].";"0,00";"0,00";"0,00"
"TUW-174216";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\groundtruth\TUW-174216.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\result\result-TUW-174216-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\extracted\grobid\grobid-TUW-174216-xstream.xml"")";"During the last decades, digital objects have become the primary medium to create, shape, and exchange information. However, in contrast to analog objects such as books that directly represent their content, digital objects are not usable without a corresponding technical environment. The fast changes in these environments and in formats and technologies mean that digital documents have a short lifespan before they become obsolete. Digital preservation, i.e. actions to ensure longevity of digital information, thus has become a pressing challenge. The dominant strategies prevailing today are migration and emulation; for each strategy, different tools are available. When converting an object to a different representation, a validation of the content is needed to verify that the transformed objects are still authentically representing the same intellectual content. This validation so far is largely done manually, which is infeasible for large collections.
Preservation planning supports decision makers in reaching accountable decisions by evaluating potential strategies against well-defined requirements. Especially the evaluation of different migration tools for digital preservation has to rely on validating the converted objects and thus on an analysis of the logical structure and the content of documents. Existing approaches for characterising and describing objects do not attempt to fully extract the informational content of digital objects and thus are not suffficient for an in-depth validation of transformed content.
This paper describes the eXtensible Characterisation Languages (XCL) that support the automatic validation of document conversions and the evaluation of migration quality by hierarchically decomposing a document and representing documents from different sources in an abstract XML language. The description language XCDL provides an abstract representation of digital content in XML, while the extraction language XCEL allows an extraction engine to create such an abstract description by mapping file format structures to XCDL concepts. We present the context of the development of these languages and tools and describe the overall concept and features of the languages. We further give examples and show how the languages can be applied to the evaluation of digital preservation solutions in the context of preservation planning.";"During the last decades, digital objects have become the primary medium to create, shape, and exchange information. However, in contrast to analog objects such as books that directly represent their content, digital objects are not usable without a corresponding technical environment. The fast changes in these environments and in formats and technologies mean that digital documents have a short lifespan before they become obsolete. Digital preservation, i.e. actions to ensure longevity of digital information, thus has become a pressing challenge. The dominant strategies prevailing today are migration and emulation; for each strategy, different tools are available. When converting an object to a different representation, a validation of the content is needed to verify that the transformed objects are still authentically representing the same intellectual content. This validation so far is largely done manually, which is infeasible for large collections. Preservation planning supports decision makers in reaching accountable decisions by evaluating potential strategies against well-defined requirements. Especially the evaluation of different migration tools for digital preservation has to rely on validating the converted objects and thus on an analysis of the logical structure and the content of documents. Existing approaches for characterising and describing objects do not attempt to fully extract the informational content of digital objects and thus are not suffficient for an in-depth validation of transformed content. This paper describes the eXtensible Characterisation Languages (XCL) that support the automatic validation of document conversions and the evaluation of migration quality by hierarchically decomposing a document and representing documents from different sources in an abstract XML language. The description language XCDL provides an abstract representation of digital content in XML, while the extraction language XCEL allows an extraction engine to create such an abstract description by mapping file format structures to XCDL concepts. We present the context of the development of these languages and tools and describe the overall concept and features of the languages. We further give examples and show how the languages can be applied to the evaluation of digital preservation solutions in the context of preservation planning.";"100,00";"100,00";"100,00"
"TUW-175428";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\groundtruth\TUW-175428.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\result\result-TUW-175428-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\extracted\grobid\grobid-TUW-175428-xstream.xml"")";"Within this thesis a real-world problem related to a warehouse for spare parts is considered. Regarding several constraints typically stated by spare parts suppliers the time needed to collect articles should be minimized. Based on continuously arriving orders by customers predefined delivery times and capacity constraints have to be met. To accomplish this task efficient pickup tours need to be determined which is the main issue covered by this work which comes to an end with experimental results of a concrete implementation of the proposed approach.
The algorithm presented embeds a specifically developed dynamic program for computing optimal walks through the warehouse into a general variable neighborhood search (VNS) scheme. Several stages are used for first splitting up all orders, then creating tours out of the results and finally assigning them to available workers. The VNS uses a variant of the variable neighborhood descent (VND) as local improvement procedure. While the neighborhood structures defined are intended to produce candidate solutions, a dynamic program specially designed to compute optimal order picking tours is used to evaluate them. For this purpose properties specific to warehouses are exploited such to compute optimal routes within polynomial time. The final assignment of workers to tours is realized by another VNS. The task is then to find an allocation such that the last article to be picked up will be collected as early as possible.
Evaluations of experimental results of a concrete implementation indicate that the presented approach provides valuable pickup plans and computation times can be kept low. Moreover the performed test runs have been compared to a reference solution which was computed based on an approach found in relevant literature. A detailed analysis of the obtained results showed that the application of the proposed approach to real-world instances is promising whereas the savings with respect to working time can be kept high. Overall an efficient as well as effective approach is introduced to solve this real-world problem.";"A-1040 Wien Karlsplatz 13 Tel. +43/(0)1/58801-0 http://www.tuwien.ac.at ii Danksagung Die vorliegende Arbeit durfte ich am Institut für Computergraphik und Algorithmen der Technischen Universität Wien erstellen und es freut mich diese nun fertig in Händen halten zu können. Ich möchte mich für die Geduld und Mithilfe von Seiten Günther Raidls bedanken und ebenso Matthias Prandtstetter großen Dank für seine Betreuung aussprechen. In den vergangenen Monaten hat er einen besonders großen Beitrag zur Vervollständi-gung dieser Arbeit geleistet. Natürlich gilt meine Anerkennung auch allen anderen Personen, die von Seiten des Instituts ihren Anteil an der Entstehung dieser Arbeit hatten. Es ist mir weiters ein Anliegen auch all jene zu erwähnen, die mich im Laufe des gesamten Studiums begleitet und unterstützt haben. Dazu zählen vor allem meine Studienkollegen Christian und Gerhard, sowie gleichermaßen auch meine Eltern, die mir zu jedem Zeitpunkt eine große Hilfe waren und Linda, der ich an dieser Stelle für ihre Ausdauer und Motivation danken will. iii iv Erklärung zur Verfassung der Arbeit Hiermit erkläre ich, Thomas Misar, wohnhaft in 1070 Wien, Seidengasse 3/108, dass ich diese Arbeit selbständig verfasst habe, dass ich die verwendeten Quellen und Hilfs-mittel vollständig angegeben habe und dass ich die Stellen der Arbeit (einschließlich Tabellen, Karten und Abbildungen), die anderen Werken oder dem Internet im Wort-laut oder dem Sinn nach entnommen sind, auf jeden Fall unter Angabe der Quelle als Entlehnung kenntlich gemacht habe. Wien, 20.04.2009 (Unterschrift Verfasser) vi Kurzfassung Im Rahmen dieser Arbeit wird eine konkrete Problemstellung aus dem Bereich der Lagerverwaltung behandelt. Dabei soll die benötigte Zeit zum Ausfassen von Artikeln aus dem Lager unter Berücksichtigung von domänenspezifischen Nebenbedingungen minimiert werden. Ausgehend von durch Kunden laufend aufgegebenen Bestellungen sollen feste Lieferzeiten eingehalten werden und Einschränkungen wie etwa Kapa-zitätslimits oder das Vermeiden von Kollisionen zwischen Arbeitern beachtet werden. Die für die gegebene Problemstellung zentrale Bestimmung effizienter Touren steht im Mittelpunkt der Arbeit, welche mit Ergebnissen aus einer konkreten Implemen-tierung des vorgestellten Ansatzes abschließt. Es wird ein Algorithmus vorgestellt, der ein eigens entwickeltes Dynamisches Pro-gramm zur Berechnung optimaler Wege durch das Warenlager mit der Umsetzung einer Variablen Nachbarschaftssuche (engl.: Variable Neighborhood Search) (VNS) verbindet. In mehreren Phasen werden dabei die vorliegenden Bestellungen zerlegt und davon ausgehend Touren gebildet, welche zuletzt auf alle verfügbaren Lagerar-beiter verteilt werden. Innerhalb der VNS kommt eine Variante des Variable Neighborhood Descent (VND) als lokale Verbesserungskomponente zum Einsatz. Während Während¨Währendüber die definierten Nachbarschaftsstrukturen unterschiedliche potentielle Lösungen erzeugt werden, erfolgt deren Bewertung durch die Berechnung von konkreten Touren mittels eines für diesen Zweck entwickelten Dynamischen Programms. Dabei werden spezielle Eigenschaften der zugrundeliegenden Lagerstruktur ausgenutzt, um so in po-lynomieller Zeit die bestmögliche Wegführung durch das Lager berechnen zu können. Für die Zuordnung von Arbeitern zu den auf diese Weise berechneten Touren wird schließlich eine zusätzliche VNS verwendet, deren Aufgabe es ist, die notwendigen Touren derart zu verteilen, dass der letzte Artikel zum frühest möglichen Zeitpunkt ausgefasst werden kann. Die anhand des implementierten Programms durchgeführten Tests zeigen, dass die erfolgte Tourenplanung wertvolle Ergebnisse liefert und die notwendige Rechenzeit niedrig gehalten werden kann. Getestet wurde mit Bezug auf eine Referenzlösung, welche auf Basis eines aus der Literatur entnommenen Ansatzes erzeugt werden konn-te. Eine ausführliche Auswertung der Testergebnisse zeigte, dass die Anwendung des hier vorgestellten Ansatzes im Echtbetrieb als sehr vielversprechend gilt und er-hebliche Einsparungen bezüglich der benötigten Arbeitszeit erreicht werden können. Insgesamt betrachtet wird ein effizientes und zielführendes Verfahren zur Lösung des vorliegenden Problems vorgestellt. vii viii Abstract Within this thesis a real-world problem related to a warehouse for spare parts is considered. Regarding several constraints typically stated by spare parts suppliers the time needed to collect articles should be minimized. Based on continuously arriving orders by customers predefined delivery times and capacity constraints have to be met. To accomplish this task efficient pickup tours need to be determined which is the main issue covered by this work which comes to an end with experimental results of a concrete implementation of the proposed approach. The algorithm presented embeds a specifically developed dynamic program for computing optimal walks through the warehouse into a general variable neighborhood search (VNS) scheme. Several stages are used for first splitting up all orders, then creating tours out of the results and finally assigning them to available workers. The VNS uses a variant of the variable neighborhood descent (VND) as local improvement procedure. While the neighborhood structures defined are intended to produce candidate solutions, a dynamic program specially designed to compute optimal order picking tours is used to evaluate them. For this purpose properties specific to warehouses are exploited such to compute optimal routes within polynomial time. The final assignment of workers to tours is realized by another VNS. The task is then to find an allocation such that the last article to be picked up will be collected as early as possible. Evaluations of experimental results of a concrete implementation indicate that the presented approach provides valuable pickup plans and computation times can be kept low. Moreover the performed test runs have been compared to a reference solution which was computed based on an approach found in relevant literature. A detailed analysis of the obtained results showed that the application of the proposed approach to real-world instances is promising whereas the savings with respect to working time can be kept high. Overall an efficient as well as effective approach is introduced to solve this real-world problem.";"0,00";"0,00";"0,00"
"TUW-176087";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\groundtruth\TUW-176087.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\result\result-TUW-176087-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\extracted\grobid\grobid-TUW-176087-xstream.xml"")";"Reviews and few non-controlled studies showed the effectiveness of several specific designed computer video-games as an additional form of treatment in several areas. However, there is a lack in the literature of specially designed serious-games for treating mental disorders. Playmancer (ICT European initiative) aims to develop and assess a serious videogame that may help to treat underlying processes (e.g. lack of self-control strategies) in Eating and Impulse control disorders. Preliminary data will be shown.";"Reviews and few non-controlled studies showed the effectiveness of several specific designed computer video-games as an additional form of treatment in several areas. However, there is a lack in the literature of specially designed serious-games for treating mental disorders. Playmancer (ICT European initiative) aims to develop and assess a serious videogame that may help to treat underlying processes (e.g. lack of self-control strategies) in Eating and Impulse control disorders. Preliminary data will be shown.";"100,00";"100,00";"100,00"
"TUW-177140";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\groundtruth\TUW-177140.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\result\result-TUW-177140-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\extracted\grobid\grobid-TUW-177140-xstream.xml"")";"The system cc? is a tool for testing correspondence between propo-sitional logic programs under the answer-set semantics with respect to different refined notions of program correspondence. The underlying methodology of cc? is to reduce a given correspondence problem to the satisfiability problem of quantified propositional logic and to employ extant solvers for the latter language as back-end inference engines. In a previous version of cc?, the system was designed to test correspondence between programs based on relativised strong equivalence under answer-set projection. Such a setting generalises the standard notion of strong equivalence by taking the alphabet of the context programs as well as the projection of the compared answer sets to a set of designated output atoms into account. This paper outlines a newly added component of cc? for testing similarly parameterised correspondence problems based on uniform equivalence.";"The system cc? is a tool for testing correspondence between propo-sitional logic programs under the answer-set semantics with respect to different refined notions of program correspondence. The underlying methodology of cc? is to reduce a given correspondence problem to the satisfiability problem of quantified propositional logic and to employ extant solvers for the latter language as back-end inference engines. In a previous version of cc?, the system was designed to test correspondence between programs based on relativised strong equivalence under answer-set projection. Such a setting generalises the standard notion of strong equivalence by taking the alphabet of the context programs as well as the projection of the compared answer sets to a set of designated output atoms into account. This paper outlines a newly added component of cc? for testing similarly parameterised correspondence problems based on uniform equivalence. 1 Motivation and General Information An important issue in software development is to determine whether two encodings of a given problem are equivalent, i.e., whether they yield the same result on a given problem instance. Depending on the context of problem representations, different definitions of ""equivalence"" are useful and desirable. The system cc? [1] (short for ""correspondence-checking tool"") is devised as a checker for a broad range of different such comparison relations defined between disjunctive logic programs (DLPs) under the answer-set semantics [2]. In a previous version of cc?, the system was designed to test correspondence between logic programs based on relativised strong equivalence under answer-set projection. Such a setting generalises the standard notion of strong equivalence [3] by taking the alphabet of the context programs as well as the projection of the compared answer sets to a set of designated output atoms into account [4]. The latter feature ?";"0,00";"0,00";"0,00"
"TUW-179146";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\groundtruth\TUW-179146.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\result\result-TUW-179146-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\extracted\grobid\grobid-TUW-179146-xstream.xml"")";"The publish/subscribe paradigm is a common concept for delivering events from information producers to consumers in a decoupled manner. Some approaches allow durable subscriptions or the transportation of events even to mobile subscribers in a dynamic network infrastructure. However, in the safety-critical telematics durable delivery of events is not sufficient enough. Short network connectiv-ity time and small bandwidth limit the number and size of events to be transmitted hence relevant information needed for safety-critical decision making may not be timely delivered.
In this paper we propose the integration of publish/ subscribe systems and Aspect-oriented Space Containers (ASC) distributed via Distributed Hash Tables (DHT) in the network. The approach allows storage, manipulation, pre-processing, and prioritization of messages sent to mobile peers during bursts of connectivity.
The benefits of the proposed approach are a) less complex application logic due to the processing capabilities of Space Containers, and b) increased efficiency due to delivery of essential messages only aggregated and processed while mobile peers are not connected. We describe the architecture of the proposed approach and explain its benefits by means of an industry use case.";"The publish/subscribe paradigm is a common concept for delivering events from information producers to consumers in a decoupled manner. Some approaches allow durable subscriptions or the transportation of events even to mobile subscribers in a dynamic network infrastructure. However, in the safety-critical telematics durable delivery of events is not sufficient enough. Short network connectiv-ity time and small bandwidth limit the number and size of events to be transmitted hence relevant information needed for safety-critical decision making may not be timely delivered. In this paper we propose the integration of publish/ subscribe systems and Aspect-oriented Space Containers (ASC) distributed via Distributed Hash Tables (DHT) in the network. The approach allows storage, manipulation, pre-processing, and prioritization of messages sent to mobile peers during bursts of connectivity. The benefits of the proposed approach are a) less complex application logic due to the processing capabilities of Space Containers, and b) increased efficiency due to delivery of essential messages only aggregated and processed while mobile peers are not connected. We describe the architecture of the proposed approach and explain its benefits by means of an industry use case.";"100,00";"100,00";"100,00"
"TUW-180162";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\groundtruth\TUW-180162.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\result\result-TUW-180162-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\extracted\grobid\grobid-TUW-180162-xstream.xml"")";"Web spam denotes the manipulation of web pages with the sole intent to raise their position in search engine rankings. Since a better position in the rankings directly and positively affects the number of visits to a site, attackers use different techniques to boost their pages to higher ranks. In the best case, web spam pages are a nuisance that provide undeserved advertisement revenues to the page owners. In the worst case, these pages pose a threat to Internet users by hosting malicious content and launching drive-by attacks against unsuspecting victims. When successful, these drive-by attacks then install malware on the victims' machines. In this paper, we introduce an approach to detect web spam pages in the list of results that are returned by a search engine. In a first step, we determine the importance of different page features to the ranking in search engine results. Based on this information, we develop a classification technique that uses important features to successfully distinguish spam sites from legitimate entries. By removing spam sites from the results, more slots are available to links that point to pages with useful content. Additionally, and more importantly, the threat posed by malicious web sites can be mitigated, reducing the risk for users to get infected by malicious code that spreads via drive-by attacks.";"Web spam denotes the manipulation of web pages with the sole intent to raise their position in search engine rankings. Since a better position in the rankings directly and positively affects the number of visits to a site, attackers use different techniques to boost their pages to higher ranks. In the best case, web spam pages are a nuisance that provide undeserved advertisement revenues to the page owners. In the worst case, these pages pose a threat to Internet users by hosting malicious content and launching drive-by attacks against unsuspecting victims. When successful, these drive-by attacks then install malware on the victims' machines. In this paper, we introduce an approach to detect web spam pages in the list of results that are returned by a search engine. In a first step, we determine the importance of different page features to the ranking in search engine results. Based on this information, we develop a classification technique that uses important features to successfully distinguish spam sites from legitimate entries. By removing spam sites from the results, more slots are available to links that point to pages with useful content. Additionally, and more importantly, the threat posed by malicious web sites can be mitigated, reducing the risk for users to get infected by malicious code that spreads via drive-by attacks. 1 Introduction Search engines are designed to help users find relevant information on the Internet. Typically, a user submits a query (i.e., a set of keywords) to a search engine, which then returns a list of links to pages that are most relevant to this query. To determine the most-relevant pages, a search engine selects a set of candidate pages that contain some or all of the query terms and calculates a page score for each page. Finally, a list of pages, sorted by their score, is returned to the user. This score is calculated from properties of the candidate pages, so-called features. Unfortunately, details on the exact algorithms that calculate these ranking values are kept secret by search engine companies, since this information directly influences the quality of the search results. Only general information is made available. For example, in 2007, Google claimed to take more than 200 features into account for the ranking value [8]. The way in which pages are ranked directly influences the set of pages that are visited frequently by the search engine users. The higher a page is ranked, the more likely it is to be visited [3]. This makes search engines an attractive target for everybody who aims to attract a large number of visitors to her site. There are three categories of web sites that benefit directly from high rankings in search engine results. First, sites that sell products or services. In their context, more visitors imply more potential customers. The second category contains sites that are financed through advertisement. These sites aim to rank high for any query. The reason is that they can display their advertisements to each visitor, and, in turn, charge the advertiser. The third, and most dangerous, category of sites that aim to attract many visitors by ranking high in search results are sites that distribute malicious software. Such sites typically contain code that exploits web browser vulnerabilities to silently install malicious software on the visitor's computer. Once infected, the attacker can steal sensitive information (such as passwords, financial information, or web-banking credentials), misuse the user's bandwidth to join a denial of service attack, or send spam. The threat of drive-by downloads (i.e., automatically down-loading and installing software without the user's consent as the result of a mere visit to a web page) and distribution of malicious software via web sites has become a significant security problem. Web sites that host drive-by downloads are either created solely for the purpose of distributing malicious software or existing pages that are hacked and modified (for example, by inserting an iframe tag into the page that loads malicious content). Provos et al. [13, 14] observe that such attacks can quickly reach a large number of potential victims, as at least 1.3% of all search queries directed to the Google search engine contain results that link to malicious pages. Moreover, the pull-based infection scheme circumvents barriers (such as web proxies or NAT devices) that protect from push-based malware infection schemes (such as traditional, exploit-based worms). As a result, the manipulation of search engine results is an attractive technique for attackers that aim to attract victims to their malicious sites and spread malware via drive-by attacks [16]. Search engine optimization (SEO) companies offer their expertise to help clients improve the rank for a given site through a mixture of techniques, which can be classified as being acceptable or malicious. Acceptable techniques refer to approaches that improve the content or the presentation of a page to the benefit of users. Malicious techniques, on the other hand, do not benefit the user but aim to mislead the search engine's ranking algorithm. The fact that bad sites can be pushed into undeserved, higher ranks via malicious SEO techniques leads to the problem of web spam. Gyöngyi and Garcia-Molina [9] define web spam as every deliberate human action that is meant to improve a site's ranking without changing the site's true value. Search engines need to adapt their ranking algorithms continuously to mitigate the effect of spamming techniques on their results. For example, when the Google search engine was launched, it strongly relied on the PageRank [2] algorithm to determine the ranking of a page where the rank is proportional to the number of incoming links. Unfortunately, this led to the problem of link farms and ""Google Bombs,"" where enormous numbers of automatically created forum posts and blog comments were used to promote an attacker's target page by linking to it. Clearly, web spam is undesirable, because it degrades the quality of search results and draws users to malicious sites. Although search engines invest a significant amount of money and effort into fighting this problem, checking the results of search engines for popular search terms demonstrates that the problem still exists. In this work, we aim to post-process results returned by a search engine to identify entries that link to spam pages. To this end, we first study the importance of different features for the ranking of a page. In some sense, we attempt to reverse-engineer the ""secret"" ranking algorithm of a search engine to understand better what features are important. Based on this analysis, we attempt to build a classifier that inspects these features to identify indications that a page is web spam. When such a page is identified, we can remove it from the search results. The two main contributions of this paper are the following: • We conducted comprehensive experiments to understand the effects of different features on search engine rankings.";"0,00";"0,00";"0,00"
"TUW-181199";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\groundtruth\TUW-181199.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\result\result-TUW-181199-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\extracted\grobid\grobid-TUW-181199-xstream.xml"")";"Answer-set programming (ASP) is an emerging logic-programming paradigm that strictly separates the description of a problem from its solving methods. Despite its semantic elegance, ASP suffers from a lack of support for program developers. In particular, tools are needed that help engineers in detecting erroneous parts of their programs. Unlike in other areas of logic programming , applying tracing techniques for debugging logic programs under the answer-set semantics seems rather unnatural, since employing imperative solving algorithms would undermine the declarative flavour of ASP. In this paper, we present the system spock, a debugging support tool for answer-set programs making use of ASP itself. The implemented techniques maintain the declarative nature of ASP within the debugging process and are independent of the actual computation of answer sets.";"Answer-set programming (ASP) is an emerging logic-programming paradigm that strictly separates the description of a problem from its solving methods. Despite its semantic elegance, ASP suffers from a lack of support for program developers. In particular, tools are needed that help engineers in detecting erroneous parts of their programs. Unlike in other areas of logic programming , applying tracing techniques for debugging logic programs under the answer-set semantics seems rather unnatural, since employing imperative solving algorithms would undermine the declarative flavour of ASP. In this paper, we present the system spock, a debugging support tool for answer-set programs making use of ASP itself. The implemented techniques maintain the declarative nature of ASP within the debugging process and are independent of the actual computation of answer sets.";"100,00";"100,00";"100,00"
"TUW-182414";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\groundtruth\TUW-182414.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\result\result-TUW-182414-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\extracted\grobid\grobid-TUW-182414-xstream.xml"")";"In den letzten Jahren haben Bibliotheken und Archive zunehmend die Aufgabe übernommen, neben konventionellen Publikationen auch Inhalte aus dem World Wide Web zu sammeln, um so diesen wertvollen Teil unseres kulturellen Erbes zu bewahren und wichtige Informationen langfristig verfügbar zu halten. Diese massiven Datensammlungen bieten faszinierende Möglichkeiten, rasch Zugriff auf wichtige Informationen zu bekommen, die im Live-Web bereits verloren gegangen sind. Sie sind eine unentbehrliche Quelle für Wissenschafter, die in der Zukunft die gesellschaftliche und technologische Entwicklung unserer Zeit nachvollziehen wollen.
Auf der anderen Seite stellt eine derartige Datensammlung aber einen völlig neuen Datenbestand dar, der nicht nur rechtliche, sondern auch zahlreiche ethische Fragen betreffend seine Nutzung aufwirft. Diese werden in dem Ausmaß zunehmen, in dem die technischen Möglichkeiten zur automatischen Analyse und Interpretation dieser Daten leistungsfähiger werden. Da sich die meisten Web-Archivierungsinitiativen dieser Problematik bewusst sind, bleibt die Nutzung der Daten derzeit meist stark eingeschränkt, oder es wird eine Art von ""Opt-Out""-Möglichkeit vorgesehen, wodurch Webseiteninhaber die Aufnahme ihrer Seiten in ein Webarchiv ausschließen können. Mit beiden Ansätzen können Webarchive ihr volles Nutzungspotential nicht ausschöpfen.
Das World Wide Web hat sich zu einem integralen Bestandteil unserer Publikations-und Kommunikationskultur entwickelt. Als solches bietet es uns einen reichhaltigen Schatz an wertvollen Informationen, die teilweise ausschließlich in elektronischer Form verfügbar sind, wie z.B. Informationsportale, Informationen zu zahlreichen Projekten und Bürgerinitiativen, Diskussionsforen, soziale Netze und Ähnliches. Weiters beeinflussen die technischen Möglichkeiten sowohl die Art der Gestaltung von Webseiten, als auch die Art, wie wir mit Information umgehen, wie unsere Gesellschaft vernetzt ist, wie sich Information ausbreitet bzw. wie sie genutzt wird. All dies stellt einen immens wertvollen Datenbestand dar, dessen Bedeutung uns teilweise erst bewusst werden mag, wenn dieser nicht mehr verfügbar ist. Dieser Artikel beschreibt einleitend kurz die Technologien, die zur Sammlung von Webinhalten zu Archivierungszwecken verwendet werden. Er hinterfragt Annahmen betreffend die freie Verfügbarkeit der Daten und unterschiedliche Nutzungsarten. Darauf aufbauend identifiziert er eine Reihe von offenen Fragen, deren Lösung einen breiteren Zugriff und Nutzung von Webarchiven erlauben könnte.";"Kurzfassung In den letzten Jahren haben Bibliotheken und Archive zunehmend die Aufgabe übernommen, neben konventionellen Publikationen auch Inhalte aus dem World Wide Web zu sammeln, um so diesen wertvollen Teil unseres kulturellen Erbes zu bewahren und wichtige Informationen langfristig verfügbar zu halten. Diese massiven Datensammlungen bieten faszinierende Möglichkeiten, rasch Zugriff auf wichtige Informationen zu bekommen, die im Live-Web bereits verloren gegangen sind. Sie sind eine unentbehrliche Quelle für Wissenschafter, die in der Zukunft die gesellschaftliche und technologische Entwicklung unserer Zeit nachvollziehen wollen. Auf der anderen Seite stellt eine derartige Datensammlung aber einen völlig neuen Datenbestand dar, der nicht nur rechtliche, sondern auch zahlreiche ethische Fragen betreffend seine Nutzung aufwirft. Diese werden in dem Ausmaß zunehmen, in dem die technischen Möglichkeiten zur automatischen Analyse und Interpretation dieser Daten leistungsfähiger werden. Da sich die meisten Web-Archivierungsinitiativen dieser Problematik bewusst sind, bleibt die Nutzung der Daten derzeit meist stark eingeschränkt, oder es wird eine Art von ""Opt-Out""-Möglichkeit vorgesehen, wodurch Webseiteninhaber die Aufnahme ihrer Seiten in ein Webarchiv ausschließen können. Mit beiden Ansätzen können Webarchive ihr volles Nutzungspotential nicht ausschöpfen. Das World Wide Web hat sich zu einem integralen Bestandteil unserer Publikations-und Kommunikationskultur entwickelt. Als solches bietet es uns einen reichhaltigen Schatz an wertvollen Informationen, die teilweise ausschließlich in elektronischer Form verfügbar sind, wie z.B. Informationsportale, Informationen zu zahlreichen Projekten und Bürgerinitiativen, Diskussionsforen, soziale Netze und Ähnliches. Weiters beeinflussen die technischen Möglichkeiten sowohl die Art der Gestaltung von Webseiten, als auch die Art, wie wir mit Information umgehen, wie unsere Gesellschaft vernetzt ist, wie sich Information ausbreitet bzw. wie sie genutzt wird. All dies stellt einen immens wertvollen Datenbestand dar, dessen Bedeutung uns teilweise erst bewusst werden mag, wenn dieser nicht mehr verfügbar ist. Dieser Artikel beschreibt einleitend kurz die Technologien, die zur Sammlung von Webinhalten zu Archivierungszwecken verwendet werden. Er hinterfragt Annahmen betreffend die freie Verfügbarkeit der Daten und unterschiedliche Nutzungsarten. Darauf aufbauend identifiziert er eine Reihe von offenen Fragen, deren Lösung einen breiteren Zugriff und Nutzung von Webarchiven erlauben könnte. Die fehlende langfristige Verfügbarkeit ist eine der entscheidenden Schwachstellen des World Wide Web. Unterschiedlichen Studien zufolge beträgt die durchschnittliche Lebensdauer eine Webressource zwischen wenigen Tagen und Monaten. So können schon binnen kürzester Zeit wertvolle Informationen nicht mehr über eine angegebene URL bezogen werden, bzw. stehen Forschern in naher und ferner Zukunft de-fakto keine Materialien zur Verfügung, um diese unsere Kommunikationskultur zu analysieren. Selbst Firmen haben zunehmend Probleme, Informationen über ihre eigenen Projekte, die vielfach nicht über zentrale Dokumentmanagementsysteme sondern webbasiert und kollaborativ in wikiartigen Systemen verwaltet werden, verfügbar zu halten.";"0,00";"0,00";"0,00"
"TUW-182899";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\groundtruth\TUW-182899.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\result\result-TUW-182899-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\extracted\grobid\grobid-TUW-182899-xstream.xml"")";"As photographic technologies continue to develop, so too do the social practices surrounding their use. The focus of this paper is on the social practices surrounding images captured from a new photographic device – SenseCam – which, rather than capturing individual images when triggered by the user, automatically captures a series of images. This paper is concerned with the use of SenseCam digital images in social contexts where there is a professional purpose: supporting the collaborative reflective practices of school teachers and university tutors as part of their professional development. Analysis of video data collected from 16 in-situ case studies of reflective discussions show evidence that reflection took place as defined in the literature. Further, the phototalk around SenseCam images was found to benefit reflection in these social situations through promoting a rich shared understanding of the lesson context: supporting return to the experience, sharing of background context, grounding conversations, illustrating and providing evidence, and allowing people to see more. The paper concludes with a discussion on how different features of SenseCam images, such as variable quality, lack of audio, incompleteness, helped in this reflection or not. Finally implications from this work and participants comments are used to suggest ways in which SenseCam may be used in the future in teacher and tutors social reflection.";"As photographic technologies continue to develop, so too do the social practices surrounding their use. The focus of this paper is on the social practices surrounding images captured from a new photographic device-SenseCam-which, rather than capturing individual images when triggered by the user, automatically captures a series of images. This paper is concerned with the use of SenseCam digital images in social contexts where there is a professional purpose: supporting the collaborative reflective practices of school teachers and university tutors as part of their professional development. Analysis of video data collected from 16 in-situ case studies of reflective discussions show evidence that reflection took place as defined in the literature. Further, the phototalk around SenseCam images was found to benefit reflection in these social situations through promoting a rich shared understanding of the lesson context: supporting return to the experience, sharing of background context, grounding conversations, illustrating and providing evidence, and allowing people to see more. The paper concludes with a discussion on how different features of SenseCam images, such as variable quality, lack of audio, incompleteness, helped in this reflection or not. Finally implications from this work and participants comments are used to suggest ways in which SenseCam may be used in the future in teacher and tutors social reflection.";"100,00";"100,00";"100,00"
"TUW-185321";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\groundtruth\TUW-185321.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\result\result-TUW-185321-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\extracted\grobid\grobid-TUW-185321-xstream.xml"")";"The semantic heterogeneity of Open Source Software (OSS) development projects comes from the using of different tools and models by the various stakeholders. These differences make the process of integration become difficult, since the project managers should recognize the different structure of the tools and models for analyzing the state of the projects. This manual analysis is costly and error prone. In this work we propose a semantic web technology approach to bridge these semantic heterogeneities, by using engineering knowledge base (EKB). The EKB enables mapping between local and domain ontology layers to allow querying the local tool knowledge using the domain-level knowledge and syntax. We empirically evaluate the feasibility of an EKB-based project monitoring system based on real-world data.";"The semantic heterogeneity of Open Source Software (OSS) development projects comes from the using of different tools and models by the various stakeholders. These differences make the process of integration become difficult, since the project managers should recognize the different structure of the tools and models for analyzing the state of the projects. This manual analysis is costly and error prone. In this work we propose a semantic web technology approach to bridge these semantic heterogeneities, by using engineering knowledge base (EKB). The EKB enables mapping between local and domain ontology layers to allow querying the local tool knowledge using the domain-level knowledge and syntax. We empirically evaluate the feasibility of an EKB-based project monitoring system based on real-world data. to model the project/domain knowledge that are used and understood by all stakeholders and tools. Concepts in this model are then mapped to local tool data models, which model common domain concepts in different ways. Based on this mapping queries to the domain knowledge can be resolved via the mappings by local tool data queries. We will check the feasibility of querying the on-tologies in EKB written in SPARQL 1 format with the case from an open source project. The major result is the easier definition of queries on project data originating from heterogeneous background.";"0,00";"0,00";"0,00"
"TUW-185441";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\groundtruth\TUW-185441.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\result\result-TUW-185441-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\extracted\grobid\grobid-TUW-185441-xstream.xml"")";"Shape analysis is a static program analysis technique for discovering properties of heap-allocated data structures. It is crucial to finding software bugs or to verify high-level correctness properties. Various analyses have been introduced but their relation in terms of precision often remains unclear as different analyses use different abstractions of the heap. The aim of our work is to compare the precision of shape analyses. We propose a novel algorithm based on three-valued logic that extracts alias sets from shape graphs. Smaller sets are more precise and indicate a more precise underlying shape analysis. Using this metric, we experimentally compare – for the first time – the relative quality of the state-of-the-art graph-based shape analyses and make recommendations concerning the combination of analysis parameters.";"Shape analysis is a static program analysis technique for discovering properties of heap-allocated data structures. It is crucial to finding software bugs or to verify high-level correctness properties. Various analyses have been introduced but their relation in terms of precision often remains unclear as different analyses use different abstractions of the heap. The aim of our work is to compare the precision of shape analyses. We propose a novel algorithm based on three-valued logic that extracts alias sets from shape graphs. Smaller sets are more precise and indicate a more precise underlying shape analysis. Using this metric, we experimentally compare-for the first time-the relative quality of the state-of-the-art graph-based shape analyses and make recommendations concerning the combination of analysis parameters. II. CONTRIBUTIONS We implemented multiple instances of state-of-the-art graph-based shape analyses for a subset of the C++ programming language and developed an algorithm based on three-valued logic that extracts alias information from shape graphs. Using this algorithm as a metric indicating the precision of the underlying shape analysis we are able to judge the effects of individual analysis parameters on runtime and precision of the analyses. In detail, the contributions address theory, practice, and assessment of shape analyses: A. THEORY";"0,00";"0,00";"0,00"
"TUW-186227";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\groundtruth\TUW-186227.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\result\result-TUW-186227-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\extracted\grobid\grobid-TUW-186227-xstream.xml"")";"One indispensable precondition for designing a functional software product for the modeling and execution of a computerized clinical practice guideline (CPG) is the comprehensive investigation of the different user groups involved and the issues they encounter. This led us to conduct a comprehensive literature study about the tasks involved in modeling a CPG into a formal representation as well as about the information needs of caregivers, i.e., physicians and nurses, and last but not least the information needs of patients. We have assessed and categorized the above mentioned information in order to create a reliable starting point for the development of a functional software tool.";"One indispensable precondition for designing a functional software product for the modeling and execution of a computerized clinical practice guideline (CPG) is the comprehensive investigation of the different user groups involved and the issues they encounter. This led us to conduct a comprehensive literature study about the tasks involved in modeling a CPG into a formal representation as well as about the information needs of caregivers, i.e., physicians and nurses, and last but not least the information needs of patients. We have assessed and categorized the above mentioned information in order to create a reliable starting point for the development of a functional software tool. evant guideline knowledge and to disambiguate this knowledge. For model-centric formalization the modeler formulates a conceptual model of a guideline without a direct relationship between the original text and the model. This modeling process involves steps like generating detailed data models of clinical concepts and fundamental parameters, specifying a logical and process structure of the CPG, and modeling the guideline knowledge by means of a flowchart-like graph.";"0,00";"0,00";"0,00"
"TUW-189842";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\groundtruth\TUW-189842.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\result\result-TUW-189842-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\extracted\grobid\grobid-TUW-189842-xstream.xml"")";"New technologies open up possibilities for designing interactive experiences that can engage and motivate post-stroke survivors to undertake what would otherwise be boring repetitive movements. In this paper we outline a few of the challenges we met as part of the cross-disciplinary Motivating Mobility project. These are: the extended 'user'; autonomy and motivation; and early prototype studies.";"Geraldine Fitzpatrick Anna Wilkinson, Sue Mawson New technologies open up possibilities for designing interactive experiences that can engage and motivate post-stroke survivors to undertake what would otherwise be boring repetitive movements. In this paper we outline a few of the challenges we met as part of the cross-disciplinary Motivating Mobility project. These are: the extended 'user'; autonomy and motivation; and early prototype studies.";"100,00";"100,00";"100,00"
"TUW-191715";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\groundtruth\TUW-191715.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\result\result-TUW-191715-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\extracted\grobid\grobid-TUW-191715-xstream.xml"")";"In this paper we give an overview of the current research trends and explore the challenges in several subfields of the scientific discipline of computer graphics: interactive and photorealistic rendering, scientific and information visualization, and visual analytics. Five challenges are extracted that play a role in each of these areas: scalability, semantics, fusion, interaction, acquisition. Of course, not all of these issues are disjunct to each other, however the chosen structure allows for a easy to follow overview of the concrete future challenges.";"In this paper we give an overview of the current research trends and explore the challenges in several subfields of the scientific discipline of computer graphics: interactive and photorealistic rendering, scientific and information visualization, and visual analytics. Five challenges are extracted that play a role in each of these areas: scalability, semantics, fusion, interaction, acquisition. Of course, not all of these issues are disjunct to each other, however the chosen structure allows for a easy to follow overview of the concrete future challenges.";"100,00";"100,00";"100,00"
"TUW-191977";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\groundtruth\TUW-191977.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\result\result-TUW-191977-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\extracted\grobid\grobid-TUW-191977-xstream.xml"")";"Digital Library (DL) interoperability requires addressing a variety of issues associated with functionality. We report on the analysis and solutions identified by the Functionality Working Group of the DL.org project during its deliberations on DL interoperability. Ultimately, we hope that work based on our perspective will lead to improved architectures and software, as well as to greater interoperability, for next-generation DL systems.";"Digital Library (DL) interoperability requires addressing a variety of issues associated with functionality. We report on the analysis and solutions identified by the Functionality Working Group of the DL.org project during its deliberations on DL interoperability. Ultimately, we hope that work based on our perspective will lead to improved architectures and software, as well as to greater interoperability, for next-generation DL systems. 1 Introduction A huge volume of information and knowledge is acquired and managed by distinct Digital Libraries (DLs). This leads to problems for academic and public libraries that often work with scores of such DLs and seek to support patrons facing a broad range of systems and services. Similar problems are faced by students, faculty, researchers, scholars, knowledge workers, and the general public. Also of concern is e-science, where labs and centers must use different DLs to address global challenges. Interoperability among all the DLs needed in each case is a serious concern. Manifesting a broad range of features and capabilities, DL systems employ diverse proprietary solutions and varying applications of a broad range of standards. The problem is further aggravated by the complexity and scale of modern DL systems and problems such as API mismatch, data format mismatch, and missing components. Interoperability has been the main issue of concern for the DL.org project [4]. Its work is based on the DELOS Digital Library Reference Model [3], in particular, the multi-dimensional representation of the DL domain and the identification of six primary concepts that characterize Digital Libraries: content, users, functionality, policy, quality, and architecture. In this paper, we present results from the discussions";"0,00";"0,00";"0,00"
"TUW-192724";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\groundtruth\TUW-192724.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\result\result-TUW-192724-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\extracted\grobid\grobid-TUW-192724-xstream.xml"")";"Human resource strategy can emerge within a decentralized decision structure that gives managers autonomy to take responsive actions while overall strategic direction is considered within a strategic planning process. This study defines the concept of employee selection (especially strategic employee selection) and hypothesizes on the positive correlation between innovation characteristic in SME and value of strategic employee (the so called personnel usefulness function). An empirical study illustrates the importance of both elements in an integrative human resource strategy formation process particularly for firms operating in the international environments.";"Human resource strategy can emerge within a decentralized decision structure that gives managers autonomy to take responsive actions while overall strategic direction is considered within a strategic planning process. This study defines the concept of employee selection (especially strategic employee selection) and hypothesizes on the positive correlation between innovation characteristic in SME and value of strategic employee (the so called personnel usefulness function). An empirical study illustrates the importance of both elements in an integrative human resource strategy formation process particularly for firms operating in the international environments.";"100,00";"100,00";"100,00"
"TUW-194085";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\groundtruth\TUW-194085.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\result\result-TUW-194085-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\extracted\grobid\grobid-TUW-194085-xstream.xml"")";"Topology is the general mathematical theory of convergence. Distributed computing is the formal investigation of communicating concurrent processes. We explore applications of topology to distributed computing in two directions: (1) Point-set topology and (2) algebraic topology.
We use the former to study the topological structure of infinite execution trees. This enables us to unify a number of impossibility proofs, in particular, the impossibility of distributed consensus — the task of all processes in a system agreeing on a single value — in various (close to) asynchronous systems with crash failures.
The latter is used to look into the combinatorial structure of configurations, i.e., the collection of current process states in the system. Configurations are regarded as simplices in a simplicial complex, and topological incompatibility of such complexes is utilized to prove the impossibility of a generalization of distributed consensus in certain systems. The particular problem considered is k-set agreement, which is the task of letting all processes agree to values within a set of at most k elements.";"A-1040 Wien Karlsplatz 13 Tel. +43/(0)1/58801-0 http://www.tuwien.ac.at Erklärung Thomas Nowak Rechte Wienzeile 73/23 1050 Wien Hiermit erkläre ich, dass ich diese Arbeit selbstständig verfasst habe, dass ich die verwendeten Quellen und Hilfsmittel vollständig angegeben habe und dass ich die Stellen der Arbeit-einschließlich Tabellen, Karten und Abbildungen-, die anderen Werken oder dem Internet im Wortlaut oder dem Sinn nach entnommen sind, auf jeden Fall unter Angabe der Quelle als Entlehnung kenntlich gemacht habe. Abstract Topology is the general mathematical theory of convergence. Distributed computing is the formal investigation of communicating concurrent processes. We explore applications of topology to distributed computing in two directions: (1) Point-set topology and (2) algebraic topology. We use the former to study the topological structure of infinite execution trees. This enables us to unify a number of impossibility proofs, in particular, the impossibility of distributed consensus-the task of all processes in a system agreeing on a single value-in various (close to) asynchronous systems with crash failures. The latter is used to look into the combinatorial structure of configurations, i.e., the collection of current process states in the system. Configurations are regarded as simplices in a simplicial complex, and topological incompatibility of such complexes is utilized to prove the impossibility of a generalization of distributed consensus in certain systems. The particular problem considered is k-set agreement, which is the task of letting all processes agree to values within a set of at most k elements. Kurzfassung Topologie ist die mathematisch adäquate Art, umüberum¨umüber Konvergenz zu sprechen. Distributed Computing ist das formale Studium von verteilten Systemen. Die Arbeit beschäftigt sich mit zwei Anwendungen der Topologie im Bereich des Distributed Computing: (1) Mengentheoretische Topologie und (2) algebraische Topologie. Erstere wird verwendet, um die topologische Struktur von unendlichen Bäumen, die die InformationüberInformation¨Informationüber mögliche Ausführungen der Algorithmen sub-sumieren, zu untersuchen. Dieses Wissen wird verwendet, um einen einheitlichen Beweis der Unmöglichkeit von Distributed Consensus in mehreren Systemmo-dellen zu geben. Consensus ist das Einigen aller Prozesse des Systems auf einen einzigen Wert. Zweitere wird verwendet, um die kombinatorische Struktur von Konfiguratio-nen, also der Zusammenfassung aller lokaler Zustände der Prozesse, zu untersu-chen. Hierbei wird eine Konfiguration als Simplex in einem Simplizialkomplex aufgefasst. Die topologische Unvereinbarkeit solcher Komplexe ermöglicht einen Beweis der Unmöglichkeit von k-Set Agreement in gewissen Systemen. Das ist eine Verallgemeinerung des Consensus-Problems: Es wird nicht mehr verlangt, dass sich die Prozesse auf nur einen Wert einigen, sondern es wird erlaubt, dass bis zu k unterschiedliche Werte auftreten.";"0,00";"0,00";"0,00"
"TUW-194561";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\groundtruth\TUW-194561.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\result\result-TUW-194561-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\extracted\grobid\grobid-TUW-194561-xstream.xml"")";"We propose a generalized version of context-sensitivity in term rewriting based on the notion of ""forbidden patterns"". The basic idea is that a rewrite step should be forbidden if the redex to be contracted has a certain shape and appears in a certain context. This shape and context is expressed through forbidden patterns. In particular we analyze the relationships among this novel approach and the commonly used notion of context-sensitivity in term rewriting, as well as the feasibility of rewriting with forbidden patterns from a computational point of view. The latter feasibility is characterized by demanding that restricting a rewrite relation yields an improved termination behaviour while still being powerful enough to compute meaningful results. Sufficient criteria for both kinds of properties in certain classes of rewrite systems with forbidden patterns are presented.";"We propose a generalized version of context-sensitivity in term rewriting based on the notion of ""forbidden patterns"". The basic idea is that a rewrite step should be forbidden if the redex to be contracted has a certain shape and appears in a certain context. This shape and context is expressed through forbidden patterns. In particular we analyze the relationships among this novel approach and the commonly used notion of context-sensitivity in term rewriting, as well as the feasibility of rewriting with forbidden patterns from a computational point of view. The latter feasibility is characterized by demanding that restricting a rewrite relation yields an improved termination behaviour while still being powerful enough to compute meaningful results. Sufficient criteria for both kinds of properties in certain classes of rewrite systems with forbidden patterns are presented.";"100,00";"100,00";"100,00"
"TUW-194660";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\groundtruth\TUW-194660.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\result\result-TUW-194660-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\extracted\grobid\grobid-TUW-194660-xstream.xml"")";"Interaction Nets are a novel model of computation based on graph rewriting. Their main properties are parallel evaluation and sharing of computation, which leads to efficient programs. However, Interaction Nets lack several features that allow for their convenient use as a programming language. In this paper , we describe the implementation of an extension for pattern matching of interaction rules. Furthermore, we show the cor-rectness of the implementation and discuss its complexity.";"Interaction Nets are a novel model of computation based on graph rewriting. Their main properties are parallel evaluation and sharing of computation, which leads to efficient programs. However, Interaction Nets lack several features that allow for their convenient use as a programming language. In this paper , we describe the implementation of an extension for pattern matching of interaction rules. Furthermore, we show the cor-rectness of the implementation and discuss its complexity. rules model the addition of natural numbers (encoded by 0 and a successor function S): T +";"100,00";"100,00";"100,00"
"TUW-197422";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\groundtruth\TUW-197422.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\result\result-TUW-197422-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\extracted\grobid\grobid-TUW-197422-xstream.xml"")";"Visual Analytics strongly emphasizes the importance of interaction. However, until now, interaction is only sparingly treated as subject matter on its own. How and why interactivity is beneficial to gain insight and make decisions is mostly left in the dark. Due to this lack of initial direction, it seems important to make further attempts in facilitating a deeper understanding of the concept of interactivity. Therefore, different perspectives towards interactivity are discussed and cognitive theories and models are investigated. The main aim of this paper is to broaden the view on interaction and spark further discussion towards a sound theoretical grounding for the field.";"Visual Analytics strongly emphasizes the importance of interaction. However, until now, interaction is only sparingly treated as subject matter on its own. How and why interactivity is beneficial to gain insight and make decisions is mostly left in the dark. Due to this lack of initial direction, it seems important to make further attempts in facilitating a deeper understanding of the concept of interactivity. Therefore, different perspectives towards interactivity are discussed and cognitive theories and models are investigated. The main aim of this paper is to broaden the view on interaction and spark further discussion towards a sound theoretical grounding for the field.";"100,00";"100,00";"100,00"
"TUW-197852";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\groundtruth\TUW-197852.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\result\result-TUW-197852-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\extracted\grobid\grobid-TUW-197852-xstream.xml"")";"To describe the structure of a system, the UML Class Diagram yields the means-of-choice. Therefor, the Class Diagram provides concepts like class, attribute, operation, association, generalization, aggregation, enumeration, etc. When students are introduced to this diagram, they often have to solve exercises where texts in natural language are given and they have to model the described systems. When analyzing such exercises, it becomes evident that certain kinds of phrases describing a particular concept appear again and again contextualized to the described domain.
In this paper, we present an approach which allows the automatic generation of tex-tual specifications from a given Class Diagram based on standard phrases in natural language. Besides supporting teachers in preparing exercises, such an approach is also valuable for various e-learning scenarios.";"To describe the structure of a system, the UML Class Diagram yields the means-of-choice. Therefor, the Class Diagram provides concepts like class, attribute , operation, association, generalization, aggregation, enumeration, etc. When students are introduced to this diagram, they often have to solve exercises where texts in natural language are given and they have to model the described systems. When analyzing such exercises, it becomes evident that certain kinds of phrases describing a particular concept appear again and again contextualized to the described domain. In this paper, we present an approach which allows the automatic generation of tex-tual specifications from a given Class Diagram based on standard phrases in natural language. Besides supporting teachers in preparing exercises, such an approach is also valuable for various e-learning scenarios.";"100,00";"100,00";"100,00"
"TUW-198400";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\groundtruth\TUW-198400.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\result\result-TUW-198400-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\extracted\grobid\grobid-TUW-198400-xstream.xml"")";"Although our society is critically dependent on software systems, these systems are mainly secured by protection mechanisms during operation instead of considering security issues during software design. Deficiencies in software design are the main reasons for security incidents, resulting in severe economic consequences for (i) the organizations using the software and (ii) the development companies. Lately, model-driven development has been proposed in order to increase the quality and thereby the security of software systems. This paper evaluates current efforts that position security as a fundamental element in model-driven development, highlights their deficiencies and identifies current research challenges. The evaluation shows that applying special-purpose methods to particular aspects of the problem is more suitable than applying generic ones, since (i) the problem can be represented on the proper abstraction level, (ii) the user can build on the knowledge of experts, and (iii) the available tools are more efficient and powerful.";"Although our society is critically dependent on software systems, these systems are mainly secured by protection mechanisms during operation instead of considering security issues during software design. Deficiencies in software design are the main reasons for security incidents, resulting in severe economic consequences for (i) the organizations using the software and (ii) the development companies. Lately, model-driven development has been proposed in order to increase the quality and thereby the security of software systems. This paper evaluates current efforts that position security as a fundamental element in model-driven development, highlights their deficiencies and identifies current research challenges. The evaluation shows that applying special-purpose methods to particular aspects of the problem is more suitable than applying generic ones, since (i) the problem can be represented on the proper abstraction level, (ii) the user can build on the knowledge of experts, and (iii) the available tools are more efficient and powerful.";"100,00";"100,00";"100,00"
"TUW-198401";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\groundtruth\TUW-198401.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\result\result-TUW-198401-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\extracted\grobid\grobid-TUW-198401-xstream.xml"")";"IT security incidents pose a major threat to the efficient execution of corporate strategies and business processes. Although companies generally spend a lot of money on security companies are often not aware of their spending on security and even more important if these investments into security are effective. This paper provides decision makers with an overview of decision support techniques, describes pros and cons of these methodologies.";"IT security incidents pose a major threat to the efficient execution of corporate strategies and business processes. Although companies generally spend a lot of money on security companies are often not aware of their spending on security and even more important if these investments into security are effective. This paper provides decision makers with an overview of decision support techniques, describes pros and cons of these methodologies.";"100,00";"100,00";"100,00"
"TUW-198405";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\groundtruth\TUW-198405.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\result\result-TUW-198405-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\extracted\grobid\grobid-TUW-198405-xstream.xml"")";"As business processes gain more importance in todays business environment, their unimpeded execution is crucial for a company's success. Corporate decision makers are faced with a wide spectrum of potential risks on the one hand and a plenitude of security safeguards on the other hand. This paper gives an overview of a new approach for the elicitation of security requirements of business processes, for the analysis of threats and vulnerabilities and for the interactive selection of an optimal security level according to the given business processes as well as multiple objectives. It provides decision makers with an instrument for interactively defining Secure Business Processes that are economically and technically efficient.";"As business processes gain more importance in todays business environment, their unimpeded execution is crucial for a company's success. Corporate decision makers are faced with a wide spectrum of potential risks on the one hand and a plenitude of security safeguards on the other hand. This paper gives an overview of a new approach for the elicitation of security requirements of business processes, for the analysis of threats and vulnerabilities and for the interactive selection of an optimal security level according to the given business processes as well as multiple objectives. It provides decision makers with an instrument for interactively defining Secure Business Processes that are economically and technically efficient.";"100,00";"100,00";"100,00"
"TUW-198408";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\groundtruth\TUW-198408.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\result\result-TUW-198408-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\extracted\grobid\grobid-TUW-198408-xstream.xml"")";"In der betrieblichen Praxis kommt der komponentenbasierten Software-Entwicklung hoher Stellenwert zu. Angesichts mehrfacher Zielsetzungen und vielfältiger Nebenbedingungen ist dabei insbesondere die Auswahl der ""besten"" Kombination von Komponenten ein nicht-triviales Entscheidungsproblem. Bislang wurden hierfür vor allem die Nutzwertanalyse bzw. der Analytic Hierarchy Process zur Entscheidungsunterstützung vorgeschlagen, wobei aber beide eine Reihe von Unzulänglichkeiten aufweisen. Diese Arbeit will dazu nunmehr eine Alternative anbieten. Darin werden in einem ersten Schritt zunächst (zulässige) Pareto-effiziente Kombinationen von Software-Komponenten berechnet und die Entscheidungsträger dann im zweiten Schritt interaktiv bei der Suche nach jener Variante unterstützt, die einen Ziele-Mix in Aussicht stellt, der den jeweiligen individuellen Präferenzen am besten entspricht. Das neue Verfahren zeichnet sich im Vergleich zu herkömmlichen Ansätzen insbesondere durch den Verzicht auf umfangreiche a priori Präferenzinformationen (wie z.B. Zielgewichtungen) aus. Darüber hinaus kann es ohne großen Anpassungsaufwand in bestehende Vorgehensmodelle zur Auswahl von Software-Komponenten integriert werden.";"In der betrieblichen Praxis kommt der komponentenbasierten Software-Entwicklung hoher Stel-lenwert zu. Angesichts mehrfacher Zielsetzungen und vielfältiger Nebenbedingungen ist dabei insbesondere die Auswahl der ""besten"" Kombination von Komponenten ein nicht-triviales Ent-scheidungsproblem. Bislang wurden hierfür vor allem die Nutzwertanalyse bzw. der Analytic Hierarchy Process zur Entscheidungsunterstützung vorgeschlagen, wobei aber beide eine Reihe von Unzulänglichkeiten aufweisen. Diese Arbeit will dazu nunmehr eine Alternative anbieten. Darin werden in einem ersten Schritt zunächst (zulässige) Pareto-effiziente Kombinationen von Software-Komponenten berechnet und die Entscheidungsträger dann im zweiten Schritt inter-aktiv bei der Suche nach jener Variante unterstützt, die einen Ziele-Mix in Aussicht stellt, der den jeweiligen individuellen Präferenzen am besten entspricht. Das neue Verfahren zeichnet sich im Vergleich zu herkömmlichen Ansätzen insbesondere durch den Verzicht auf umfang-reiche a priori Präferenzinformationen (wie z.B. Zielgewichtungen) aus. Darüber hinaus kann es ohne großen Anpassungsaufwand in bestehende Vorgehensmodelle zur Auswahl von Software-Komponenten integriert werden.";"100,00";"100,00";"100,00"
"TUW-200745";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\groundtruth\TUW-200745.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\result\result-TUW-200745-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\extracted\grobid\grobid-TUW-200745-xstream.xml"")";"In this work, an algorithm for the generalized minimum spanning tree problem (GMST) is developed. Given is a complete graph where the nodes are partitioned into clusters. A solution is a spanning tree which contains exactly one node of each cluster and its costs are minimal. This problem is NP-hard. In this work, a heuristic is developed for this problem.
	
In this method, an evolutionary algorithm (EA) is used with two different solution archives. Using a solution archive, it is possible to store solutions generated by the EA in order to detect duplicates and converts duplicate solutions into new solutions. One solution archive based on an encoding in which the spanned nodes of each cluster in the solution are stored. The other archive is based on an encoding which characterizes the connections between the clusters.

These archives are extended by a bounding strategy based on the branch-and-bound technique. They try to calculate appropriate bounds at a convenient positions which give information about how good the solutions in the respective area of the archive can be in the best case. If a bound was found which is worse than the best known solution, the solutions are unattractive in the course of the algorithm and will not be considered. Therefore inferior solutions can be detected at an early stage and only promising solutions that can bring improvements will be pursued.

In addition to the bounding strategy a nearest neighbor approach is implemented in which a cluster attached to the spanning tree is preferred among the the n nearest neighboring clusters.

Tests were carried out in which the bounding strategy was used in the different variants. These tests led to the conclusion that the bounding strategy leads to an improvement in comparison to the ""normal"" archives. The comparison between the archives shows that the pop version lead to better results than the gosh version. When both archives are used simultaneously, the results are better than the results of the other two variants.";"A-1040 Wien Karlsplatz 13 Tel. +43-1-58801-0 www.tuwien.ac.at Erklärung Christian Gruber Wachbergsraße 29 3382 Schollach Hiermit erkläre ich, dass ich diese Arbeit selbständig verfasst habe, dass ich die verwendeten Quellen und Hilfsmittel vollständig angegeben habe und dass ich die Stellen der Arbeit-einschließlich Tabellen, Karten und Abbildungen-, die an-deren Werken oder dem Internet im Wortlaut oder dem Sinn nach entnommen sind, auf jeden Fall unter Angabe der Quelle als Entlehnung kenntlich gemacht habe. ii Kurzfassung In dieser Arbeit wird ein Algorithmus für das Generalized Minimum Spanning Tree-Problem (GMST) entwickelt. Beim GMST-Problem ist ein vollständiger Graph gegeben, bei dem die Knoten in Cluster partitioniert sind. Als Lösung wird ein Spannbaum gesucht, der von jedem Cluster genau einen Knoten beinhaltet und dessen Kosten minimal sind. Dieses Problem ist NP-schwierig. In dieser Arbeit wird eine Heuristik für dieses Problem entwickelt. Bei diesem Verfahren wird ein Evolutionärer Algorithmus (EA) mit zwei ver-schiedenen Lösungsarchiven verwendet. Die Lösungsarchive werden dazu benutzt Lösungen zu speichern, um Duplikate zu erkennen und diese in neue Lösungen umzuwandeln. Das eine Lösungsarchiv beruht auf einer Kodierung, bei der die ausgewählten Knoten der Cluster einer Lösung gespeichert werden, während das andere Archiv auf einer Kodierung beruht, bei der gespeichert wird, welche Cluster in der Lösung verbunden sind. Diese Archive werden in dieser Arbeit durch eine Bounding-Strategie basierend auf dem Branch and Bound Verfahren erweitert. Dabei wird versucht im Archiv an günstigen Positionen geeignete Bounds zu berechnen, die Auskunft darüber geben, wie gut die Lösungen in diesem Bereich des Archivs höchstens sein können. Wird eine Bound gefunden, die schlechter als die beste gefunden Lösung ist, sind diese Lösungen im weiteren Verlauf des Algorithmus uninteressant und werden nicht mehr berücksichtigt. Das führt dazu, dass mehrere Lösungen von vornherein als schlecht erkannt werden können und somit nur Lösungen verfolgt werden, die auch Verbesserungen bringen können. Zusätzlich zu der Bounding-Strategie wird auch noch ein Nearest Neighbour Ansatz verwendet, bei dem beim Anhängen eines Clusters an den Spannbaum die n näch-sten Nachbarcluster bevorzugt werden. Am Ende der Arbeit wurden Tests durchgeführt, bei denen die Bounding Strate-gie in den unterschiedlichen Archiven verwendet wurde. Diese Tests führten zu dem Ergebnis, dass die Bounding Strategie zu einer Verbesserung gegenüber den Archiven ohne Bounding Strategie führt. Der Vergleich zwischen den Archiven hat ergeben, dass die Pop-Variante bessere Ergebnisse liefert als die Gosh-Variante. Die Variante, in der beide Archive gleichzeitig verwendet werden, ist wiederum besser als die anderen beiden Varianten. iii Abstract In this work, an algorithm for the generalized minimum spanning tree problem (GMST) is developed. Given is a complete graph where the nodes are partitioned into clusters. A solution is a spanning tree which contains exactly one node of each cluster and its costs are minimal. This problem is NP-hard. In this work, a heuristic is developed for this problem. In this method, an evolutionary algorithm (EA) is used with two different solution archives. Using a solution archive, it is possible to store solutions generated by the EA in order to detect duplicates and converts duplicate solutions into new solutions. One solution archive based on an encoding in which the spanned nodes of each cluster in the solution are stored. The other archive is based on an encoding which characterizes the connections between the clusters. These archives are extended by a bounding strategy based on the branch-and-bound technique. They try to calculate appropriate bounds at a convenient positions which give information about how good the solutions in the respective area of the archive can be in the best case. If a bound was found which is worse than the best known solution, the solutions are unattractive in the course of the algorithm and will not be considered. Therefore inferior solutions can be detected at an early stage and only promising solutions that can bring improvements will be pursued. In addition to the bounding strategy a nearest neighbor approach is implemented in which a cluster attached to the spanning tree is preferred among the the n nearest neighboring clusters. Tests were carried out in which the bounding strategy was used in the different variants. These tests led to the conclusion that the bounding strategy leads to an improvement in comparison to the ""normal"" archives. The comparison between the archives shows that the pop version lead to better results than the gosh version. When both archives are used simultaneously, the results are better than the results of the other two variants.";"0,00";"0,00";"0,00"
"TUW-200748";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\groundtruth\TUW-200748.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\result\result-TUW-200748-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\extracted\grobid\grobid-TUW-200748-xstream.xml"")";"The Rooted Delay-Constrained Steiner Tree Problem (RDCSTP) is a variant of the well-known Steiner Tree Problem on a graph in which the paths to all terminal nodes are restricted by a certain maximum delay. The problem mostly appears in the context of network routing for multicasts, i.e., sending packages from a fixed source to a subset of other participants in the network. Since the RDCSTP belongs to the class of N P-hard problems it is in general not possible to solve large instances exactly in a reasonable amount of time. Therefore, the focus mostly lies on developing good heuristics that can still solve large instances comparatively fast to near optimality.
In this thesis a Multilevel Refinement heuristic – which has already been successfully applied to other problems like the Graph Partitioning Problem – is implemented as an improvement heuristic for the RDCSTP. In the general approach of this metaheuristic the problem's complexity is first iteratively reduced while still maintaining its general characteristics. The problem is thereby simplified and can at the top level finally easily be solved. Then, the solution on this highest level is refined until a solution for the original problem is obtained.
The algorithm introduced here implements the Multilevel Refinement approach as an improvement heuristic, iteratively changing an existing solution. However, it is designed in a way that also allows it to be used to construct an initial solution. Another distinctiveness is that, due to the additional delay constraints, supplementary data structures have to be used to avoid creating invalid solutions on higher levels as much as possible. In the refinement phase an additional improvement algorithm, the Key Path Improvement, is executed on each level, drastically increasing result quality.
Experimental tests are carried out, evaluating the performance of the algorithm on large instances and comparing it to other algorithms in the literature. The obtained results are promising and indicate that the Multilevel Refinement metaheuristic is indeed a competitive approach for the RDCSTP.";"Hiermit erkläre ich, dass ich diese Arbeit selbstständig verfasst habe, dass ich die verwen-deten Quellen und Hilfsmittel vollständig angegeben habe und dass ich die Stellen der Arbeit-einschließlich Tabellen, Karten und Abbildungen-, die anderen Werken oder dem Internet im Wortlaut oder dem Sinn nach entnommen sind, auf jeden Fall unter Angabe der Quelle als Entlehnung kenntlich gemacht habe. i Acknowledgements I would like to thank my advisor, Prof. Dr. Günther Raidl, for letting me work on this thesis and for his help and suggestions with creating it. I also thank the Vienna University of Technology for the years of education I received there, and for the prolific scientific environment it provided. My special thanks go to my mentor for this thesis, Dipl.-Ing. Mario Ruthmair. Without his countless suggestions, our numerous discussions, his help and his thorough reviews, this thesis would never have been completed. Lastly, I want to sincerely thank my parents, who supported me in every possible way throughout my education and without whom none of this would have been possible. iii Abstract The Rooted Delay-Constrained Steiner Tree Problem (RDCSTP) is a variant of the well-known Steiner Tree Problem on a graph in which the paths to all terminal nodes are restricted by a certain maximum delay. The problem mostly appears in the context of network routing for multicasts, i.e., sending packages from a fixed source to a subset of other participants in the network. Since the RDCSTP belongs to the class of N P-hard problems it is in general not possible to solve large instances exactly in a reasonable amount of time. Therefore, the focus mostly lies on developing good heuristics that can still solve large instances comparatively fast to near optimality. In this thesis a Multilevel Refinement heuristic-which has already been successfully applied to other problems like the Graph Partitioning Problem-is implemented as an improvement heuristic for the RDCSTP. In the general approach of this metaheuristic the problem's complexity is first iteratively reduced while still maintaining its general characteristics. The problem is thereby simplified and can at the top level finally easily be solved. Then, the solution on this highest level is refined until a solution for the original problem is obtained. The algorithm introduced here implements the Multilevel Refinement approach as an improvement heuristic, iteratively changing an existing solution. However, it is designed in a way that also allows it to be used to construct an initial solution. Another distinctiveness is that, due to the additional delay constraints, supplementary data structures have to be used to avoid creating invalid solutions on higher levels as much as possible. In the refinement phase an additional improvement algorithm, the Key Path Improvement, is executed on each level, drastically increasing result quality. Experimental tests are carried out, evaluating the performance of the algorithm on large instances and comparing it to other algorithms in the literature. The obtained results are promising and indicate that the Multilevel Refinement metaheuristic is indeed a competitive approach for the RDCSTP. v Kurzfassung Das Rooted Delay-Constrained Steiner Tree Problem (RDCSTP) ist eine Variante des bekannten Steinerbaum-Problems auf einem Graphen in welcher die Pfade zu allen Zielknoten durch eine bestimmte maximale Verzögerung beschränkt sind. Das Problem tritt hauptsächlich im Bereich des Netzwerk-Routings beim Multicast auf, das heißt wenn Pakete von einer einzelnen Quelle zu einer bestimmten Untermenge der anderen Netzwerk-Teilnehmer gesendet werden sollen. Da das RDCSTP, wie das ursprüngliche Steiner-Problem, zur Klasse der N P-schwierigen Probleme gehört, ist es allgemein nicht möglich die exakte Lösung einer großen Probleminstanz in vertret-barer Zeit zu finden. Der Fokus der Forschung liegt daher großteils auf der Entwicklung guter Heuristiken, die auch bei großen Probleminstanzen in der Lage sind in vergleichbar kurzer Zeit zu möglichst guten Lösungen zu kommen. In dieser Arbeit wird hierfür die Multilevel-Refinement-Heuristik-die bereits erfolgreich auf etliche andere Probleme, wie das Graph Partitioning Problem, angewandt wurde-als Ver-besserungsheuristik für das RDCSTP entwickelt. Grundsätzlich werden bei dieser Metaheuristik in einem ersten Schritt Knoten sukzessive zusammengefasst um den Graphen auf höheren ""Lev-els"", mit weniger Knoten, darzustellen. Das so vereinfachte Problem kann dann auf der höchsten Abstraktionsebene in simpler Weise gelöst werden. Dann wird diese Lösung schrittweise wieder soweit verfeinert, bis eine Lösung für das ursprüngliche Problem erreicht wird. Der hier vorgestellte Algorithmus für das RDCSTP implementiert diesen Multilevel-Ansatz als Verbesserungsheuristik, die eine existierende Lösung iterativ verändert. Er wurde allerdings in einer Weise entworfen, die es ihm ebenso erlaubt eine Anfangslösung selbst zu generieren. Eine weitere Besonderheit ist, dass wegen der zusätzlichen Verzögerungs-Einschränkung wei-tere Datenstrukturen benötigt werden, um auf höheren Levels möglichst gültige Lösungen zu erzeugen. Außerdem wird während der Verfeinerung der Lösung auf jedem Level eine weite-re Verbesserungsheuristik angewandt, das Key Path Improvement, welches die Lösungsqualität drastisch verbessert. Umfangreiche experimentelle Tests wurden durchgeführt um die Leistungsfähigkeit des Al-gorithmus bei großen Instanzen zu messen, und ihn mit anderen Algorithmen aus der Literatur zu vergleichen. Die hierbei erhaltenen Ergebnisse sind durchwegs sehr positiv und weisen somit darauf hin, dass der verfolgte Multilevel-Ansatz tatsächlich eine konkurrenzfähige Heuristik für das RDCSTP darstellt. vii";"0,00";"0,00";"0,00"
"TUW-200948";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\groundtruth\TUW-200948.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\result\result-TUW-200948-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\extracted\grobid\grobid-TUW-200948-xstream.xml"")";"We introduce a new approach for establishing fixed-parameter tractability of problems parameterized above tight lower bounds or below tight upper bounds. To illustrate the approach we consider two problems of this type of unknown complexity that were introduced by Mahajan, Raman and Sikdar (J. Comput. Syst. Sci. 75, 2009). We show that a generalization of one of the problems and three nontrivial special cases of the other problem admit kernels of quadratic size.";"We introduce a new approach for establishing fixed-parameter tractability of problems parameterized above tight lower bounds or below tight upper bounds. To illustrate the approach we consider two problems of this type of unknown complexity that were introduced by Mahajan, Raman and Sikdar (J. Comput. Syst. Sci. 75, 2009). We show that a generalization of one of the problems and three nontrivial special cases of the other problem admit kernels of quadratic size.";"100,00";"100,00";"100,00"
"TUW-200950";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\groundtruth\TUW-200950.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\result\result-TUW-200950-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\extracted\grobid\grobid-TUW-200950-xstream.xml"")";"We study the complexity of several coloring problems on graphs, pa-rameterized by the treewidth t of the graph:
(1) The list chromatic number ?l(G) of a graph G is defined to be the smallest positive integer r, such that for every assignment to the vertices v of G, of a list Lv of colors, where each list has length at least r, there is a choice of one color from each vertex list Lv yielding a proper coloring of G. We show that the problem of determining whether ?l(G) ? r, the LIST CHROMATIC NUMBER problem, is solvable in linear time for every fixed treewidth bound t. The method by which this is shown is new and of general applicability.
(2) The LIST COLORING problem takes as input a graph G, together with an assignment to each vertex v of a set of colors Cv. The problem is to determine whether it is possible to choose a color for vertex v from the set of permitted colors Cv, for each vertex, so that the obtained coloring of G is proper. We show that this problem is W [1]-hard, parameterized by the treewidth of G. The closely related PRECOLORING EXTENSION problem is also shown to be W [1]-hard, pa-rameterized by treewidth.
(3) An equitable coloring of a graph G is a proper coloring of the vertices where the numbers of vertices having any two distinct colors differs by at most one. We show that the problem is hard for W [1], parameterized by (t, r). We also show that a list-based variation, LIST EQUITABLE COLORING is W [1]-hard for trees, parameterized by the number of colors on the lists.";"We study the complexity of several coloring problems on graphs, pa-rameterized by the treewidth t of the graph: (1) The list chromatic number ?l(G) of a graph G is defined to be the smallest positive integer r, such that for every assignment to the vertices v of G, of a list Lv of colors, where each list has length at least r, there is a choice of one color from each vertex list Lv yielding a proper coloring of G. We show that the problem of determining whether ?l(G) ? r, the LIST CHROMATIC NUMBER problem, is solvable in linear time for every fixed treewidth bound t. The method by which this is shown is new and of general applicability. (2) The LIST COLORING problem takes as input a graph G, together with an assignment to each vertex v of a set of colors Cv. The problem is to determine whether it is possible to choose a color for vertex v from the set of permitted colors Cv, for each vertex, so that the obtained coloring of G is proper. We show that this problem is W [1]-hard, parameterized by the treewidth of G. The closely related PRECOLORING EXTENSION problem is also shown to be W [1]-hard, pa-rameterized by treewidth. (3) An equitable coloring of a graph G is a proper coloring of the vertices where the numbers of vertices having any two distinct colors differs by at most one. We show that the problem is hard for W [1], parameterized by (t, r). We also show that a list-based variation, LIST EQUITABLE COLORING is W [1]-hard for trees, parameterized by the number of colors on the lists.";"100,00";"100,00";"100,00"
"TUW-200959";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\groundtruth\TUW-200959.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\result\result-TUW-200959-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\extracted\grobid\grobid-TUW-200959-xstream.xml"")";"We consider monotonicity problems for graph searching games. Variants of these games – defined by the type of moves allowed for the players – have been found to be closely connected to graph decompositions and associated width measures such as path-or tree-width.
Of particular interest is the question whether these games are monotone, i.e. whether the cops can catch a robber without ever allowing the robber to reach positions that have been cleared before. The monotonicity problem for graph searching games has intensely been studied in the literature, but for two types of games the problem was left unresolved. These are the games on digraphs where the robber is invisible and lazy or visible and fast. In this paper, we solve the problems by giving examples showing that both types of games are non-monotone.
Graph searching games on digraphs are closely related to recent proposals for digraph decom-positions generalising tree-width to directed graphs. These proposals have partly been motivated by attempts to develop a structure theory for digraphs similar to the graph minor theory developed by Robertson and Seymour for undirected graphs, and partly by the immense number of algorith-mic results using tree-width of undirected graphs and the hope that part of this success might be reproducible on digraphs using a ""directed tree-width"". For problems such as disjoint paths and Hamiltonicity, it has indeed been shown that they are tractable on graphs of small directed tree-width. However, the number of such examples is still small.
We therefore explore the limits of the algorithmic applicability of digraph decompositions. In particular, we show that various natural candidates for problems that might benefit from digraphs having small ""directed tree-width"" remain NP-complete even on almost acyclic graphs.";"We consider monotonicity problems for graph searching games. Variants of these games-defined by the type of moves allowed for the players-have been found to be closely connected to graph decompositions and associated width measures such as path-or tree-width. Of particular interest is the question whether these games are monotone, i.e. whether the cops can catch a robber without ever allowing the robber to reach positions that have been cleared before. The monotonicity problem for graph searching games has intensely been studied in the literature, but for two types of games the problem was left unresolved. These are the games on digraphs where the robber is invisible and lazy or visible and fast. In this paper, we solve the problems by giving examples showing that both types of games are non-monotone. Graph searching games on digraphs are closely related to recent proposals for digraph decom-positions generalising tree-width to directed graphs. These proposals have partly been motivated by attempts to develop a structure theory for digraphs similar to the graph minor theory developed by Robertson and Seymour for undirected graphs, and partly by the immense number of algorith-mic results using tree-width of undirected graphs and the hope that part of this success might be reproducible on digraphs using a ""directed tree-width"". For problems such as disjoint paths and Hamiltonicity, it has indeed been shown that they are tractable on graphs of small directed tree-width. However, the number of such examples is still small. We therefore explore the limits of the algorithmic applicability of digraph decompositions. In particular, we show that various natural candidates for problems that might benefit from digraphs having small ""directed tree-width"" remain NP-complete even on almost acyclic graphs.";"100,00";"100,00";"100,00"
"TUW-201066";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\groundtruth\TUW-201066.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\result\result-TUW-201066-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\extracted\grobid\grobid-TUW-201066-xstream.xml"")";;;"none extracted value";"none expected";"NA"
"TUW-201160";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\groundtruth\TUW-201160.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\result\result-TUW-201160-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\extracted\grobid\grobid-TUW-201160-xstream.xml"")";;;"none extracted value";"none expected";"NA"
"TUW-201167";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\groundtruth\TUW-201167.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\result\result-TUW-201167-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\extracted\grobid\grobid-TUW-201167-xstream.xml"")";;;"none extracted value";"none expected";"NA"
"TUW-201821";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\groundtruth\TUW-201821.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\result\result-TUW-201821-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\extracted\grobid\grobid-TUW-201821-xstream.xml"")";"Multi-Context Systems are an expressive formalism to model (possibly) non-monotonic information exchange between heterogeneous knowledge bases. Such information exchange, however, often comes with unforseen side-effects leading to violation of constraints, making the system inconsistent, and thus unusable. Although there are many approaches to assess and repair a single inconsistent knowledge base, the heterogeneous nature of Multi-Context Systems poses problems which have not yet been addressed in a satisfying way: How to identify and explain a inconsistency that spreads over multiple knowledge bases with different logical formalisms (e.g., logic programs and ontologies)? What are the causes of inconsistency if inference/information exchange is non-monotonic (e.g., absent information as cause)? How to deal with inconsistency if access to knowledge bases is restricted (e.g., companies exchange information, but do not allow arbitrary modifications to their knowledge bases)? Many traditional approaches solely aim for a consistent system, but automatic removal of inconsistency is not always desireable. Therefore a human operator has to be supported in finding the erroneous parts contributing to the inconsistency. In my thesis those issues will be adressed mainly from a foundational perspective, while our research project also provides algorithms and prototype implementations.";"Multi-Context Systems are an expressive formalism to model (possibly) non-monotonic information exchange between heterogeneous knowledge bases. Such information exchange, however, often comes with unforseen side-effects leading to violation of constraints, making the system inconsistent, and thus unusable. Although there are many approaches to assess and repair a single inconsistent knowledge base, the heterogeneous nature of Multi-Context Systems poses problems which have not yet been addressed in a satisfying way: How to identify and explain a inconsistency that spreads over multiple knowledge bases with different logical formalisms (e.g., logic programs and ontologies)? What are the causes of inconsistency if inference/information exchange is non-monotonic (e.g., absent information as cause)? How to deal with inconsistency if access to knowledge bases is restricted (e.g., companies exchange information, but do not allow arbitrary modifications to their knowledge bases)? Many traditional approaches solely aim for a consistent system, but automatic removal of inconsistency is not always desireable. Therefore a human operator has to be supported in finding the erroneous parts contributing to the inconsistency. In my thesis those issues will be adressed mainly from a foundational perspective, while our research project also provides algorithms and prototype implementations.";"100,00";"100,00";"100,00"
"TUW-202034";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\groundtruth\TUW-202034.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\result\result-TUW-202034-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\extracted\grobid\grobid-TUW-202034-xstream.xml"")";"In this thesis the application of clustering algorithms for solving the Hierarchical Ring Network Problem (HRNP) is investigated.
When the network is represented as a graph, an informal problem definition for this NP-complete problem is: Given a set of network sites (nodes) assigned to one of three layers and the costs for establishing connections between sites (i.e., edge costs) the objective is to find a minimum cost connected network under certain constraints that are explained in detail in the thesis. The most important constraint is that the nodes have to be assigned to rings of bounded size that connect the layers hierarchically.
The ring structure is a good compromise between the robustness of a network and the cost for establishing it. It is guaranteed, that the network can continue to provide its service if one network node per ring fails.
The basic idea in this thesis for solving this network design problem was to cluster the sites with hierarchical clustering heuristics and to use the resulting hierarchy as support for the ring-finding heuristics. Previous apporaches for related network design problems did not use the inherent network structure in such a way. Usual approaches are based on greedy heuristics.
Three clustering heuristics were implemented: Girvan-Newman, K-means and Kernighan-Lin. Especially the first algorithm is interesting, because it was successfully applied analyzing large network structures, also in the context of internet communities.
For finding rings three heuristics were implemented too. Strategic variation of the maximum allowed ring size helps the first heuristic to find rings using the cluster hierarchy. The second heuristic finds rings by searching for paths that are connected to previously found rings. Third a repair heuristic was implemented that tries to add remaining nodes to existing rings. Local search heuristics are applied last to improve the solution quality.
To check how the clustering approach performs for solving the problem of this thesis two test instance generators were implemented. One generates instances randomly and the second generates instances based on the popular TSPLIB archive.
The evaluation of the random test instances has shown, that all three clustering heuristics were able to solve those test instances, while Girvan-Newman and Kernighan-Lin found valid solutions in each test run this was not possible for K-means. When Kernighan-Lin was used as clustering algorithm solutions could be found faster on average, but the resulting costs where slightly higher. For the TSPLIB based instances the clustering algorithms had more problems to find valid solutions, but for each test instance at least one type of clustering was successful.";"Hiermit erkläre ich, dass ich diese Arbeit selbständig verfasst habe, dass ich die verwen-deten Quellen und Hilfsmittel vollständig angegeben habe und dass ich die Stellen der Arbeit-einschließlich Tabellen, Karten und Abbildungen-, die anderen Werken oder dem Internet im Wortlaut oder dem Sinn nach entnommen sind, auf jeden Fall unter Angabe der Quelle als Entlehnung kenntlich gemacht habe. (Ort, Datum) (Unterschrift Verfasser) i Acknowledgements I want to thank my advisors ao.Univ.-Prof. Dipl.-Ing. Dr. techn. Günther Raidl and Univ.-Ass. Dipl.-Ing. Christian Schauer. Their constructive feedback and their experience was a big help for writing and improving this thesis. Special thanks goes to my family and my friends. Studying can be quite time-consuming and stressful sometimes and their support is invaluable. ii Abstract In this thesis the application of clustering algorithms for solving the Hierarchical Ring Network Problem (HRNP) is investigated. When the network is represented as a graph, an informal problem definition for this NP-complete problem is: Given a set of network sites (nodes) assigned to one of three layers and the costs for establishing connections between sites (i.e., edge costs) the objective is to find a minimum cost connected network under certain constraints that are explained in detail in the thesis. The most important constraint is that the nodes have to be assigned to rings of bounded size that connect the layers hierarchically. The ring structure is a good compromise between the robustness of a network and the cost for establishing it. It is guaranteed, that the network can continue to provide its service if one network node per ring fails. The basic idea in this thesis for solving this network design problem was to cluster the sites with hierarchical clustering heuristics and to use the resulting hierarchy as support for the ring-finding heuristics. Previous apporaches for related network design problems did not use the inherent network structure in such a way. Usual approaches are based on greedy heuristics. Three clustering heuristics were implemented: Girvan-Newman, K-means and Kernighan-Lin. Especially the first algorithm is interesting, because it was successfully applied analyzing large network structures, also in the context of internet communities. For finding rings three heuristics were implemented too. Strategic variation of the maximum allowed ring size helps the first heuristic to find rings using the cluster hierarchy. The second heuristic finds rings by searching for paths that are connected to previously found rings. Third a repair heuristic was implemented that tries to add remaining nodes to existing rings. Local search heuristics are applied last to improve the solution quality. To check how the clustering approach performs for solving the problem of this thesis two test instance generators were implemented. One generates instances randomly and the second generates instances based on the popular TSPLIB archive. The evaluation of the random test instances has shown, that all three clustering heuristics were able to solve those test instances, while Girvan-Newman and Kernighan-Lin found valid solutions in each test run this was not possible for K-means. When Kernighan-Lin was used as clustering algorithm solutions could be found faster on average, but the resulting costs where slightly higher. For the TSPLIB based instances the clustering algorithms had more problems to find valid solutions, but for each test instance at least one type of clustering was successful. iii Kurzfassung In dieser Diplomarbeit wird die Anwendung von Clusteringalgorithmen untersucht, um das Hierarchical Ring Network Problem (HRNP) zu lösen. Wenn das Netzwerk als Graph repräsentiert ist, ist dieses NP-vollständige Problem wie folgt definiert: Gegeben ist Menge von Knoten welche jeweils einer von drei Schichten zugewiesen sind, und eine Kostenfunktion, welche die Verbindungskosten zwischen zwei Knoten (d.h. Kantenkosten) zuweist. Gesucht ist ein zusammenhängendes Netzwerk mit minimalen Gesamtkosten, wobei dieses bestimmte Struktureigenschaften zu erfüllen hat, welche im Detail in der Diplomarbeit beschrieben werden. Die wichtigste dieser Eigenschaften ist, dass Knoten gemäß einer hierarchischen Struktur zu größenbeschränkten Ringen verbunden werden. Ringstrukturen sind ein guter Kompromiss zwischen der Verfügbarkeit von Netzwerken und deren Herstellungskosten. Die Verfügbarkeit ist gewährleistet, solange maximal ein Knoten pro Ring ausfällt. Die grundlegende Idee dieser Diplomarbeit um dieses Netzwerkdesign-Problem zu lösen, ist die Knoten mit Hilfe von hierarchischen Clusteringalgorithmen anzuordnen und die resul-tierende Hierarchie für nachfolgende Heuristiken zu verwenden, welche die Ringe finden. Vorhergehende Ansätze für vergleichbare Netzwerkdesign-Probleme haben die inhärente Netzwerkstruktur nicht auf solche Weise genützt und eher Greedy-Heuristiken eingesetzt. Um gültige Ringe zu finden, wurden drei Heuristiken implementiert. Strategisches Variieren der erlaubten Ringgröße hilft der ersten Heuristik Ringe unter Benützung der Cluster-Hierarchie zu finden. Die zweite Heuristik baut auf den in der vorherigen Schicht gefundenen Ringen auf, indem sie nach gültigen Pfaden sucht, die an diese Ringe angeschlossen werden können. Drittens wird eine Reparaturheuristik angewendet, welche versucht verbleibende Knoten zu bestehenden Ringen zuzuweisen. Zuletzt werden lokale Suchverfahren eingesetzt, um die Gesamtkosten zu verbessern. Um zu überprüfen, wie gut dieser Lösungsansatz funktioniert, wurden zwei Testinstanz-Generatoren implementiert. Der Erste generiert Instanzen zufallsbasiert, der Zweite baut auf dem bekannten TSPLIB-Archiv auf. Die Evaluierung der zufallsbasierten Testinstanzen hat gezeigt, dass alle drei Heuristiken sämtliche Instanzen lösen konnten, wobei Girvan-Newman und Kernighan-Lin in jedem Testlauf Lösungen gefunden haben, war dies bei K-means nicht der Fall. Mit Kernighan-Lin konnte im Durchschnitt schneller eine Lösung gefunden werden, aber die Gesamtkosten waren bei den beiden anderen Algorithmen etwas besser. Mit den TSPLIB-basierten Testinstanzen konnte nicht mit allen Clusteringalgorithmen eine Lösung erzielt werden, aber zumindest war für jede Testinstanz mindestens ein Clustering-Verfahren erfolgreich. iv";"0,00";"0,00";"0,00"
"TUW-202824";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\groundtruth\TUW-202824.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\result\result-TUW-202824-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\extracted\grobid\grobid-TUW-202824-xstream.xml"")";"Cloud computing is introducing the next big shift in the IT-industry. It fundamentally changes the IT-strategy of organizations. Cloud computing promises many advantages such as reduced capital expense, support of brief surges in capacity and a better economies of scale [1]. Cloud computing is not only a useful technology for the private sector rather it also can benefit the public sector in many ways [2]. It makes e-government systems faster and cheaper and accelerates the adaptation of use of IT by citizens [3]. Cloud computing is high on the agenda of Obama administration and is being used by federal and local governments with significant benefits [4]. In the European Union the potentials of the Cloud computing have been recognized and the Cloud agenda is being pushed forward, not only because of its cost saving potentials but also because of its impact on the environment.
In this work we have conducted a case study for integration of Cloud computing in the Austrian Public sector. The contribution of this work is identification of the requirements of the public sector and obstacles for integration of cloud computing in the Austrian public sector. In this case study eight ministries and the office of chancellor have been interviewed.";"Cloud computing is introducing the next big shift in the IT-industry. It fundamentally changes the IT-strategy of organizations. Cloud computing promises many advantages such as reduced capital expense, support of brief surges in capacity and a better economies of scale [1]. Cloud computing is not only a useful technology for the private sector rather it also can benefit the public sector in many ways [2]. It makes e-government systems faster and cheaper and accelerates the adaptation of use of IT by citizens [3]. Cloud computing is high on the agenda of Obama administration and is being used by federal and local governments with significant benefits [4]. In the European Union the potentials of the Cloud computing have been recognized and the Cloud agenda is being pushed forward, not only because of its cost saving potentials but also because of its impact on the environment. In this work we have conducted a case study for integration of Cloud computing in the Austrian Public sector. The contribution of this work is identification of the requirements of the public sector and obstacles for integration of cloud computing in the Austrian public sector. In this case study eight ministries and the office of chancellor have been interviewed.";"100,00";"100,00";"100,00"
"TUW-203409";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\groundtruth\TUW-203409.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\result\result-TUW-203409-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\extracted\grobid\grobid-TUW-203409-xstream.xml"")";"Our work focuses on investigating novel ways of efficient processor simulation using just-in-time compilation techniques. We can automatically generate a cycle-accurate simulator from a processor description that captures hardware structure and instruction set. The simulator employs an adaptive two-level just-in-time compilation scheme based on LLVM to attain high simulation speeds.
As the main source of slowdown during simulation we identified the LLVM code generator. We reduced compilation time in our own experimental code generator by an order of magnitude compared to LLVM's original backend. Current work aims at leveraging instruction descriptions already available in LLVM to extend the coverage of our fast JIT code generator.";"Our work focuses on investigating novel ways of efficient processor simulation using just-in-time compilation techniques. We can automatically generate a cycle-accurate simulator from a processor description that captures hardware structure and instruction set. The simulator employs an adaptive two-level just-in-time compilation scheme based on LLVM to attain high simulation speeds. As the main source of slowdown during simulation we identified the LLVM code generator. We reduced compilation time in our own experimental code generator by an order of magnitude compared to LLVM's original backend. Current work aims at leveraging instruction descriptions already available in LLVM to extend the coverage of our fast JIT code generator.";"100,00";"100,00";"100,00"
"TUW-203924";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\groundtruth\TUW-203924.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\result\result-TUW-203924-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\extracted\grobid\grobid-TUW-203924-xstream.xml"")";"Genetic dispositions play a major role in individual disease risk and treatment response. Genomic medicine, in which medical decisions are refined by genetic information of particular patients, is becoming increasingly important. Here we describe our work and future visions around the creation of a distributed infrastructure for pharmacogenetic data and medical decision support, based on industry standards such as the Web Ontology Language (OWL) and the Arden Syntax.";"Genetic dispositions play a major role in individual disease risk and treatment response. Genomic medicine, in which medical decisions are refined by genetic information of particular patients, is becoming increasingly important. Here we describe our work and future visions around the creation of a distributed infrastructure for pharmacogenetic data and medical decision support, based on industry standards such as the Web Ontology Language (OWL) and the Arden Syntax.";"100,00";"100,00";"100,00"
"TUW-204724";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\groundtruth\TUW-204724.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\result\result-TUW-204724-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\extracted\grobid\grobid-TUW-204724-xstream.xml"")";"The strategic management of intellectual capital involves rethinking how the companies creates value from a knowledge -centric perspective and redesigning and orchestrating the role of staff knowledge in the firm's strategy. This paper presents the author's software system for facilitating decision making at a strategic level in terms of the profitability of investment in staff knowledge. This software is a computer implementation of the method for planning and selection of personnel in SME.";"The strategic management of intellectual capital involves rethinking how the companies creates value from a knowledge -centric perspective and redesigning and orchestrating the role of staff knowledge in the firm's strategy. This paper presents the author's software system for facilitating decision making at a strategic level in terms of the profitability of investment in staff knowledge. This software is a computer implementation of the method for planning and selection of personnel in SME. information systems that support knowledge management in SMEs can give guarantees a constant competitive advantage in the market.";"0,00";"0,00";"0,00"
"TUW-205557";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\groundtruth\TUW-205557.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\result\result-TUW-205557-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\extracted\grobid\grobid-TUW-205557-xstream.xml"")";"The spread of safety critical real time systems results in an increased necessity for worst case execution time (WCET) analysis of these systems: finding the time limit within which the software system responds in all possible scenarios. Computing the WCET for programs with loops or recursion is, in general, un-decidable. We present an automatic method for computing tight upper bounds on the iteration number of special classes of program loops. These upper bounds are further used in the WCET analysis of programs. The technique deploys pattern-based recurrence solving in conjunction with program flow refinement using SMT reasoning. To do so, we refine program flows using SMT reasoning and rewrite certain multi-path loops into single-path ones, possibly over-approximating the loop-bound. The multi-path loops we consider are I) abruptly-terminating loops that might terminate early due to break statements and II) loops with additional monotonic updates, that conditionally modify the loop counter. For those, the minimum increase of the loop counter is computed and used as loop step expression. Single-path loops are further translated into a set of recurrence relations over program variables. For solving recurrences we deploy a pattern-based recurrence solving algorithm, computing closed forms for a restricted class of recurrence equations. Finally, iteration bounds are derived for program loops from the computed closed forms. We only compute closed forms for a restricted class of loops, however, in practice, these recurrences describe the behavior of a large set of program loops that are relevant to WCET analysis. Our technique is implemented in the r-TuBound tool and was successfully tried out on a number of challenging WCET benchmarks: we evaluate the symbolic loop bound generation technique and present an experimental evaluation of the method carried out with the r-TuBound software tool. We evaluate our method against various academic and industrial WCET benchmarks, and compare the results to the original TuBound tool.";"Extended Abstract. The spread of safety critical real time systems results in an increased necessity for worst case execution time (WCET) analysis of these systems: finding the time limit within which the software system responds in all possible scenarios. Computing the WCET for programs with loops or recursion is, in general, un-decidable. We present an automatic method for computing tight upper bounds on the iteration number of special classes of program loops. These upper bounds are further used in the WCET analysis of programs. The technique deploys pattern-based recurrence solving in conjunction with program flow refinement using SMT reasoning. To do so, we refine program flows using SMT reasoning and rewrite certain multi-path loops into single-path ones, possibly over-approximating the loop-bound. The multi-path loops we consider are I) abruptly-terminating loops that might terminate early due to break statements and II) loops with additional monotonic updates, that conditionally modify the loop counter. For those, the minimum increase of the loop counter is computed and used as loop step expression. Single-path loops are further translated into a set of recurrence relations over program variables. For solving recurrences we deploy a pattern-based recurrence solving algorithm, computing closed forms for a restricted class of recurrence equations. Finally, iteration bounds are derived for program loops from the computed closed forms. We only compute closed forms for a restricted class of loops, however, in practice, these recurrences describe the behavior of a large set of program loops that are relevant to WCET analysis. Our technique is implemented in the r-TuBound tool and was successfully tried out on a number of challenging WCET benchmarks: we evaluate the symbolic loop bound generation technique and present an experimental evaluation of the method carried out with the r-TuBound software tool. We evaluate our method against various academic and industrial WCET benchmarks, and compare the results to the original TuBound tool.";"100,00";"100,00";"100,00"
"TUW-205933";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\groundtruth\TUW-205933.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\result\result-TUW-205933-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\extracted\grobid\grobid-TUW-205933-xstream.xml"")";"Philosophy-of-information considerations can analyse information concepts according to four ways of thinking. A Unified Theory of Information (UTI) requires the fourth way of thinking – integration. This integration can be performed, if a complex systems view is informed by the heuristics of a historical and logical account. In particular, the terms of ""difference"" or ""variety"", negentropy and semiosis are used for integration. Reference is made to Gregory Bateson, Arkady D. Ursul, Edgar Morin, and Charles Sanders Peirce. An integrated information definition is presented. Information is defined as relation such that an Evolutionary System se (signator; the signmaker) reflects (1) some perturbation P (signandum/signatum; (to-be-)signified (2) by the order O it builds up spontaneously (signans; the sign) (3) for the sake of negentropy. The process of information-generation coincides with the process of sign-production and both coincide with the process of self-organisation; so do their respective results: information, sign, and self-organised order. The concepts of self-organisation and information (sign) turn out to be co-extensive. The notion ""emergent information"" is applied to characterise the complexity of information processes that proceed between determinacy and indeterminacy. Since information generation is a process that allows novelty to emerge, it is worth noting that it is not a mechanical process that can be formalised, expressed by a mathematical function, or carried out by a computer.";"Philosophy-of-information considerations can analyse information concepts according to four ways of thinking. A Unified Theory of Information (UTI) requires the fourth way of thinking-integration. This integration can be performed, if a complex systems view is informed by the heuristics of a historical and logical account. In particular, the terms of ""difference"" or ""variety"", negentropy and semiosis are used for integration. Reference is made to Gregory Bateson, Arkady D. Ursul, Edgar Morin, and Charles Sanders Peirce. An integrated information definition is presented. Information is defined as relation such that an Evolutionary System se (signator; the signmaker) reflects (1) some perturbation P (signandum/signatum; (to-be-)signified (2) by the order O it builds up spontaneously (signans; the sign) (3) for the sake of negentropy. The process of information-generation coincides with the process of sign-production and both coincide with the process of self-organisation; so do their respective results: information, sign, and self-organised order. The concepts of self-organisation and information (sign) turn out to be co-extensive. The notion ""emergent information"" is applied to characterise the complexity of information processes that proceed between determinacy and indeterminacy. Since information generation is a process that allows novelty to emerge, it is worth noting that it is not a mechanical process that can be formalised, expressed by a mathematical function, or carried out by a computer.";"100,00";"100,00";"100,00"
"TUW-213513";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\groundtruth\TUW-213513.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\result\result-TUW-213513-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\extracted\grobid\grobid-TUW-213513-xstream.xml"")";;;"none extracted value";"none expected";"NA"
"TUW-216744";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\groundtruth\TUW-216744.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\result\result-TUW-216744-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\extracted\grobid\grobid-TUW-216744-xstream.xml"")";"Medical practitioners often have unmet information needs that impact patient care. However, currently available web-based search engines are not suitable for routine use. Finding relevant information takes too long, assessing the trustworthiness of found information is difficult, and support for the heterogeneity of languages and nomenclature across European countries is lacking. In this paper, we analyze the current barriers to web-based searching by medical practitioners and introduce the European Khresmoi project, which aims to dismantle these barriers.";"Medical practitioners often have unmet information needs that impact patient care. However, currently available web-based search engines are not suitable for routine use. Finding relevant information takes too long, assessing the trustworthiness of found information is difficult, and support for the heterogeneity of languages and nomenclature across European countries is lacking. In this paper, we analyze the current barriers to web-based searching by medical practitioners and introduce the European Khresmoi project, which aims to dismantle these barriers. 1. Background Physicians often have unmet information needs. These have been reported as occurring for up to 2 of every 3 patients seen [1], or more recently for 41% of the questions they pursued [2]. Although these medical professionals have many tools for information search available (such as PubMed), studies have revealed that they do not use them to their full capabilities. Most questions arise during consultations and have a direct impact on the medical decision process [3]. There is evidence suggesting that physicians primarily respond to their information needs when they perceive the question to be urgent and believe that definitive answers can be found [4]. However, physicians are often restricted in their search by time constraints [5]. Physicians search on average for less than 5 minutes to answer questions [6]. A so-called ""90 second rule"" has been described in the literature-meaning physicians do not even attempt to find information unless they think they can do it in a minute and a half [7]. Hence, it is important that the pertinent information is found during this time. However, the time taken to answer questions using PubMed averages 30 minutes [1] and the information found is often scattered over multiple articles, making PubMed searching impractical for routine clinical use [6]. Furthermore, physicians that are not native English speakers using systems in English language are prone to use erroneous search terms, resulting in poorer returned results [8]. The World Wide Web has a lot to offer in terms of both quantity and quality of medical information [9]. There is no consensus within the literature as to what extent doctors currently rely on web-based searching as compared to other information sources. One line of research suggests that physicians often find it quicker and easier to look up answers in a pocket reference book or ask a colleague for advice [10] rather than searching on the Internet. In addition current web-based solutions fail to provide psychological support, guidance, affirmation, sympathy, judgement, and feedback, which colleagues can provide within the daily decision making process of a physician. A review by Davies [3] compared relevant research between 2000 and 2005, and found that text books (39%) and colleagues (25%) were the information sources physicians accessed most frequently, while computer resources were used only by 13%. However, an upward trend of Internet use is visible as the highest percentage of use examined in the study, 53%, occurred in the latest published research from 2005. A Spanish study published in 2007 [11] found that the majority of physicians still relied on colleagues, drug compendiums and textbooks rather than on web-based resources. A possible explanation is language as a potential barrier to web-based searching, and it appears to be inadequately addressed by current web-based solutions within the medical domain. In contrast to these findings is research claiming a clear preference of the Internet as a primary informational resource amongst physicians [12],[13]. Both studies provide support for the notion that the Internet has become an important information source amongst physicians. Possible explanations of conflicting study outcomes could be the variance among medical specialities, different geographical locations and potential biases introduced by different methodologies used in the studies (e.g., user observations versus self reports). Furthermore, the rapid changes and advances in the field of information technology make comparisons over larger time spans difficult. There is also conflicting data about which web sites and tools physicians use to look for medical information on the web. Some publications suggest that general-purpose search services such as Google can play a useful role in the medical decision making process [14],[15]. In contrast, Leo et. al. [16] reported that physicians mistrust the quality of results from such search engines and prefer to directly access specialized medical websites. A study by Yu and Kaufman [14] suggests that Google is preferred for finding medical definitions, as it is easy to use and provides good answers to simple questions. However, for more complex information needs more advanced search systems may be required. A recent study funded by Google [13] is in strong contrast with prior findings. It postulates that the majority of physicians use Google or a similar search engine as their primary information sources in the clinical decision making process. However, it is unclear as to what extent the study was biased in terms of sample selection. Another study showed that general practitioners use Google as their first information source, primarily to lead them to higher quality websites [17]. Thus, it appears that physicians are currently willing to use a search engine for simple questions and as an initial source to help them find their way to higher quality websites.";"0,00";"0,00";"0,00"
"TUW-217690";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\groundtruth\TUW-217690.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\result\result-TUW-217690-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\extracted\grobid\grobid-TUW-217690-xstream.xml"")";"Precise localization for mobile Augmented Reality in large indoor environments without specific tracking infrastructure is challenging. This is especially true for rooms with changing properties, like lighting, seating and carpeting. With these constraints a map for a vision based tracking approach has to be continuously updated. The Parallel Tracking and Mapping (PTAM) algorithm is capable of generating and extending a map while tracking the camera pose in an unknown environment. However, it has originally been designed for small workspace environments and has therefore certain limitations. We have extended and modified the original implementation in order to ensure efficient and robust map generation and tracking in large rooms. Furthermore, we have tested a mobile setup with the system in Festsaal in Vienna’s Hofburg, which is close to thousand square meters in size. The user’s position and path was tracked while the environment was augmented with virtual objects and the system was successfully tested for robustness and occlusions.";;"none extracted value";"0,00";"0,00"
"TUW-217971";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\groundtruth\TUW-217971.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\result\result-TUW-217971-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\extracted\grobid\grobid-TUW-217971-xstream.xml"")";"Many real-world Visual Analytics applications involve time-oriented data. I am working in a research project related to this challenge where I am responsible for the interactive visualization part. My goal are interactive visualizations to explore such time-oriented data according to the user tasks while considering the structure of time. Time is composed of many granularities that are likely to have crucial influence on the formation of the data. The challenge is to integrate the granularities into a detailed compound view on the data, like the compound eye of insects integrates many images into one view. Other members of our team are experts in temporal data mining and user centered design. The goal is to combine our research topics to an integrated system that helps domain experts to get more insight from their time-oriented data.";"Many real-world Visual Analytics applications involve time-oriented data. I am working in a research project related to this challenge where I am responsible for the interactive visualization part. My goal are interactive visualizations to explore such time-oriented data according to the user tasks while considering the structure of time. Time is composed of many granularities that are likely to have crucial influence on the formation of the data. The challenge is to integrate the granularities into a detailed compound view on the data, like the compound eye of insects integrates many images into one view. Other members of our team are experts in temporal data mining and user centered design. The goal is to combine our research topics to an integrated system that helps domain experts to get more insight from their time-oriented data. a natural context, like the change of seasons, they are usually regular , but granularities based on social context can also be irregular, e.g., easter time. They play an important role in many datasets from real-world applications, like customer data from shops. 1.2 Interactive Visualization Application domains where time-oriented-data occur include commerce , health care, public security, and others. These data are usually multivariate, resulting from heterogeneous data sources. In many cases, the structure of time has strongly influenced the events the data results from. The user tasks are complex and involve many abstract questions. By studying surveys of visualizations for time-oriented data like the one by Aigner [1], it becomes apparent that most current work at visualizing time-oriented data is focused on solving one or more of the pressing tasks, but most visualizations neglect the structure of time. In the following, this structure is discussed , as well as what we consider the most important research topics in developing interactive visualizations. User interaction is one of the most important elements of Info-Vis, or even the ""heart"" as Spence states [9]. User interaction is even more important in Visual Analytics, as studies like the one by Saraiya et al. [7] shows: users preferred inferior visualizations with interaction over superior static visualizations. The basic interactions in visualizations of time-oriented data are interval selections, like zooming and panning, but also others like detail on demand or brushing. Most interactions are not tailored particularly for time-oriented data. E.g., if a user analyzes a day and wants to see a day one month later, she often has to go forward 28-31 days, because the visualization is at day scale. It would be easier to just go forward one month, without having to think about details. If another user wants to zoom out to one month, he has not only to respect different month lengths, but also consider that the day viewed before might not be in the center of the month. Still, it is most likely the user wants to see the month that contains the day, from the first day to the last day. These examples show that it is important for interactive visualizations to respect the structure of time.";"0,00";"0,00";"0,00"
"TUW-221215";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\groundtruth\TUW-221215.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\result\result-TUW-221215-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\extracted\grobid\grobid-TUW-221215-xstream.xml"")";"The Selective Graph Coloring Problem (SGCP) is about finding a subgraph of a particular structure whose chromatic number is as low as possible. The original graph is divided into several clusters, and from each cluster the subgraph has to contain exactly one node. This problem is NP-hard and therefore it is usually solved by means of heuristics.
	
I implemented several variants of an algorithm making use of Variable Neighborhood Search (VNS) to search the space of solution candidates and then evaluating the solution using heuristic or exact methods. Furthermore, each variant can be used with or without a solution archive, i.e. a data structure in which previously found solutions are stored so that duplicates need not be re-evaluated but can be efficiently converted into new solutions instead. For exact computation of the chromatic number integer linear programming was used. To obtain an upper bound a variant of greedy coloring was used. Another variant of the algorithm also counts the number of conflicts that would appear if one color less were used. Finally, two methods were implemented to obtain a lower bound: maximum clique and linear programming using column generation.

The program was tested with various instances from the literature. My algorithm often finished computation within a very short time, but in general it led to slightly worse results.";"Hiermit erkläre ich, dass ich diese Arbeit selbständig verfasst habe, dass ich die verwendeten Quellen und Hilfsmittel vollständig angegeben habe und dass ich die Stellen der Arbeit-einschließlich Tabellen, Karten und Abbildungen-, die anderen Werken oder dem Internet im Wortlaut oder dem Sinn nach entnommen sind, auf jeden Fall unter Angabe der Quelle als Entlehnung kenntlich gemacht habe. ______________________ ______________________________________________ (Ort, Datum) (Unterschrift Verfasser) Danksagung Ich möchte mich vor allem bei Günther Raidl für die Möglichkeit bedanken, meine Diplomarbeit an seiner Abteilung zu verfassen, und bei Bin Hu für die ausgezeichnete Betreuung. Weiters möchte ich mich bei meinen Eltern bedanken, die mir das Studium finanziell ermöglicht haben. Zudem habe ich ihnen auch zu verdanken, dass sie mein Interesse an Computern frühzeitig erkannt und gefördert haben. Abstract The Selective Graph Coloring Problem (SGCP) is about finding a subgraph of a particular structure whose chromatic number is as low as possible. The original graph is divided into several clusters, and from each cluster the subgraph has to contain exactly one node. This problem is NP-hard and therefore it is usually solved by means of heuristics. I implemented several variants of an algorithm making use of Variable Neighborhood Search (VNS) to search the space of solution candidates and then evaluating the solution using heuristic or exact methods. Furthermore, each variant can be used with or without a solution archive, i.e. a data structure in which previously found solutions are stored so that duplicates need not be re-evaluated but can be efficiently converted into new solutions instead. For exact computation of the chromatic number integer linear programming was used. To obtain an upper bound a variant of greedy coloring was used. Another variant of the algorithm also counts the number of conflicts that would appear if one color less were used. Finally, two methods were implemented to obtain a lower bound: maximum clique and linear programming using column generation. The program was tested with various instances from the literature. My algorithm often finished computation within a very short time, but in general it led to slightly worse results. Kurzfassung Beim Selective Graph Coloring Problem (SGCP) geht es darum, einen Teilgraphen mit spezieller Struktur zu finden, dessen chromatische Zahl so niedrig wie möglich ist. Der Ursprungsgraph ist in mehrere Cluster unterteilt, und von jedem Cluster muss der Teilgraph genau einen Knoten enthalten. Dieses Problem ist NP-schwer und wird daher meistens mit Heuristiken gelöst. Ich habe mehrere Varianten eines Algorithmus implementiert, der Variable Neighborhood Search (VNS) benutzt, um den Lösungsraum zu durchsuchen, und dann die gefundene Lösung mit heuristischen oder exakten Methoden evaluiert. Jede Variante kann mit oder ohne ein Lösungsarchiv verwendet werden. Ein Lösungsarchiv ist eine Datenstruktur, in der bereits gefundene Lösungen gespeichert werden, so dass Duplikate nicht neu evaluiert werden müssen, sondern effizient zu neuen Lösungen konvertiert werden können. Um eine obere Schranke zu errechnen, wurde eine Variante von Greedy Coloring verwendet. Eine weitere Variante des Algorithmus zählt auch die Anzahl der Konflikte, die entstünden, würde eine Farbe weniger verwendet werden. Schließlich wurden zwei Methoden umgesetzt, um eine untere Schranke zu berechnen: maximale Clique und lineare Programmierung mit Spaltengenerierung. Das Programm wurde mit verschiedenen Instanzen aus der Literatur getestet. Mein Algorithmus beendete die Berechnungen oft schon nach sehr kurzer Laufzeit, führte aber im Allgemeinen zu geringfügig schlechteren Ergebnissen.";"0,00";"0,00";"0,00"
"TUW-223906";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\groundtruth\TUW-223906.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\result\result-TUW-223906-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\extracted\grobid\grobid-TUW-223906-xstream.xml"")";"This paper describes our contribution to the social event detection (SED) task of the MediaEval Benchmark 2013. We present a robust unsupervised approach for the clustering of tagged photos and videos into social events. Results on the SED datasets show that the proposed approach yields an excellent generalization ability and state-of-the-art clustering performance.";"This paper describes our contribution to the social event detection (SED) task of the MediaEval Benchmark 2013. We present a robust unsupervised approach for the clustering of tagged photos and videos into social events. Results on the SED datasets show that the proposed approach yields an excellent generalization ability and state-of-the-art clustering performance.";"100,00";"100,00";"100,00"
"TUW-223973";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\groundtruth\TUW-223973.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\result\result-TUW-223973-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\extracted\grobid\grobid-TUW-223973-xstream.xml"")";"Translating clinical practice guidelines into a computer-inter-pretable format is a challenging and laborious task. In this project we focus on supporting the early steps of the modeling process by automatically identifying conditional activities in guideline documents in order to model them automatically in further consequence. Therefore, we developed a rule-based, heuristic method that combines domain-independent information extraction rules and semantic pattern rules. The classification also uses a weighting coefficient to verify the relevance of the sentence in the context of other information aspects, such as effects, intentions , etc. Our evaluation results show that even with a small set of training data, we achieved a recall of 75 % and a precision of 88 %. This outcome shows that this method supports the modeling task and eases the translation of CPGs into a semi-formal model.";"Translating clinical practice guidelines into a computer-inter-pretable format is a challenging and laborious task. In this project we focus on supporting the early steps of the modeling process by automatically identifying conditional activities in guideline documents in order to model them automatically in further consequence. Therefore, we developed a rule-based, heuristic method that combines domain-independent information extraction rules and semantic pattern rules. The classification also uses a weighting coefficient to verify the relevance of the sentence in the context of other information aspects, such as effects, intentions , etc. Our evaluation results show that even with a small set of training data, we achieved a recall of 75 % and a precision of 88 %. This outcome shows that this method supports the modeling task and eases the translation of CPGs into a semi-formal model.";"100,00";"100,00";"100,00"
"TUW-225252";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\groundtruth\TUW-225252.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\result\result-TUW-225252-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\extracted\grobid\grobid-TUW-225252-xstream.xml"")";;;"none extracted value";"none expected";"NA"
"TUW-226000";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\groundtruth\TUW-226000.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\result\result-TUW-226000-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\extracted\grobid\grobid-TUW-226000-xstream.xml"")";"The user interface is the most important feature of interaction between users and (AAL) services. Explicitly defined user interfaces are bound to a specific toolkit and programming language or markup language. Thus a separate user interface definition has to be created manually for different classes of I/O devices to be supported. Compared to manual user interface creation, the automatic or semi-automatic generation of user interfaces based on interaction descriptions considerably reduces the manual effort necessary for integrating a large number of devices and therefore automatically increases the number of supported devices. The main goal of this paper is to provide an overview of selected existing solutions for the definition of generic user interactions and the generation of user interfaces. The comparison shows that the aspect of adaptability is partly covered by the presented User Interaction Description Languages. Nevertheless it is important to analyze them with respect to additional criteria, like accessibility, context-and use-case awareness, to receive a meaningful overview of advantages and drawbacks of the different approaches leading to a good basis for choosing one of the presented approaches.";"The user interface is the most important feature of interaction between users and (AAL) services. Explicitly defined user interfaces are bound to a specific toolkit and programming language or markup language. Thus a separate user interface definition has to be created manually for different classes of I/O devices to be supported. Compared to manual user interface creation, the automatic or semi-automatic generation of user interfaces based on interaction descriptions considerably reduces the manual effort necessary for integrating a large number of devices and therefore automatically increases the number of supported devices. The main goal of this paper is to provide an overview of selected existing solutions for the definition of generic user interactions and the generation of user interfaces. The comparison shows that the aspect of adaptability is partly covered by the presented User Interaction Description Languages. Nevertheless it is important to analyze them with respect to additional criteria, like accessibility, context-and use-case awareness, to receive a meaningful overview of advantages and drawbacks of the different approaches leading to a good basis for choosing one of the presented approaches.";"100,00";"100,00";"100,00"
"TUW-226016";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\groundtruth\TUW-226016.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\result\result-TUW-226016-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\extracted\grobid\grobid-TUW-226016-xstream.xml"")";"Invariant genereation is a critical problem in proving different properties for programs with loops, properties including correctnes. The problem becomes harder with the incresing numbers of quantifiers in the property to be proven. In this paper we study and combine different methods of invariant generation in order to obtain stronger properties.";"Hiermit erkläre ich, dass ich diese Arbeit selbständig verfasst habe, dass ich die verwendeten Quellen und Hilfsmittel vollständig angegeben habe und dass ich die Stellen der Arbeit-einschließlich Tabellen, Karten und Abbildungen-, die anderen Werken oder dem Internet im Wortlaut oder dem Sinn nach entnommen sind, auf jeden Fall unter Angabe der Quelle als Entlehnung kenntlich gemacht habe. (Ort, Datum) (Unterschrift Verfasserin) ii Acknowledgements I would like to express my very great appreciation to Dr. Laura Kovács for her valuable and constructive suggestions during the planning and development of this research work. Her willingness to give her time so generously has been very much appreciated. I would also like to thank Mr. Ioan Dr? agan for his support with one of the tools needed. iii Abstract Invariant genereation is a critical problem in proving different properties for programs with loops, properties including correctnes. The problem becomes harder with the incresing numbers of quantifiers in the property to be proven. In this paper we study and combine different methods of invariant generation in order to obtain stronger properties. iv Kurzfassung Invariant generiert ist ein kritische Problem für Programmen mit Schleife zum Beweisen der Eigenschaften, inclusive die Richtigkeit. Die problem wird schwerer bei hohe Anzhal des Quantoren in die geprüfte Eigenschaft. In diese arbeit wir studiere diese Problem und versuchen combinieren verschieden Methoden für schwarer invariants zu beweisen. v";"0,00";"0,00";"0,00"
"TUW-228620";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\groundtruth\TUW-228620.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\result\result-TUW-228620-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\extracted\grobid\grobid-TUW-228620-xstream.xml"")";"Business networking relies on application-specific quantity and quality of information in order to support social infrastructures in, e.g., energy allocation coordinated by smart grids, healthcare services with electronic health records, traffic management with personal sensors, RFID in retail and logistics, or integration of individuals' social network information into good, services, and rescue operations. Due to the increasing reliance of networking applications on sharing ICT services, dependencies threaten privacy, security, and reliability of information and, thus, innovative business applications in smart societies. Resilience is becoming a new security approach, since it takes dependencies into account and aims at achieving equilibriums in case of opposite requirements. This special issue on 'Security and Privacy in Business Networking' contributes to the journal 'Electronic Markets' by introducing a different view on achieving acceptable secure business networking applications in spite of threats due to covert channels. This view is on adapting resilience to enforcement of IT security in business networking applications. Our analysis shows that privacy is an evidence to measure and improve trustworthy relationships and reliable interactions between participants of business processes and their IT systems. The articles of this special issue, which have been accepted after a double-blind peer review, contribute to this view on interdis-ciplinary security engineering in regard to the stages of security and privacy requirements analysis, enforcement of resulting security requirements for an information exchange, testing with a privacy-preserving detection of policy violations, and knowledge management for the purpose of keeping business processes resilient.";"Business networking relies on application-specific quantity and quality of information in order to support social infrastructures in, e.g., energy allocation coordinated by smart grids, healthcare services with electronic health records, traffic management with personal sensors, RFID in retail and logistics , or integration of individuals' social network information into good, services, and rescue operations. Due to the increasing reliance of networking applications on sharing ICT services , dependencies threaten privacy, security, and reliability of information and, thus, innovative business applications in smart societies. Resilience is becoming a new security approach , since it takes dependencies into account and aims at achieving equilibriums in case of opposite requirements. This special issue on 'Security and Privacy in Business Networking' contributes to the journal 'Electronic Markets' by introducing a different view on achieving acceptable secure business networking applications in spite of threats due to covert channels. This view is on adapting resilience to enforcement of IT security in business networking applications. Our analysis shows that privacy is an evidence to measure and improve trustworthy relationships and reliable interactions between participants of business processes and their IT systems. The articles of this special issue, which have been accepted after a double-blind peer review, contribute to this view on interdis-ciplinary security engineering in regard to the stages of security and privacy requirements analysis, enforcement of resulting security requirements for an information exchange, testing with a privacy-preserving detection of policy violations , and knowledge management for the purpose of keeping business processes resilient.";"100,00";"100,00";"100,00"
"TUW-231707";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\groundtruth\TUW-231707.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\result\result-TUW-231707-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\extracted\grobid\grobid-TUW-231707-xstream.xml"")";"The VRVis Research Center in Vienna is the largest technology transfer institution in the area of Visual Computing in Austria. The requirements of the funding body FFG include the publication of scientific research results in first class peer reviewed media, and the active cooperation with co-funding companies. As a consequence the requirements on the staff of VRVis are manifold: they have to communicate with real users, use real data, know about software and hardware, understand the market, do professional documentation, initiate new projects and write funding proposals for these, be part of the scientific community and publish and review papers, manage several projects in parallel and obey strict deadlines for their projects and some more. Such staff is barely available and must be trained on the job.";"The VRVis Research Center in Vienna is the largest technology transfer institution in the area of Visual Computing in Austria. The requirements of the funding body FFG include the publication of scientific research results in first class peer reviewed media, and the active cooperation with co-funding companies. As a consequence the requirements on the staff of VRVis are manifold: they have to communicate with real users, use real data, know about software and hardware, understand the market, do professional documentation, initiate new projects and write funding proposals for these, be part of the scientific community and publish and review papers, manage several projects in parallel and obey strict deadlines for their projects and some more. Such staff is barely available and must be trained on the job.";"100,00";"100,00";"100,00"
"TUW-233317";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\groundtruth\TUW-233317.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\result\result-TUW-233317-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\extracted\grobid\grobid-TUW-233317-xstream.xml"")";"Visual Analytics prototypes increasingly support human sensemak-ing through providing Provenance information. For data analysts the challenge of knowledge generation starts with assessing the quality of a data set, but Provenance is not yet utilized to aid this task. This position paper aims at characterizing the complexity of Visual Analytics methods introducing Provenance in Data Quality by highlighting the challenges of (1) generating Provenance from Data Quality Control and (2) sensemaking based on Data Quality Provenance.";"Visual Analytics prototypes increasingly support human sensemak-ing through providing Provenance information. For data analysts the challenge of knowledge generation starts with assessing the quality of a data set, but Provenance is not yet utilized to aid this task. This position paper aims at characterizing the complexity of Visual Analytics methods introducing Provenance in Data Quality by highlighting the challenges of (1) generating Provenance from Data Quality Control and (2) sensemaking based on Data Quality Provenance.";"100,00";"100,00";"100,00"
"TUW-233657";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\groundtruth\TUW-233657.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\result\result-TUW-233657-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\extracted\grobid\grobid-TUW-233657-xstream.xml"")";"Recent research in Visualization has focused mostly on data analysis systems for domain experts, but also considered presentation to external people in the form of storytelling. The established directions assume that the target audience has in inherent interest in the facts to be discovered, sometimes even to the point of them being willing to learn how to operate a complex visualization system and spend considerable time and effort. In reality, sometimes the opposite is true: people unwilling to face an inconvenient truth actively avert their eyes. As a solution, we propose the presentation of facts by experts who manage to gain a limited amount of attention by means of rapid and expressive visualization. Using conventional desktop systems, this method is hard to implement, but new visual channels will open up new possibilities.";"Recent research in Visualization has focused mostly on data analysis systems for domain experts, but also considered presentation to external people in the form of storytelling. The established directions assume that the target audience has in inherent interest in the facts to be discovered, sometimes even to the point of them being willing to learn how to operate a complex visualization system and spend considerable time and effort. In reality, sometimes the opposite is true: people unwilling to face an inconvenient truth actively avert their eyes. As a solution, we propose the presentation of facts by experts who manage to gain a limited amount of attention by means of rapid and expressive visualization. Using conventional desktop systems, this method is hard to implement, but new visual channels will open up new possibilities.";"100,00";"100,00";"100,00"
"TUW-236063";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\groundtruth\TUW-236063.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\result\result-TUW-236063-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\extracted\grobid\grobid-TUW-236063-xstream.xml"")";"The purpose of the project VALCRI is to develop a new system prototype for information exploitation by intelligence analysts working in law enforcement agencies. Information visu-alisation will be a core element of the prototype. Such systems have to be designed to support the sensemaking and reasoning processes of the analysts. One of the goals of the project is, therefore, to get a more thorough understanding of sensemaking processes and to develop a set of recommendations for the design of intelligence analysis systems to help analysts in their work.";"The purpose of the project VALCRI is to develop a new system prototype for information exploitation by intelligence analysts working in law enforcement agencies. Information visu-alisation will be a core element of the prototype. Such systems have to be designed to support the sensemaking and reasoning processes of the analysts. One of the goals of the project is, therefore, to get a more thorough understanding of sensemaking processes and to develop a set of recommendations for the design of intelligence analysis systems to help analysts in their work.";"100,00";"100,00";"100,00"
"TUW-236120";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\groundtruth\TUW-236120.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\result\result-TUW-236120-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\extracted\grobid\grobid-TUW-236120-xstream.xml"")";"As part of the project ""Educational standards in vocational schools"" the responsible Federal Ministry compiled competence models, descriptors and didactic examples for Informatics in technical high schools (HTL). The implementation of this project implies a paradigm shift for the vocational school system , which requires a new pedagogic foundation. This paper presents a didacti-cal concept, that meets the requirements of the educational standards in general and competence orientation in particular. A proposal for the implementation of these educational standards in the competence area industrial information technology (INIT) is presented. The specification is based on selected teaching sessions for micro controller technique. Learner-oriented teaching methods are applied , along with procedures to promote the learners' intrinsic motivation and creativity in general. It became apparent that the competence area INIT conveys remarkably demanding learning outcomes and its implementation proved challenging on multiple levels.";"As part of the project ""Educational standards in vocational schools"" the responsible Federal Ministry compiled competence models, descriptors and didactic examples for Informatics in technical high schools (HTL). The implementation of this project implies a paradigm shift for the vocational school system , which requires a new pedagogic foundation. This paper presents a didacti-cal concept, that meets the requirements of the educational standards in general and competence orientation in particular. A proposal for the implementation of these educational standards in the competence area industrial information technology (INIT) is presented. The specification is based on selected teaching sessions for micro controller technique. Learner-oriented teaching methods are applied , along with procedures to promote the learners' intrinsic motivation and creativity in general. It became apparent that the competence area INIT conveys remarkably demanding learning outcomes and its implementation proved challenging on multiple levels.";"100,00";"100,00";"100,00"
"TUW-237297";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\groundtruth\TUW-237297.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\result\result-TUW-237297-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\extracted\grobid\grobid-TUW-237297-xstream.xml"")";"This paper is about the role of e-learning environments to support first year students of computer science. Our goal is to make their transition easier from high school to university learning as well as to introduce our students self-regulated learning step by step. Our results are based on our qualitative study with our students. We analyzed the interviews we carried out with them to identify challenges in the first semester program. In this paper we discussed some possible solutions with support of our e-learning environment we have already established. We know that we need to adapt our system to meet these challenges. We conclude our paper with our future work.";"This paper is about the role of e-learning environments to support first year students of computer science. Our goal is to make their transition easier from high school to university learning as well as to introduce our students self-regulated learning step by step. Our results are based on our qualitative study with our students. We analyzed the interviews we carried out with them to identify challenges in the first semester program. In this paper we discussed some possible solutions with support of our e-learning environment we have already established. We know that we need to adapt our system to meet these challenges. We conclude our paper with our future work. INTRODUCTION Seen as a supporting electronic technology e-learning is well-established at several universities for several reasons: to support the lecturers to prepare their courses in a multimodal way, to support students to get all relevant information of a course asynchronously, to provide additional support to students by answering their questions and helping them in their assignments, to name a few. Additional features of creating time schedules, discussion groups and forums among participants, messaging mechanisms, grading and feedback possibilities, etc. make the teaching and learning easier to all stakeholders involved. Nevertheless the most e-learning systems do not provide enough support for learning from students' individual point of view and see learners as ""deindividualized and demoted noncritical homogenous users"" [6, p.273]. In this paper we show that the opposite is the case, especially when considering first semester students of higher education. If it comes to introduce self-regulated learning, it is not only about the material provided for the students [6] or using ICT to improve assessment processes [8], it is about processes like scheduling, planning, and managing the learning activities, or assessment of one's knowledge and preparation for exams, etc. These are the factors for what we show in this paper evidence from our field study. Considering learning as an active, self-regulated, constructive, and situated process [4] [1] e-learning systems need to support learners in management and organization of learning activities, especially when the study requirements are unfamiliar, high (at least higher than expected), and not much individualized. One of the main goals of our paper is to show how to accompany ""novice"" learners in the first semester of a computer science university study to ""advanced beginners"" [3]. Our focus is on e-learning support to self-regulated learning. Since there are several definitions of e-learning, we want to clarify that we refer to the following definition: ""… all forms of electronic supported learning and teaching, which are procedural in character and aim to effect the construction of knowledge with reference to individual experience, practice and knowledge of the learner. Information and communication systems, whether networked or not, serve as specific media (specific in the sense elaborated previously) to implement the learning process."" [6, p.274]. Self-regulation is based on ""students' self-generated thoughts and behaviors that are systematically oriented toward the attainment of their learning goals"" [5, p.59]. This also means that students contribute actively to their learning goals and procedure. The main research question we deal with in this paper is: How can we apply e-learning mechanisms and systems to help students to transit from familiar learning structures and habits at high school to autonomous self-organized learning at a university? This is followed by other questions like: How can we support a newcomer at a university at all? What do we need to consider in a first year computer science (CS)";"0,00";"0,00";"0,00"
"TUW-240858";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\groundtruth\TUW-240858.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\result\result-TUW-240858-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\extracted\grobid\grobid-TUW-240858-xstream.xml"")";"The chase procedure is considered as one of the most fundamental algorithmic tools in database theory. It has been successfully applied to different database problems such as data exchange, and query answering and containment under constraints, to name a few. One of the central problems regarding the chase procedure is all-instance termination, that is, given a set of tuple-generating dependencies (TGDs) (a.k.a. existential rules), decide whether the chase under that set terminates, for every input database. It is well-known that this problem is un-decidable, no matter which version of the chase we consider. The crucial question that comes up is whether existing restricted classes of TGDs, proposed in different contexts such as ontological reasoning, make the above problem decidable. In this work, we focus our attention on the oblivious and the semi-oblivious versions of the chase procedure, and we give a positive answer for classes of TGDs that are based on the notion of guardedness.";"The chase procedure is considered as one of the most fundamental al-gorithmic tools in database theory. It has been successfully applied to different database problems such as data exchange, and query answering and containment under constraints, to name a few. One of the central problems regarding the chase procedure is all-instance termination, that is, given a set of tuple-generating dependencies (TGDs) (a.k.a. existential rules), decide whether the chase under that set terminates, for every input database. It is well-known that this problem is un-decidable, no matter which version of the chase we consider. The crucial question that comes up is whether existing restricted classes of TGDs, proposed in different contexts such as ontological reasoning, make the above problem decidable. In this work, we focus our attention on the oblivious and the semi-oblivious versions of the chase procedure, and we give a positive answer for classes of TGDs that are based on the notion of guardedness.";"100,00";"100,00";"100,00"
"TUW-245336";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\groundtruth\TUW-245336.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\result\result-TUW-245336-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\extracted\grobid\grobid-TUW-245336-xstream.xml"")";"As technology designers, we often inadvertently inscribe values and concepts in systems beyond what we intended. Further, while we aim to work from a user-and use-centred perspective, we often miss the perspectives of other critical stakeholders and the broader context in which technology systems are to be used. Reflecting on issues of responsible innovation from the bottom up, this paper explores two exemplar cases: designing for older people and health care agendas, and designing for eco-behaviour change agendas. While very different in focus, both highlight the importance that different conceptualisations have on shaping possible solutions, and the challenge of identifying and negotiating competing agendas. This draws attention to the limits of both top-down and bottom-up perspectives and suggests instead a middle out approach to responsible design that considers both policy and practice aspects in the process. It also calls for us as responsible designers to be reflective practitioners aware of our power in inscribing possible futures.";"As technology designers, we often inadvertently inscribe values and concepts in systems beyond what we intended. Further, while we aim to work from a user-and use-centred perspective, we often miss the perspectives of other critical stakeholders and the broader context in which technology systems are to be used. Reflecting on issues of responsible innovation from the bottom up, this paper explores two exemplar cases: designing for older people and health care agendas, and designing for eco-behaviour change agendas. While very different in focus, both highlight the importance that different conceptualisations have on shaping possible solutions, and the challenge of identifying and negotiating competing agendas. This draws attention to the limits of both top-down and bottom-up perspectives and suggests instead a middle out approach to responsible design that considers both policy and practice aspects in the process. It also calls for us as responsible designers to be reflective practitioners aware of our power in inscribing possible futures.";"100,00";"100,00";"100,00"
"TUW-245799";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\groundtruth\TUW-245799.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\result\result-TUW-245799-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\extracted\grobid\grobid-TUW-245799-xstream.xml"")";"The diagnosis of failures in train traffic installations can be done in several ways: direct observations and measurements, software assisted diagnosis using specific software packages, process variable monitoring for electronically centralized installations. This work presents basic concepts for Model Based Diagnosis (MBD) that uses fuzzy logic to analyse the integrity of Centralized Traffic Control Installations (CTC) in train traffic. We define the diagnosis relations to be used and show how to apply them to train traffic security installations. Implementing these concepts into an expert system assists maintenance operators in quick failure diagnosis of the train traffic security installations.";"The diagnosis of failures in train traffic installations can be done in several ways: direct observations and measurements, software assisted diagnosis using specific software packages, process variable monitoring for electronically centralized installations. This work presents basic concepts for Model Based Diagnosis (MBD) that uses fuzzy logic to analyse the integrity of Centralized Traffic Control Installations (CTC) in train traffic. We define the diagnosis relations to be used and show how to apply them to train traffic security installations. Implementing these concepts into an expert system assists maintenance operators in quick failure diagnosis of the train traffic security installations.";"100,00";"100,00";"100,00"
"TUW-247301";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\groundtruth\TUW-247301.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\result\result-TUW-247301-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\extracted\grobid\grobid-TUW-247301-xstream.xml"")";;;"none extracted value";"none expected";"NA"
"TUW-247741";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\groundtruth\TUW-247741.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\result\result-TUW-247741-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\extracted\grobid\grobid-TUW-247741-xstream.xml"")";"Evaluating the quality of segmentations is an important process in image processing, especially in the medical domain. Many evaluation metrics have been used in evaluating segmentation. There exists no formal way to choose the most suitable metric(s) for a particular segmentation task and/or particular data. In this paper we propose a formal method for choosing the most suitable metrics for evaluating the quality of segmenta-tions with respect to ground truth segmentations. The proposed method depends on measuring the bias of metrics towards/against the properties of the the seg-mentations being evaluated. We firstly demonstrate how metrics can have bias towards/against particular properties and then we propose a general method for ranking metrics according to their overall bias. We finally demonstrate for 3D medical image segmentations that ranking produced using metrics with low overall bias strongly correlate with manual rankings done by an expert.";"Evaluating the quality of segmentations is an important process in image processing, especially in the medical domain. Many evaluation metrics have been used in evaluating segmentation. There exists no formal way to choose the most suitable metric(s) for a particular segmentation task and/or particular data. In this paper we propose a formal method for choosing the most suitable metrics for evaluating the quality of segmenta-tions with respect to ground truth segmentations. The proposed method depends on measuring the bias of metrics towards/against the properties of the the seg-mentations being evaluated. We firstly demonstrate how metrics can have bias towards/against particular properties and then we propose a general method for ranking metrics according to their overall bias. We finally demonstrate for 3D medical image segmentations that ranking produced using metrics with low overall bias strongly correlate with manual rankings done by an expert.";"100,00";"100,00";"100,00"
"TUW-247743";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\groundtruth\TUW-247743.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\result\result-TUW-247743-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\extracted\grobid\grobid-TUW-247743-xstream.xml"")";"The Self-Organizing Map (SOM) is a useful and strong tool for data analysis, especially for large data sets or data sets of high dimensionality. SOM visualizations map the data model dimensions to visual dimensions like color and position, thus they help exploring the SOM. Visualization can also involve the data itself so that it helps accessing information that are not available in the trained SOM, thereby enabling a deeper look inside the data. If the data comes with supervised class labels, these labels can be also involved in the visualization, thus enabling the user to have a clearer idea about the data and the structures learned by the SOM. In this work we propose a novel SOM visualization method, namely the SOM class coloring, which is based on the data class labels. This method finds a colored partitioning of the SOM lattice, that reflects the class distribution. SOM class coloring helps discovering class information such as class topology, class clusters, and class distribution. Furthermore class labels can be assigned to new data items by estimating the point on the lattice, that best represents the data item and then assigning the class of the partition that includes this point to the data item.";"Quellenstrasse 24B/13/13 A1100 Wien Wien, am 3. Jänner 2016 Unterschrift i Kurzfassung Die Selbstorganisierende Karte (SOM) ist ein nützliches und starkes Werkzeug für die Datenanalyse, besonders für groÿe Datensätze oder Datensätze von hoher Dimensionalität. SOM Visualisierungen bilden die Dimensionen des Datenmod-ells auf graphische Dimensionen wie Farbe und Position ab, so helfen sie der Navigation und dem Erforschen von dem SOM. SOM Visualisierungen können auch die Daten selbst einbeziehen, so dass der Zugri auf Informationen möglich wird, die in einem reinen SOM nicht verfügbar sind. Dadurch wird ein tieferer Einblick in die Daten möglich. Wenn die Daten mit klassen gekennzeichnet sind, können diese Klassen auch in der Visualisierung einbezogen werden, so dass, eine klarere Idee über die Klassinformation gewonnen wird. In dieser Arbeit schlagen wir eine neuartige SOM Visualisierungsmethode, nämlich die SOM Klassenfär-bung vor, welche auf den Datenklassen beruht. Diese Methode ndet eine farbige Partition des SOM-Gitters, die die Klassenstruktur widerspiegelt. Diese Visu-alisierung ermöglicht das entdecken von Klassinformation wie Klassenstruktur, Klassenverteilung und Klassenclusters. Auÿerdem können neue Daten Klassen zugeordnet werden und zwar indem der Punkt auf dem SOM-Gitter ermittelt wird, welcher das neue Datum (Messwert) am besten repräsentiert; das neue Datum wird dann jener Klasse zugeordnet, die die Partition repräsentiert, auf der sich der Punkt bendet. Abstract The Self-Organizing Map (SOM) is a useful and strong tool for data analysis, especially for large data sets or data sets of high dimensionality. SOM visualiza-tions map the data model dimensions to visual dimensions like color and position, thus they help exploring the SOM.";"0,00";"0,00";"0,00"
"TUW-251544";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\groundtruth\TUW-251544.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\result\result-TUW-251544-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\extracted\grobid\grobid-TUW-251544-xstream.xml"")";"Die Wiener Zauberschule der Informatik (WIZIK) führt Kinder der Primarstufe an die Denkweise der Informatik heran und vermittelt ihnen erste informatische Kompetenzen. Die Kinder lernen spielerisch verschiedene Problemlösungsstrategien kennen und erhalten einen ersten Einblick in die Grundlagen logischen und prozessorientierten Denkens.";"Die Wiener Zauberschule der Informatik (WIZIK) führt Kinder der Primarstufe an die Denkweise der Informatik heran und vermittelt ihnen erste informatische Kompetenzen. Die Kinder lernen spielerisch verschiedene Problemlösungsstrategien kennen und erhalten einen ersten Einblick in die Grundlagen logischen und prozessorientierten Denkens.";"100,00";"100,00";"100,00"
"TUW-252847";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\groundtruth\TUW-252847.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\result\result-TUW-252847-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\extracted\grobid\grobid-TUW-252847-xstream.xml"")";"This paper provides an overview of the Retrieving Diverse Social Images task that is organized as part of the MediaEval 2016 Benchmarking Initiative for Multimedia Evaluation. The task addresses the problem of result diversification in the context of social photo retrieval where images, meta-data, text information, user tagging profiles and content and text models are available for processing. We present the task challenges, the proposed data set and ground truth, the required participant runs and the evaluation metrics.";"This paper provides an overview of the Retrieving Diverse Social Images task that is organized as part of the MediaEval 2016 Benchmarking Initiative for Multimedia Evaluation. The task addresses the problem of result diversification in the context of social photo retrieval where images, meta-data, text information, user tagging profiles and content and text models are available for processing. We present the task challenges, the proposed data set and ground truth, the required participant runs and the evaluation metrics.";"100,00";"100,00";"100,00"
"TUW-255712";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\groundtruth\TUW-255712.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\result\result-TUW-255712-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\extracted\grobid\grobid-TUW-255712-xstream.xml"")";"Abstract argumentation is a rich research subfield of AI and till today, numerous frameworks for it have been proposed. It is thus natural to ask whether one can translate between these structures, and what are the price and consequences of undergoing this process. Although every study explains how a given structure relates to the cornerstone of abstract argu-mentation-Dung's framework-there are less results available concerning the connections between more advanced formalisms. Moreover, the existing research is not particularly systematized or classified in a way that would clearly show us the properties of a given transformation. In our work, we address these issues by creating an in-depth compendium on the intertranslatability of argumentation frameworks, describing approximately eighty translations. Furthermore, we provide a system for analyzing a given transformation in terms of its functional, syntactical, semantical and computational properties and the underlying methodology.";"argumentation is a rich research subfield of AI and till today, numerous frameworks for it have been proposed. It is thus natural to ask whether one can translate between these structures, and what are the price and consequences of undergoing this process. Although every study explains how a given structure relates to the cornerstone of abstract argu-mentation-Dung's framework-there are less results available concerning the connections between more advanced formalisms. Moreover, the existing research is not particularly systematized or classified in a way that would clearly show us the properties of a given transformation. In our work, we address these issues by creating an in-depth compendium on the intertranslatability of argumentation frameworks, describing approximately eighty translations. Furthermore, we provide a system for analyzing a given transformation in terms of its functional, syntactical, semantical and computational properties and the underlying methodology. Over the last years, argumentation has become an influential subfield of artificial intelligence, with applications ranging from legal reasoning (Bench-Capon, Prakken, and Sartor 2009) to dialogues and persuasion (McBurney and Parsons 2009; Prakken 2009) to medicine (Fox et al. 2010; Hunter and Williams 2012) to eGovernment (Atkinson, Bench-Capon, and McBurney 2006). Within it, we can distinguish the abstract argumentation approaches, at the heart of which lies Dung's argumentation framework (Dung 1995). Since the structure itself was relatively limited, as it took into account only the conflict relation between the arguments, it inspired the search for more general models (Brewka, Pol-berg, and Woltran 2014). Throughout the years, many of its extensions were proposed, ranging from the ones employing various strengths and preferences to those that focus on researching new types of relations between arguments (Baroni et al.). Such an amount of frameworks should not come as a surprise. Argumentation is a wide area with numerous applications , in which one has to face different classes of problems. Frameworks of a given type can be seen as tools to model particular issues and concepts, which on one side gives us more insight into how to approach the problems, but on the other affects the framework's design. Nevertheless, with so many available structures, it is only natural to ask whether one can translate one framework into another, and what are the price and consequences of undergoing this process. The ability to transform one framework into another is both of theoretical and practical value. The majority of the existing formalisms does not have a dedicated solver. Therefore , a translation into one that does, such as Dung's framework or abstract dialectical framework (Egly, Gaggl, and Woltran 2010; Ellmauthaler and Strass 2014), can facilitate the development of argumentation-based applications. Moreover, if our purpose is to solve a variety of problems for which different frameworks are suitable, translations would allow us to choose the most adequate one to work ""in the background"". Our study can be seen as more research-oriented. The behavior of the semantics and what structural changes a framework has to undergo gives us an insight into how e.g. a given relation between arguments works and how it can or cannot be simulated by other concepts. For example, we can try to transform one form of support into another, support into attack or preference into an argument. However, the ability to perform a conversion is one thing; what is also important is the price we need to pay for it, and by this we do not mean just the computational cost of the process. Depending on how intrusive the modifications are, our source framework can be represented in a way that it is no longer possible to retrieve the original structure from it. We can be forced to assume some structure of arguments, drop or add-possibly exponentially many-elements of the framework. As a result, we can reach a point in which propagating the change in the source structure to the target one can become nearly impossible without repeating the translation altogether. This can make using translations in a dynamic setting quite problematic. Finally, even if we manage to create a non-intrusive, well-behaved translation, it might be the case it is such only for a subclass of the possible source frameworks. Similarly, an intricate transformation can be significantly simplified if certain assumptions are made. Therefore , the efficiency, semantics behavior, structural changes and domain coverage attributed with a given translation can be used to compare both the transformations and different argumentation frameworks. The result of our work is an in-depth compendium on the intertranslatability of argumentation frameworks, consisting of approximately eighty translations. Our focus will be on the Dung's framework (Dung 1995), frameworks with joint attacks (Nielsen and Parsons 2007) and recursive attacks (Baroni et al. 2011), extended argumentation framework and its collective generalization (Modgil 2009; Mod-gil and Bench-Capon 2011), bipolar argumentation frame-work(Cayrol and Lagasquie-Schiex 2013), argumentation framework with necessities (Nouioua 2013), evidential system (Polberg and Oren 2014) and abstract dialectical framework (Brewka and Woltran 2010). We not only propose a number of new approaches, but also complete and, if necessary , correct, the existing ones (Nielsen and Parsons 2007; Oren, Reed, and Luck 2010; Baroni et al. 2011; Cayrol and Lagasquie-Schiex 2009; Nouioua 2013; Brewka et al. 2013; Modgil and Bench-Capon 2011; Oren, Reed, and Luck 2010; Polberg and Oren 2014; Cayrol and Lagasquie-Schiex 2013). As a result of our study, the abstract dialectical frameworks emerge as perhaps the most general structures, capable of handling even the extended argumentation framework, for which the existing results were more limited (Modgil and Bench-Capon 2011). In order to be able to compare our approaches and speak of their quality, we also introduce a classification system for describing a given translation in terms of functional, syn-tactical, semantical and computational properties. These attributes are meant to grasp different aspects of a transformation that we have discussed previously. Furthermore, we identify certain common patterns behind various translations and thus also propose to categorize them with respect to the underlying methodology. Finally, when possible, we use the existing research on semantics signatures (Dunne et al. 2015; Dyrkolbotn 2014) in order to show whether the proposed translations can or cannot be replaced by methods with more desirable semantical aspects.";"0,00";"0,00";"0,00"
"TUW-256654";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\groundtruth\TUW-256654.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\result\result-TUW-256654-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\extracted\grobid\grobid-TUW-256654-xstream.xml"")";"A section is a contiguous region of memory, to which data or code can be appended (like the Forth dictionary). Assembly languages and linkers have supported multiple sections for a long time. This paper describes the benefits of supporting multiple sections in Forth, interfaces and implementation techniques.";"A section is a contiguous region of memory, to which data or code can be appended (like the Forth dictionary). Assembly languages and linkers have supported multiple sections for a long time. This paper describes the benefits of supporting multiple sections in Forth, interfaces and implementation techniques. • You put B in allocated memory. Unfortunately , that usually means that B does not survive a savesystem, and it's also cumbersome if B is a growable structure.";"0,00";"0,00";"0,00"
"TUW-257397";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\groundtruth\TUW-257397.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\result\result-TUW-257397-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\extracted\grobid\grobid-TUW-257397-xstream.xml"")";"Configuration files, command-line arguments and environment variables are the dominant tools for local configuration management today. When accessing such program execution environments, however, most applications do not take context, e.g. the system they run on, into account. The aim of this paper is to integrate unmodified applications into a coherent and context-aware system by instrumenting the getenv API. We propose a global database stored in configuration files that includes specifications for contextual interpretations and a novel matching algorithm. In a case study we analyze a complete Debian operating system where every getenv API call is intercepted. We evaluate usage patterns of 16 real-world applications and systems and report on limitations of unforeseen context changes. The results show that getenv is used extensively for variability. The tool has acceptable overhead and improves context-awareness of many applications.";"Configuration files, command-line arguments and environment variables are the dominant tools for local configuration management today. When accessing such program execution environments, however, most applications do not take context , e.g. the system they run on, into account. The aim of this paper is to integrate unmodified applications into a coherent and context-aware system by instrumenting the getenv API. We propose a global database stored in configuration files that includes specifications for contextual interpretations and a novel matching algorithm. In a case study we analyze a complete Debian operating system where every getenv API call is intercepted. We evaluate usage patterns of 16 real-world applications and systems and report on limitations of unforeseen context changes. The results show that getenv is used extensively for variability. The tool has acceptable overhead and improves context-awareness of many applications.";"100,00";"100,00";"100,00"
"TUW-257870";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\groundtruth\TUW-257870.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\result\result-TUW-257870-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit\MethodDemos\output\extracted\grobid\grobid-TUW-257870-xstream.xml"")";"This position paper describes a critical incident from an early AAL project related to the design decisions made about which features to include. In order to give the older users of a sensor-based telecare monitoring system more tangible value, a number of non-sensor-based interactive services were incorporated into the system which was installed in a residential facility. These services were chosen based on recommendations and input from older people. In the end though, many services were not used and actually contributed to the system being removed from residences.";"This position paper describes a critical incident from an early AAL project related to the design decisions made about which features to include. In order to give the older users of a sensor-based telecare monitoring system more tangible value, a number of non-sensor-based interactive services were incorporated into the system which was installed in a residential facility. These services were chosen based on recommendations and input from older people. In the end though, many services were not used and actually contributed to the system being removed from residences. willing to invest money. They also received some national funding for later development steps.";"100,00";"100,00";"100,00"
