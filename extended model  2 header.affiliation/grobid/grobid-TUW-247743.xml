<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Users\Angela\git\grobid\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="de">
		<encodingDesc>
			<appInfo>
				<application version="0.4.5-dummy" ident="GROBID" when="2017-12-29T00:16+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">MASTERARBEIT Coloring of the Self-Organising Maps based on class labels</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anleitung</forename><surname>Von</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">ausgeführt am Institut für Softwaretechnik der Technischen Universität Wien</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Prof</roleName><forename type="first">Ao</forename><forename type="middle">Andreas</forename><surname>Univ</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">ausgeführt am Institut für Softwaretechnik der Technischen Universität Wien</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rauber</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">ausgeführt am Institut für Softwaretechnik der Technischen Universität Wien</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taha</forename><forename type="middle">Abdel</forename><surname>Aziz</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">ausgeführt am Institut für Softwaretechnik der Technischen Universität Wien</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">MASTERARBEIT Coloring of the Self-Organising Maps based on class labels</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>Visualization can also involve the data itself so that it helps accessing information that are not available in the trained SOM, thereby enabling a deeper look inside the data. If the data comes with supervised class labels, these labels can be also involved in the visualization, thus enabling the user to have a clearer idea about the data and the structures learned by the SOM. In this work we propose a novel SOM visualization method, namely the SOM class coloring, which is based on the data class labels. This method nds a ii colored partitioning of the SOM lattice, that reects the class distribution. SOM class coloring helps discovering class information such as class topology, class clusters, and class distribution. Furthermore class labels can be assigned to new data items by estimating the point on the lattice, that best represents the data item and then assigning the class of the partition that includes this point to the data item.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Quellenstrasse 24B/13/13 A1100 Wien Wien, am 3. Jänner 2016 Unterschrift i Kurzfassung Die Selbstorganisierende Karte (SOM) ist ein nützliches und starkes Werkzeug für die Datenanalyse, besonders für groÿe Datensätze oder Datensätze von hoher Dimensionalität. SOM Visualisierungen bilden die Dimensionen des Datenmod-ells auf graphische Dimensionen wie Farbe und Position ab, so helfen sie der Navigation und dem Erforschen von dem SOM. SOM Visualisierungen können auch die Daten selbst einbeziehen, so dass der Zugri auf Informationen möglich wird, die in einem reinen SOM nicht verfügbar sind. Dadurch wird ein tieferer Einblick in die Daten möglich. Wenn die Daten mit klassen gekennzeichnet sind, können diese Klassen auch in der Visualisierung einbezogen werden, so dass, eine klarere Idee über die Klassinformation gewonnen wird. In dieser Arbeit schlagen wir eine neuartige SOM Visualisierungsmethode, nämlich die SOM Klassenfär-bung vor, welche auf den Datenklassen beruht. Diese Methode ndet eine farbige Partition des SOM-Gitters, die die Klassenstruktur widerspiegelt. Diese Visu-alisierung ermöglicht das entdecken von Klassinformation wie Klassenstruktur, Klassenverteilung und Klassenclusters. Auÿerdem können neue Daten Klassen zugeordnet werden und zwar indem der Punkt auf dem SOM-Gitter ermittelt wird, welcher das neue Datum (Messwert) am besten repräsentiert; das neue Datum wird dann jener Klasse zugeordnet, die die Partition repräsentiert, auf der sich der Punkt bendet. Abstract The Self-Organizing Map (SOM) is a useful and strong tool for data analysis, especially for large data sets or data sets of high dimensionality. SOM visualiza-tions map the data model dimensions to visual dimensions like color and position, thus they help exploring the SOM.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="de">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Analyzing data usually needs more than getting pure statistical properties of the data set. There are many methods for quickly producing overall summaries of a data set. For example, ve-number summary consisting of some statistical values (greatest, median, lower quartile and upper quartile) provides a help understand- ing simple data sets of low dimensionality and limited volume <ref type="bibr">[Kas97]</ref>. Data structure and topology such as nding the clusters and class distribution of the data is very essential. The more data there is available the more dicult it is to understand this data set. Also, the dimensionality of the feature space represent- ing the data is a factor. For this problem there is an essential need of methods for data exploration and especially the methods, that can discover and illustrate eectively the structure of the data. Data mining is one of the elds where such exploratory methods are needed in the form of tools in Knowledge discovery in database (KDD), whose purpose is to nd new knowledge from databases where dimensionality, complexity, or amount of data is too large or complex for pure human observation to be studied. Data mining is now an important area of re- search, responding to the presence of large databases in commerce, industry, and research. Methods of data exploration vary depending on the nature of the data and the goal of study: clustering methods are the more conventional ones; they aim to organize the data patterns into groups, so that patterns in one group are more similar to each other than to those in another group. In other words clus- tering tries to reduce the amount of data items by grouping them. Clustering is useful in several areas such as pattern analysis, grouping, and machine-learning, 1 document retrieval, image segmentation, and many other applications <ref type="bibr">[AJ99]</ref>.</p><p>Projection methods try to reduce the dimensionality of the data: they map the multidimensional data features to low dimensional space (usually 2D or 3D) that represents the data and can be easier visualized and studied. Of course this map- ping should preserve the original data topology as much as possible. If the goal of projection is to visualize the data, then low output dimensionality should be cho- sen in order to achieve meaningful visualizations. Visualization methods aim to map the dimensions of the data to visual dimensions such as position, color, etc.</p><p>in a way that the observer can get a deeper insight in the structure of the data.</p><p>Of course the visual dimensions are limited, so if the data is of high dimensional- ity, direct visualization will be dicult, not helpful or impossible <ref type="bibr">[Ves99]</ref>. In such a case visualization should make use of the two previous methods (Clustering and projection) as a pre-processing step to prepare the data in a form that it can be eciently visualized. Self-Organizing Maps (SOMs) <ref type="bibr">[Koh97]</ref> are ecient tools that implicitly combine all of the previous methods. A SOM is a neural network consisting of low dimensional grid (mostly 2D) of nodes that are trained with high-dimensional data. The nodes on the trained map represent the original data in a manner that similar data points in the feature space are mapped to nodes which are close to each other on the map. SOMs are strong tools used in the data exploration. Some properties that distinguish SOM from the other data mining tools are that it is numerical instead of symbolic, nonparametric, and capable of learning without supervision <ref type="bibr">[Kas97]</ref>; Many variants and extensions of the standard SOM were proposed which try to cover some drawbacks of the standard SOM. In practical applications the process of deciding which method to use is essential: this depends on the nature of the data and the goal of ex- ploration; the following questions are in any case to be resolved: What kind of structure the method can extract?; How does it illustrate this structure?; Does the method reduce the data dimensionality and/or the number of data items?;</p><p>Which and if so which type of data pre-processing is necessary?. The focus of this work is visualization of the SOM, in particular SOMs that are trained with labeled data. It is meaningful here to distinguish between two things that are of great importance in this work: SOM is based on an unsupervised learning process in the meaning that no prior class information is needed to perform the training, because the learning is data-driven. Regardless of this fact, class information can be used in the form of labels which can be then used for exploration and visualization purposes; such labels can be collected in a pre-processing step, or even as a post-processing step. These labels do not aect the training process and the resulting structure or topology, but they help in generating visualizations displayed on top of the nished map. In particular we propose in this work a visualization method of SOM which is based on such class labels namely a SOM coloring that depends on these labels.</p><p>The remainder of this thesis is structured as follows: In Chapter 2 we will dis- cuss some related concepts and methods such as the concept of the Self-Organizing Map, some clustering and classication methods and methods for projection and visualization of the trained SOM. In Chapter 3 we present our novel method for coloring the SOM based on class labels. In Chapter 4 we test and evaluate our method with some collected data sets. Finally we present our conclusions in Chapter 5.</p><p>Chapter 2</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Work</head><p>In this chapter we will introduce some of the concepts and worksljubíme that have been proposed which are closely related with the underlying issue, coloring of the SOM. In Section 2.1 the general issue of the SOM will be introduced.</p><p>In Section 2.2 and 2.3 we will give some idea about the two processes, which are closely related to visualization: these are clustering and projection. In Sec- tion 2.4 visualization will be discussed briey especially in respect to the SOM, Some SOM-based clustering, projection and visualization methods will be also discussed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Self organizing map</head><p>The Self-Organizing Map (SOM) in the basic form is an articial neural network model. The nodes of this network are trained to various input patterns from the feature space. SOMs use an unsupervised learning process: no a priori clas- sication for the input is needed. The result of the learning process is a map lattice (in this work we assume a 2 dimensional lattice, which is usually the case ), that map the high-dimensional feature space of the input in a way, so that similar inputs in the feature space have associated nodes that are close to each other on the map lattice, thus reducing the high-dimensionality of the feature space. This projection is capable (a) to help clustering the data and (b) to ap- proximately preserve the original data topology of the input. SOMs are therefore especially useful for data visualization and exploration purposes. I a classic SOM, </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1">The map initialization</head><p>If the map contains tens of thousands of neurons then it usually takes too much time to train the system for the practical tasks. Thus making choice of the nodes quantity requires a reasonable trade-o. Before training the map it is necessary to initialize the weight coecients of the neurons. The learning rate factor can be signicantly increased by an appropriate initialization method, thus bringing better results. In general, there are three methods of weights initialization:</p><p>• Initialization with random values. All the weights are assigned small ran- dom values.</p><p>• Initialization with patterns. Initial values are set to randomly chosen pat- terns of the training sample.</p><p>• PCA-based initialization. The weights are initialized by values of the vec- tors ordered along a two-dimensional subspace spanned by the two principal eigenvectors of the input data vectors. <ref type="figure" target="#fig_2">Figure 2</ref>.1: Adapting the best-matching unit and its neighbors weights The input vector coordinates are marked with a cross, coordinates of the map nodes before modication are shown as full circles and after modication as empty circles.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2.1.2</head><p>The training process</p><p>The learning process consists of sequential corrections of the vectors representing neurons. On every step of the learning process a random vector is chosen from the initial data set and then the best-matching neuron coecient vector (up now BMU or best-matching unit) is identied. This is the most similar unit to the input vector. The word 'similarity' in this task means the distance between the vectors. There are various distance metrics that can be used for this purpose.</p><p>The most common one is the Euclidean metric, where the the winner unit must meet the following relation:</p><formula xml:id="formula_0">− W c = min i − W i (2.1)</formula><p>Where W c is the winner vector at index c, X is the input vector and W i is any Unit vector in the SOM. After the BMU is found the neural network weights are adapted. The winning unit and its neighbors adapt to represent the input by modifying their reference vectors towards the current input. </p><formula xml:id="formula_1">W i (t + 1) = W i (t) + h ci (t) * [X(t) − W (t)] (2.2)</formula><p>where t is the discrete time index. Vector X(t) is a randomly selected sample at iteration t. The function h(t) is the neighborhood function: </p><formula xml:id="formula_2">h(t) = h( c − r i t) * α(t</formula><formula xml:id="formula_3">h(d, t) = const for d ≤ σ(t) 0 for d &gt; σ(t) (2.4)</formula><p>or Gaussian function: </p><formula xml:id="formula_4">h(d, t) = e</formula><formula xml:id="formula_5">α(t) = A t + B (2.6)</formula><p>where A and B are constants. There are two main phases in the learning pro- cess. At the beginning the learning rate and the neighborhood radius are fairly large, which allows ordering the neurons vectors according to the sample patterns distribution. Then the weights are accurately adjusted with the learning rate pa- rameters much smaller than they initially were. In case of using PCA-based initialization the rst step of rough adjustment can be skipped.</p><p>The previous training algorithm, which was described formally is now sum- marized in the following steps:</p><p>1. Initialize the weights of each node .</p><p>2. Chose a sample vector randomly from the set of training data.</p><p>3. Every node is examined to nd the best-matching unit. <ref type="bibr">4</ref>. The radius of the neighborhood of the BMU is now calculated. This is a value that starts large, typically set to the 'radius' of the lattice, but diminishes each time-step. Any nodes found within this radius are deemed to be inside the BMU's neighborhood.</p><p>5. Each of neighboring node's weights are adjusted to make them more like the input vector. The closer a node is to the BMU, the more its weights get altered.</p><p>6. Iterate from step 2 for N iterations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Clustering and classication</head><p>Cluster analysis is the organization of a collection of patterns into clusters based on similarity among these patterns. That is patterns belonging to a cluster are (more) similar to each other to than those in another clusters. There are many clustering methods, however it is useful at this point to distinguish between supervised and unsupervised methods: in the case of supervised classication (discriminant analysis) a collection of already classied patterns is provided and the problem is to classify a new pattern. In the case of unsupervised classication (clustering) a collection of unclassied pattern is provided and the problem is to group this collection into meaningful clusters, but this process of labeling is data driven and not as according to a given class structure as in the case of supervised classication. The clustering process typically consist of the following steps:</p><p>Pattern representation, pattern measure denition, grouping, data abstraction.</p><p>1. Pattern representation: this step involves preparing the input data and nding important information such as the number of available patterns and the number, type, and scale of the features involved in the clustering.</p><p>Furthermore a subset of features may be selected to be the most important subset of the original feature set. This process is called feature selection.</p><p>If the features are in form that is not suitable for clustering, one or more transformation could be used to put the features in an appropriate form to start clustering.</p><p>2. Pattern proximity measure denition: this is the distance function, that is applied on pairs of patterns to measure the similarity between the two patterns. There is a variety of distance measure functions. The simplest one may be the Euclidean distance.</p><p>3. Grouping: this is the process of partitioning of the date into groups. The partitions can be hard, where a pattern can belong to only one partition and it can be fuzzy, where a pattern can have membership degrees in many clusters. Furthermore there are two conceptual clustering algorithms: hier- archical algorithms, that produce many nested partitions based on similar- ity according to a criterion for merging or splitting clusters and partitive clustering algorithms, that identify the partition that optimizes a clustering criterion. Clustering algorithms will be discussed in the next sub sections in more details.</p><p>4. Data abstraction: is the process of extracting a simple and compact repre- sentation of a data set. In the clustering context, a typical data abstraction is a compact description of each cluster, usually in terms of cluster proto- types.</p><p>To have optimal clustering, there are many parameters to be tuned. This depends also on the purpose of the clustering. We note, that the purpose is usually not to nd an optimal clustering of the data, but to have a clustering that enables an optimal and ecient exploring the data considering speed, robustness and the ability of visualization [AJ99].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">Hierarchical algorithms</head><p>Hierarchical clustering approaches can be divided into agglomerative and divi- sive algorithms, corresponding to bottom-up and top-down strategies, to build a hierarchical clustering tree. Agglomerative algorithms are more commonly used than the divisive methods and usually have the following steps: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2">Partitive algorithms</head><p>In this approach the algorithm identies the partition by minimizing some error function. The number of clusters is either already predened or determined by the algorithm by trying various numbers of clusters. In general partitive algorithms consist of the following steps:</p><p>1. Determine the number of clusters.</p><p>2. Initialize the cluster centers.</p><p>3. Compute partitioning for data.</p><p>4. Compute (update) cluster centers.</p><p>5. If the partitioning is unchanged (converged), stop; otherwise, return to step 3.</p><p>The most common partitive clustering algorithm is the k-means algorithm, which partitions N data points into K disjoint subsets S j containing N j data points.</p><p>The algorithm minimizes the sum-of-squares criterion E:</p><formula xml:id="formula_6">E = Σ K j=1 Σ n∈S j ||x n − c j || 2 (2.7)</formula><p>where x n is a vector representing the nth data point and c j is the geometric centroid of the data points in S j .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.3">Two-level clustering approach / Clustering of the SOM</head><p>There are two main problems in the standard clustering algorithms mentioned in the previous sub sections above: the rst problem is the high computational costs when applying the algorithm directly on the original data, especially if a large collection of patterns underlies. The second problem is the sensibility to noise: applying the algorithm to original data makes it sensible to the occurrence of noise and outlier because this could make clusters overlap or have inaccurate boundaries. Both of these problems can be solved by using the two-level clus- tering approach: The clustering algorithm is not applied directly on the original data but rather on cluster prototypes, that are generated by a previously trained SOM. SOM nodes are considered as cluster prototypes and are grouped into clusters by applying a suitable clustering algorithm. The number of the cluster prototypes (nodes) must be much larger than the expected number of clusters but much smaller than the number of the data patterns. The computational costs is dramatically reduced especially in case of hierarchical algorithms. This is because the SOM algorithm can be applied to large data sets and the compu- tational complexity is linearly proportional with the number of input patterns, but on the other hand, the complexity scales quadratively with the number of map nodes (prototypes). However this overhead can be solved with special op- timization techniques. The problem of noise and outliers is also solved because the prototypes are the local averages of the original data produced by training the SOM und thus less sensible to outliers and nodes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Projection</head><p>The structure of the data cloud formed by prototype vectors is the key for un- PCA is a linear transformation that transforms the data to a new coordinate system such that the greatest variance by any projection of the data comes to lie on the rst coordinate (called the rst principal component), the second greatest variance on the second coordinate, and so on. PCA can be used for dimensionality reduction in a dataset while retaining those characteristics of the dataset that contribute most to its variance, by keeping lower-order principal components and ignoring higher-order ones. PCA is an optimal linear projection algorithms, because it has the minimum mean-square-error values between the original data vectors and the points in the projection, thereby it achieves the equation 2.8:</p><formula xml:id="formula_7">min Σ T m T i=1 [x i − Σ j=1 (q j x i )q j ] 2 (2.8) Where X = [x 1 , x 2 , .., x n ]</formula><p>T is the n-dimensional input vector. {q j , j = 1, 2, ...m, m ≤ n} are orthogonal vectors representing principal directions. The term q T j repre- sents the projection of x onto the j-th principal dimension.</p><p>PCA has limited power to project data of high dimensionality, and thus it has limited power for capturing nonlinear relationships in practical data. For this reason extensions to nonlinear PCA are used such as Generalized PCA, Kernel PCA, and principal curves and surfaces <ref type="bibr">[Yin03]</ref>.</p><p>Multidimensional scaling (MDS) is a nonlinear projection method, that tries to project points of data onto a two-dimensional plot, thereby preserving as close as possible the inter-point metrics. For this purpose arbitrary optimization algorithms can be used, MDS does not specify that a certain algorithm to be used. In the optimization process local minima and divergence problems may occur. Furthermore, this process incurs high commotional costs. These costs depend on the selected algorithm and the starting conguration. The overall structure of the data is maintained thereby.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.1">Sammon's Mapping</head><p>Sammon's mapping is a common example of MDS projection. This algorithm aims to minimize the dierence between inter-point distances in the original data and in the projection. The Sammon's mapping algorithm minimizes the stress function 2.9.</p><formula xml:id="formula_8">2 S sammon = 1 Σ [d ij − D ij ] n Σ n i&lt;j (2.9) i&lt;j d ij d ij</formula><p>where X i and X j are two points in the input space and Y i and Y j are their projection points respectively, d ij is the Euclidean distances between X i and X j , D ij is the Euclidean distance between Y i and Y j , and n is the number of patterns.</p><p>It performs a recursive learning algorithm using the Newton optimization to achieve the optimal conguration <ref type="bibr">[Sam69]</ref>. Although this algorithm converges relatively fast, it has the drawback of high computational costs and higher risk of sub-optimal solution due to local minima. Furthermore Sammon mapping is a point-to-point mapping which means that all the inter-point distances must be explicitly calculated for every data set. In the broader since this means that there is no explicit mapping function. Accommodation of new data point needs recalculation of all points, which is a serious problem for applications that deal with sequential data <ref type="bibr">[Yin03]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Visualizations</head><p>Data visualization is an important process in data mining. Generally, to visual- ize multi dimensional data, two steps are needed: the rst is vector quantization, where the original data set is reduced to a smaller one, that still represents the original, suppresses noise and is easier to work with than the original set. The second is a vector projection to a low dimensionality (usually up to 3D). Of course in our context this projection must still represent the original vector set in terms of distances between vectors or at least topological ordering. Visualization is the   This problem is solved by using smoothed data histograms(SDH) proposed in <ref type="bibr">[PRM02]</ref>. SDH aims to visualize the clusters on the SOM by taking into account the probability density of the high-dimensional data. The SOM is used as a basis for the visualization. Instead of assigning a data sample to a specic unit, the SDH estimates a membership degree of every data sample to each unit. This membership is calculated based on distances between the data sample and all other units. The eect of these distances in the membership is controlled with a smoothing parameter s. The Data set consists of 5000 samples, that are randomly drawn from a probability distribution, that is a mixture of 5 Gaussians. The SOM is of 10X10 units arranged on a rectangular grid.</p><p>more details are visualized but not the overall cluster topology. By increasing of the value of s, the general characteristics of the data become more visible.</p><p>Vector eld visualization A method of visualizing the cluster structure based on vectors is proposed in <ref type="bibr">[PDR]</ref>. The vectors are drawn on top of the SOM lattice in the form of arrows having one vector per unit. The length and direction of the vectors are determined so that they give information about the cluster structure. The directions of arrows point to the location of cluster centers and the lengths visualize the location of the unit inside the cluster in term of being a boundary unit or not: long arrows indicates that the unit is located closely to the cluster boundary while short arrows means in general that the unit is located closely to the cluster center or between two clusters to which the unit have equal similarity. In particular, the length of the arrow demonstrates the ratio between the dissimilarity of the unit to the area the arrow is pointing to and the dissimilarity of the unit to the area the arrow is pointing from. This visualization is similar to the U-Matrix  and individual regions can be distinguished with σ = 5. In <ref type="figure" target="#fig_2">Figure 2</ref>.6 -(c), the global structure is shown for σ = 15.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.2">Visualization of SOM with Labeled data</head><p>All of the visualizations presented in the previous sections either only make use of the SOM units to visualize the data set or access the data vectors to make calculations that help by visualizing the data topology. In this subsection, we will present two visualization methods that assume the availability of labeled data and uses these (class) labels to produce a visualization, which shows the class topology of the data.</p><p>Graph-based class visualization 1. isolated data units for which all neighbours through the graph have a class dierent from its own.</p><p>2. border for which there is at least one of its neighbours, which has the same class as its own.</p><p>3. normal for which all neighbours have the same class as its own.   connected components of G2 and G4, are indicatd beside nodes (edges) of G3 and G5. The class graph G3 shows the topology of the classes and the way they are connected and the density of the connections between the dierent components, where two classes are drawn with their densities and topology; the class and quality graph G5 provides the same of information as in G3 in addition to the visualization of border and isolated components. The purpose is to obtain a visualization which 1. reect the class topology of the data set.</p><p>2. help assigning new data items and labeling them with one or more classes.</p><p>. The rst step in the SOM Class Coloring is assigning the labels to the SOM nodes; this is a straightforward step. In this work we assume that this assignment is given and we don't take this step in more details.</p><p>This chapter is structured as follows: In Section 3.1 the SOM Toolbox is de- scribed in more details, which provides the base conguration for the SOM Class Coloring. Section 3.2 describes the problem formally. In Section 3.3 we will describe one of the tries to achieve the coloring by letting color point ood in all directions. Section 3.4 introduces the use of graphs for coloring the SOM by 25 discussing some theoretical issues like voronoi diagrams and Delaunay triangula- tions which are important for our coloring method and we will then introduce the attractor functions which provide the key algorithms for achieving the smoothed class coloring.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">SOM Toolbox</head><p>Although the trained SOM itself can be used to explore the data, there are infor- mation that cannot be visualized with the pure trained SOM; some information is always lost through dimension mapping which cannot hold the exact topology and all data relations. Many interfaces and tools were proposed which visual- ize the SOM and try to cover the drawback caused by lost information through mapping.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Problem formulation</head><p>In this section we will give a formal description of the problem.</p><p>Let C = {c 1 , c 2 , ..c n |n ≥ 0} be the set of classes involved in the data set and U = {u 1 , u 2 , ...u m |m ≥ 0} the set of units in the SOM. We assume that the assignment relation A is known which is a function f : U → R n that assigns the classes of the data to the unit in which these data items are represented by, as well as the fractions of contribution for each class, that is f (u) = {a 1 , a 2 , ...a n |a ∈ R} where a i is a real number giving the contribution of class c i in the unit u; for example if we assume 5 classes and f (u) = {0.0, 0.25, 0.5, 0.25, 0.0} that means that the following data classes and their contribution fractions are assigned to the unit u: 25% from class c2, 50% from class c3, 25% from class c3 and 0.0 (or no data) of the other classes.</p><p>We assume that the units are located on a SOM grid of the dimensions p × q where p is number of rows and q is the number of columns which implies a number of units m = p · q.</p><p>The problem is to color the grid plane in such a way that the class topology of the data is visualized and that, according to this coloring, a new data item can be assigned to a class. <ref type="figure" target="#fig_18">Figure 3</ref>.2 illustrates this base conguration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">SOM coloring by color ooding</head><p>The rst try to color the SOM was to let colored points ood, that is if we imagine the units as colored points and that the colors spread at the same time, we will get a colored map.</p><p>At this point we will try to dene and determine the process of ooding the color; there are several approaches how the ooding of a set of color points can be simulated: the rst one is that in each iteration a point grows to a ring more. The second is that in each iteration a point grows to one graphic grid (for example a pixel) more in this case the question arises which pixel: we can take the next one at the boundary or a randomly selected pixel from the boundary; this process continues as long as the reached areas are not occupied by other ooding points.</p><p>In the simplest case if each unit has one color (single class), things will go  In fact this was too simple case: the main diculty in this issue is that a unit can contain data of many classes, that is it can have many colors. In such cases a direct color ooding can not produce a coloring that meets our expectation.</p><p>To solve this problem we suggest the following concept: a unit with more than one class is substituted by a number of points equal to the number of classes, and these points are located closely to the original unit; then they are rotated round the original point so that they are arranged according to the classes at the neighboring units, that is we thereby aim to have neighboring classes as similar as possible as it is in <ref type="figure" target="#fig_18">Figure 3</ref>.4 illustrated. When all units are substituted, we let them ood as in the last example. We note here that this concept is not complete because it doesn't take into account the values of contribution of the classes, but it considers as if all classes were equally involved. That means if this concept works well for the rst step, it must be then extended to consider the contribution values.</p><p>We implemented the suggested concept and tested it and we found, that it   works with critical drawbacks especially the low accuracy and stability: a slice change in the locations of the substitution points could result a dramatic change in the resulting coloring. The random positioning could produce a dierent result for every run. We analyzed the issue and found that it is caused by a starvation eect which occurs as the following: in one case the ow of an isolated color is stopped by the ow of the surrounding ones; in the other case if a slice change is done, the ow of the isolated color nds a way outwards and grows dramatically more eventually in a dieren region. See <ref type="figure" target="#fig_18">Figure 3</ref>.5. This eect can also happen without making changes in the positions, but only by changing the order in which the pixels are occupied(for example by randomly selection). The drawbacks seemed for us to be in the nature of the concept and thus we stopped to search in this direction. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Using graphs to color the SOM</head><p>Our next try was to use graphs segmentation for the visualization instead of color ooding. In this case the decision on Voronoi diagrams is straightforward, because it can produce a similar result like that of ooding the colors in <ref type="figure" target="#fig_18">Figure 3</ref> However we will face the same situation as in color ooding: After the voronoi areas are found, a solution must be found which considers units with many classes (colors) that have dierent ratios. Before we come to this issue, the Voronoi Diagrams are discussed in the following subsection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.1">Voronoi diagrams and Delaunay triangulations</head><p>Voronoi diagram <ref type="bibr">[For97]</ref> of a set of points (n points here called sites) located on a plane is the diagram that partitions this plane into exactly n regions, so that each Voronoi region is assigned to a site and consists of all points on the plane that are closer to this site than to any other site. More formally: Given a set of sites S = {s 1 , s 2 , ..s n } on a plane, then the Voronoi region R of a site s i is • Each vertex of Voronoi diagram is a center of a circumsphere of a triangle in the DT.</p><p>• Every line segment connecting two sites in two neighboring voronoi regions (two regions with a common edge) is an Edge in the DT.</p><p>See The complexity of the problem depends on the used algorithm; however the most optimal algorithms have a time complexity of O(n log n) such as the sweepline algorithm.</p><p>In this work we will describe two of these algorithms: the sweepline algorithm and the Bowyer-Watson, which was also implemented in the SOM Toolbox.</p><p>The sweepline algorithm</p><p>The sweepline algorithm was rst introduced by Steven Fortune in <ref type="bibr">[For86]</ref> and is often referred to as Fortunes Algorithm. This algorithm has complexity O(n log n), which is optimal for this problem. This algorithm is well-suited to streaming techniques; points are processed in sorted order and potential edges are main-   The randomized-incremental algorithm (Bowyer-Watson algorithm)</p><p>The randomized-incremental (RE) <ref type="bibr">[For86]</ref> algorithm in general adds the sites se- quentially to the Delaunay triangulation updating the triangulation after each addition; for this update three steps are needed, as in <ref type="figure" target="#fig_18">Figure 3</ref>.9: the rst step is discovering all triangles that are aected by the addition, which are the triangles whose circumspheres contains the added site; the second step is deleting all tri- angles discovered; the third step is the rebuilding of the empty region as a result of the deletion taking into account the new site, so that every new triangle has the new site as a vertex. The eciency of the algorithm depends on the eciency of the used algorithm for discovering faces to be deleted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.2">Voronoi cell coloring</head><p>In the case of single-class nodes, each voronoi cell can be assigned the according class color. As already said in previous chapters the diculty of our problem is that a SOM unit could have to many classes assigned. If the units were assigned to single classes, then coloring the voronoi regions with the corresponding colors should be the optimal solution of the problem.</p><p>In this section we will at rst describe some of the solutions for this problem, which we tried but have failed because of some reasons. Then we will intro- duce in the next subsections two methods that solve the problem and meet our expectations.</p><p>We started by a solution that is based on node substitution similar to the substitution described in <ref type="figure" target="#fig_18">Figure 3</ref>.3 and <ref type="figure" target="#fig_18">Figure 3</ref>.4,where each node is substituted with n sites (substitution set) where:</p><p>1. n is the number of classes assigned to the node and the sites.</p><p>2. each site is assigned a class. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.3">Angular segmentation of Voronoi cells</head><p>We gave up the substitution and tried another method, namely to divide the voronoi region into sectors similar to a pie chart. Each of them is dened with an angle. The angles are to be calculated depending on the sharing value of the class and on the neighboring classes as depicted in <ref type="figure" target="#fig_18">Figure 3</ref>.11. The method solves the drawback in the substitution method which causes a global topology change if the sites are moved. But there is another problem or shortage with this solution: we consider the case in <ref type="figure" target="#fig_18">Figure 3</ref>.11, for input like in (A) the described method would produce a coloring like in (B). If we let a class have more than one sector, then the resulting coloring could be like (C). Actually, in this case the distribution of the classes seems like a red region, that spread over three Voronoi <ref type="figure" target="#fig_18">Figure 3</ref>.11: Sector partitioning.</p><p>cells. However what we would like to have is something like the coloring in (D).</p><p>Also in case of an isolated class, we would like to have it as an isolated area in the middle of the Voronoi cell and not as a sector. Such a smoothed coloring can not be reached any using the sectoring method.</p><p>So we stopped our investing in this direction and searched for other methods.</p><p>We found two methods, that were satisfying: the rst is assigning every grid unit </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.4">Smooth partitioning of Voronoi cells</head><p>In this section we will introduce a method for producing a smoothed partitioning of the voronoi regions into class (color) areas.</p><p>At rst let us dene some notations to help expressing the problem and our algorithms to solve them.</p><p>• R = {r 1 , r 2 , ..r n |n ≥ 0} is the set of voronoi regions which is drawn on top of the SOM lattice as described in Section 3.4.1 with the units as faces.</p><p>• C = {c 1 , c 2 , ..c m |m ≥ 0} is the set of all classes that exist in the data set.</p><p>• N (r i ) = {r i 1 , r i 2 , ...} is the set of the neighbor regions of region r i . These are regions that share r i with an edge. • C(r i ) = {c i 1 , c i 2 , ...} is the set of classes share in the unit located at the face of r i .</p><p>• S(r i , c j ) ∈ R + is the value of the sharing of the class c j in the region r i .</p><p>• Furthermore we dene the term connection line which is an imaginary line that connects the site of a region r i with the middle of the edge of a neighboring region r n if there is a class c where c ∈ C(r i ) and c ∈ C(r n ). In other words this line denotes that a region has a class that also the neighboring region, to which the line is connected, has, as shown in A key issue in this method is a function for assigning pixels to classes according to a distance function we will call it attractor f unction because it produces an eect which is similar to the magnet eect, this function is applied on a region, a line segment, a class and a number n that denote the number of pixels to be assigned.The function works as follows:</p><p>1. At rst all pixels (or a predened number of pixel blocks in case lower granularity representation is desired for performance improvement ) in the The work of attractor function: In region r1, at rst the 34 grids, that are most close to L1, are colored in red then the next nearest 45 grids are colored in yellow. The 41 grids most nearest to L5 are colored in blue, also 14 grids from r4 that are nearest to L5 from the other side, are colored in blue. that gives the eect of an area extending over two regions In regions r2 and r3 the same thing is done with L2 and L3, because L2 and L3 have a common point the resulting colored area appears as a thin area, that extends over two regions In r4 the line L4 has the length 0 (is equivalent to a point), thus the resulting colored area has a circular shape.</p><p>region are sorted according to their distance to the line segment. The distance here is dened to be the distance to the closest point that lies on the line segment(Note that this distance is dierent from the commonly known distance which measures the length of the normal).</p><p>2. After all pixels are sorted, the rst n pixels (the nearest ones to the line seg- ment), that are not yet assigned, are assigned to the given class. <ref type="figure" target="#fig_18">Figure 3</ref>.13</p><p>shows this functionality.</p><p>The eect of this function is that line segments (we will call them attractors)</p><p>attracts the pixels of a specic color toward them. Depending on the position of this line segments in the Voronoi cell, the length, and direction of the segment, dierent attractors are obtained, we list some them:</p><p>• Edge of the Voronoi cell: applying the function on an edge of the region lets the color go to the border, now if the function is also applied to the same edge but from the other side (in the neighboring region), then the resulting area reects a cluster that extends over two regions. see the attractor L5</p><p>in <ref type="figure" target="#fig_18">Figure 3</ref>.13 applied in r1 and r3; this can be applied if two neighboring regions share the same class.</p><p>• In the remainder of this section we will describe the algorithm for the attractor function and then we will describe some rules for nding the ref erence lines de- pending on the classes that are present in a region and in it neighboring regions, so that the resulting coloring reects the class topology of the data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Distance Function</head><p>The underlying distance function measures the distance between a point (grid center) and a line segment. In particular it measures the distance d from a point P to the closest point on the line segment P 0 P 1 .</p><p>Only three cases that can occur:</p><p>1. If P lies on the line segment, then d = 0</p><p>2. If the perpendicular line from P on the line −−→ P 0 P 1 intersects the line segment P 0 P 1 , then the searched distance is the normal distance.</p><formula xml:id="formula_9">d = |P ⊥ −−→ P 0 P 1 | 3.</formula><p>When P lies at one of the sides of the segment line, then the searched distance is the shorter of the distances between the point and the segment end points; that is d = min{|P P 0 |, |P P 1 |} <ref type="figure" target="#fig_18">Figure 3</ref>.14: Distance between a point and a line segment <ref type="bibr">[Sof]</ref>.</p><p>An easy and sucient way to nd the distance without measuring unnecessary distances is to consider the angles between the segment P 0 P 1 and the vectors P 0 P and P 1 P from the segment endpoints to P . See <ref type="figure" target="#fig_18">Figure 3</ref>.13. If either of these angles is 90 o , then the corresponding endpoint is the perpendicular base P (b).</p><p>Otherwise, if the angle is not 90</p><p>• , then the base lies to one side or the other of the endpoint according to whether the angle is acute or obtuse. These conditions are easily tested by computing the dot product of the vectors involved and testing whether it is positive, negative, or zero. The result determines if the distance should be computed to one of the points P 0 or P 1, or as the perpendicular distance to the line itself <ref type="bibr">[Sof]</ref>. See <ref type="figure" target="#fig_18">Figure 3</ref>.14.</p><p>Note that the two tests can be done just using the two dot products W 0 · V and V · V which are also the numerator and denominator of the formula to nd the parametric base of the perpendicular from P to the extended line L of the segment S <ref type="figure" target="#fig_18">Figure 3</ref>.14. This leads to the following algorithm as shown in the pseudocode:</p><formula xml:id="formula_10">distance(P, P 0 P 1 ){ − → V = P 1 − P 0 − → W = P − P 0 if ((c1 = − → W · − → V ) &lt;= 0)</formula><p>return distance(P, P 0)</p><formula xml:id="formula_11">if ((c2 = − → V · − → V ) &lt;= c1)</formula><p>return distance(P, P 1)</p><formula xml:id="formula_12">b = c1/c2 P b = P 0 + bv return distance(P, P b) } Coloring algorithm</formula><p>The underlying algorithm for coloring assumes that there is a SOM lattice with • Finding a dominant class for each region and coloring the region with its color. The dominant class is not the class with largest fraction, but the class having the most number of neighbor regions, which have the same class. If the number is equal for several classes, then the dominant class is selected randomly from these.</p><p>• For the remaining classes, line segments are found depending on the classes in the region and in the neighborhood. The attractor function is then ap- plied on these line segments, thereby overwriting the dominant class color.</p><p>The algorithm for the rst step is straight forward: a simple iteration through the neighbor regions and counting the occurrence of the underlying class. If this is done for all classes in the region, then the one with the most hits is selected.</p><p>In the second step, the main issue is how to nd the best suitable line segments for applying the attractor function, so that the resulting coloring reects the data class topology as faithfully as possible. In the rest of this subsection, we will describe and improve an algorithm for solving this problem.</p><p>We consider four cases, that can occur concerning the relation between the class c being processed in the region r with the sharing fraction s and between the classes c i in the neighboring regions r n . We also assume that the region r contains p pixels. The following attractor types can be applied:</p><p>1. The class c is isolated: there is no neighboring region r ∈ r n that contains data of the class c. This is the simplest case because we do not need to consider a cluster that extends to other regions for this class. Thus we use the point attractor by adding a segment with length 0 (a point) in the site location and assign p * s pixels for the class c by applying the attractor function for this class and this line segment. See <ref type="figure" target="#fig_18">Figure 3</ref>.15 -A 2. There is only one neighboring region r n that contains the class c: in this case we try to attract the colored pixels at the region border close to r n .</p><p>Thus we add a segment line from the site location to the middle point of the common edge e (the edge between r and r n ) and apply the attractor function on this segment with p * s pixels. See <ref type="figure" target="#fig_18">Figure 3</ref>.15 -B</p><p>3. There are two neighbor regions r 1 and r 2 that have the class c and that are themselves neighbors to each others. This implies that r,r 1 ,r 2 all shares a vertex v. In this case we use the point attractor by add a line segment of length 0 to the point v (the common vertex). We do that hoping that when all of the three regions concentrate the color at this point, then we have a colored cloud extending over the three regions. See <ref type="figure" target="#fig_18">Figure 3</ref>.15 -C 4. There are more than one neighbor region {r 1 , .., r n |n &gt; 1} that have the class c but none of them is a neighbor of the other: we treat each of these regions similar as in case 2 with the dierence that the number of pixels assigned is (p * s)/n that is we add n line segments, one for each region from the site to the middle point of the corresponding common edge. See The class yellow in the region r remains after painting the dominant colors. There is one neighbor, that have the same class which is r1, thus we add a segment s from the site to the middle of the common edge. The same thing happens when painting r1, from the view of r1 there is a neighbor region r that have the same class yellow, thus adding the line segment s1. After painting r and r1 we get continuous yellow area extending over the two regions. (C)In this case the two neighbor regions r1 and r2 are self neighbors and shares r with the vertex at s because all of the three regions have the class cyan, we add the line segment s with length 0 at the common vertex, so that the resulting class cluster is continuous for all of the three regions as it is seen in the nished coloring. (D)Because r have two neighbors of r1 and r2, that have the same class yellow, but they are not self neighbors, we add two segments s1 and s2 each one from the site location to the middle of the edge of the corresponding region. After nishing, we have a yellow stripe, that connect r1 and r2 crossing r.</p><p>boring regions have the same color but they do not have smooth bordering. In particular, this occurs if a class has dierent contributions in the two neighboring regions or if the regions are of dierent areas, so that the the border lines at the common edge have a zigzag form. See <ref type="figure" target="#fig_18">Figure 3</ref>.16 -A. To solve this problem we use a weighted comparator for the sorting process needed in the attractor algorithm described in 3.4.4. Recall that the pixels are sorted according their distances to a line segment and then they are assigned to the class in this order.</p><p>The weighted comparator uses a weighted distance metric for nding the the distance to the line segment. That is, the line segment is assigned two weights for the two ends and the weights are taken in account in measuring the distance:</p><p>If p is the point to be measured and p 1 p 2 is the line segment then the weighted distance is:</p><formula xml:id="formula_13">d w (p, p 1 , p 2 ) = d(p, p 1 , p 2 ) + |pp 1 |(1/w 1 ) 2 + |pp 2 |(1/w 2 ) 2 (3.1)</formula><p>where d(p, p 1 , p 2 ) the normal (not weighted) distance and w 1 , w 2 the weights for the line segment ends p 1 and p 2 respectively. The weighted distance results the eect that pixels tends to be concentrated more around the end point with the larger weight. Now if we have two neighboring regions having the same class like For this reason we dened a parameter, the minimum visible class, which can be set by the user to values from 0 to 100%. This parameter denes the minimum contribution, that a class must have in order to be painted. Setting the parameter to 0 will cause that all details are painted; while setting the parameter to 100%</p><p>will cause that only the dominant classes are painted. setting the value to any other value will cause that only these classes are painted, that have a contribution equal or greater than this parameter. Note that the eect of this parameter is applied only on classes other than the dominant class, because the dominant class is painted in all cases as a background, which forms a base for painting the other classes. <ref type="figure" target="#fig_18">Figure 3</ref>.17 shows a comparison between three visualizations, with the parameter set to 0 in the rst and to 50% in the second, and to 100% in the third.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Chessboard visualization</head><p>In this section we describe another alternative for visualizing the regions with multiclass contribution. In this visualization every region r is divided into grids.</p><p>For every class c with contribution f in r, a number m = n * f grids is calculated, where n is the total number of the grids in the region. Now the pixels are are The same data visualized with the parameter set to 50% and (C) set the parameter set to 100%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Summary</head><p>To visualize a trained SOM with labeled data, we propose a method for coloring the SOM lattice according to the class labels. We assume that every unit in the trained SOM has one ore more classes assigned to it and every class is assigned a fraction which gives the contribution of the class in the unit. The Coloring method consists of the following steps:</p><p>• Partitioning the SOM lattice with a Voronoi diagram having the units as sites. That is every unit is the site of a Voronoi cell.</p><p>• Using the attractor function to partition the Voronoi cells, each one sepa- rately taking into account the classes in the unit and in the neighborhood.</p><p>The attractor function is an algorithm, that attracts the colored pixels to a line Chapter 4</p><p>Experements and evaluation</p><p>In this chapter we will test and evaluate our work with three data sets, selected from three dierent practical areas: the rst one the iris data set, which is a simple data set consisting of 150 sample of owers. The second data set is the banksearch data set, a standard data set for testing clustering and classication method in the web elds; it consists of 11000 web documents. The third one is an audio data set: it consists of audio samples taken from the broadcast of 8 radio station.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Iris data</head><p>The cluster boundaries. In fact the main class clusters can also be seen in the pie chart visualization, but the absence of exact borders make it dicult to decide for data points that are close to the cluster borders and those points that lie near outliers or noise nodes. See the regions marked with circles in <ref type="figure" target="#fig_1">Figure 4</ref>.1.</p><p>The main benet of the coloring in case of such simple data is the ability to label new data: after we obtain such a coloring from a labeled data set, we can assign a new unlabeled data by nding the best matching unit for this data and then nd in which colored area this unit is located. Note that the best matching unit can be eventually a new node, if so, we can see the benet in comparison with the pure pie chart visualization, where we must estimate and choose one of the nodes in the neighborhood to assign the class.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Text data</head><p>In this section we will use the Banksearch data set described in [SC02] to test and evaluate our visualization method. Banksearch data set was designed as a standard data set for general use in web document clustering and similar experi- ments and in particular for research in the eld of unsupervised clustering of web documents, so that clustering methods can be tested with a common data set, and thus be easily and objectively compared. It was aimed to use a relatively large number of categories, but with sucient of each category to allow a wide variety of sensible experiments, each using clearly dened subsets of the data set.</p><p>The banksearch data set consists of 4 main categories (Banking &amp; Finance, Pro- gramming languages, Science and Sport), which are further divided into a total of 11 subcategories (themes). These Categories, their themes and identication Ids are listed in <ref type="table" target="#tab_4">Table 4</ref>.1. For each one of the 11 themes, the data set contains 1000 documents,which means a total number of 11000 documents.</p><p>A SOM with the following conguration was trained with this data set: 35 X 25 SOM grid, 10000 iterations, 0.7 learn rate, random seed of the value 7, no normalization was performed on the data. is the smoothed class coloring of the trained SOM.   to paint the isolated classes at the end, then we will face the problem, isolated cells could be split int two or more parts, which is also not optimal.</p><p>The sample in <ref type="figure" target="#fig_1">Figure 4</ref>.4 -(D) shows an optimal partitioning in our opinion:</p><p>The class colored in brown form a continuous cluster that extends over many  This visualization is meaningful especially in the areas, where the classes are interleaved, because in this case the smoothed partitioning makes no sense.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Audio data</head><p>In this subsection we will test and evaluate our visualization method by using the data set presented in <ref type="bibr">[LR06]</ref>, where a method for proling radio stations is described. The aim of this method is to use SOM to give listeners the possibility to easy select the radio stations they like from the overwhelming number of radio stations, that exist online and over the air. Radio station maps are created, which visualize the proles of radio stations so that it is possible to directly pick a specic program type instead of having to search for a suitable radio station.</p><p>In this data set, audio output of 8 radio stations was analyzed: features from the audio signal received from each radio station were extracted, then a two- dimensional SOM was trained with this data. Rhythm Histograms and Statistical Spectrum Descriptors were used for proling radio stations. Data was sampled with a rate of one feature vector for every 6 seconds received from a radio station broadcast. For more information see <ref type="bibr">[LR06]</ref>. To help testing and analyzing the coloring, we manually divided the lattice into regions labeled with the most important categories of genres, such as speech, classic, pop, schlager, etc. These labels was taken from <ref type="bibr">[LR06]</ref>. Each region represents a main genre and can be further divided into subregions that represents dierent genres that are similar; for example speech can be divided into news, reports, advertisement, etc.</p><p>At rst sight, we can see that there are some large areas painted in one (main) color forming class clusters; also, there are other areas that are painted in many colors and in some cases they are heavy interweaved. Class clusters can be interpreted as radio stations that are specialized or have the focus on audio broadcast of certain genres, an example for this type could be the radio station Oe1 and Bgld: For some one who is familiar with these radio stations, this is meaningful, because radio station Oe1 is a radio station which classic music as one of the main broadcas categories, whereas the other analyzed stations seldom broadcast classic music. The same thing holds also for the genre schlager and the radio station Bgld: Bgld Radio station broadcast very often schlager music.</p><p>Interweaved classes can be interpreted as radio stations, that broadcast similar or almost the same genres; an example could be the radio stations OE3 and TIROL, which are mostly represented by the area top/left in <ref type="figure" target="#fig_1">Figure 4</ref>.6 -B. Also it can interpreted as radio stations that broadcast genres in a wide spectrum and has no focus, such as the radio station ONE.</p><p>The area represents the genre Pop music (upper part of the gure) is very interleaved. This is meaningful because because Pop music is very popular and is broadcast by almost all radio stations. The same thing holds exactly for the area Speech especially those regions representing news genre, which is also expected, because all radio stations broadcast news.</p><p>In the areas with heavily interleaved classes we face the problem of ambiguous- ness: if the classes are uniformly (or nearly uniformly) distributed, the coloring algorithm fails to nd a denite order to color the areas; namely to nd the dom- inant class, which is painted as a background for the voronoi region, on which the other classes are painted. The selection of the dominant class depends on the class distribution in the neighborhood. In case of heavily interleaved areas and, the dominant class may be selected randomly. This means that more than one coloring are possible. <ref type="figure" target="#fig_1">Figure 4</ref>.6 -E shows such a case. On the contrary <ref type="figure" target="#fig_1">Figure 4</ref>.6 -F shows areas with classes, that are not very interleaved. These areas have a clear class structure, because the dominant classes and coloring order is algorithmically clear dened. In fact the ambiguousness in the coloring of heavy interleaved classes can be ignored: we can not assume that the trained SOM holds and reects the data exactly, but only the general topology and details up to a certain grad are hold; this means that the coloring algorithm deals with a class distribution that meet the reality with a certain accuracy level, which varies depending on many factors; in such a situation we can ignore the error and the ambiguousness that result from the heavy interleaved class distribution. right/buttom have the class green in most of the cells but in a low contribution, nevertheless it is considered as a dominant class and this is the error.</p><p>We corrected the algorithm to consider the weighted occurrence instead of only the occurrence of the class in the surrounding regions in nding the dominant class; That is, the dominant class is the class for which the sum of the fractions in the surrounding neighborhood is the maximum.</p><p>The same test after the correction produced a satisfying result, which matches the expectation and agree with the dominant class seen by the human eye. The result after correction is illustrated in <ref type="figure" target="#fig_1">Figure 4.</ref>8.</p><p>Also, we tested the banksearch data set after the correction and we found that some eects, which we considered as sub-optimal, disappeared. For example the eect in <ref type="figure" target="#fig_1">Figure 4</ref>.4-(C). The class coloring of the banksearch data set after the correction of the error is illustrated in <ref type="figure" target="#fig_1">Figure 4</ref>.9           The class coloring method was tested with three dierent data sets, namely the iris data set, banksearch data set and an audio data set consisting of the broadcast of eight radio stations. The most of test showed satisfying results.</p><p>Some drawbacks were found such as the problem of ambiguousness; that is more than one coloring is possible. The problem occurs if the class distribution is very interleaved.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>U</head><label></label><figDesc>-Matrix, PMatrix, and U*Matrix [Ult05]. (A)Dataset consisting of a mixture of two Gaussians with 2048 points each. (B)The U-Matrix of the dataset, darker colors correspond to large distances. (C)The P-Matrix of the dataset, darker colors correspond to larger dinsities. (D)The U*-Matrix of the dataset, which shows clearly the two Gaussians. . . . . . . . . . . . . . . . . . . 16 2.4 Hit Histogram visualization methods [Ves99] (a)The size of the black hexagon is proportional to the value of the histogram in the corresponding unit. (b)The height of the bar tells the same thing as in (a). . . . . . . . . . . . . . . 17 2.5 SDH visualization with dierent values of the smoothing parameter s[PRM02]. The Data set consists of 5000 samples, that are randomly drawn from a prob- ability distribution, that is a mixture of 5 Gaussians. The SOM is of 10X10 units arranged on a rectangular grid. . . . . . . . . . . . . . . . . . . . 18 2.6 Gradient visualization with parameters:(a) σ = 1, (b) σ = 5, (c) σ = 15 [PDR] 19 v vi 2.7 Gabriel Graph Visualization [Aup03] (a)Notation for the three data quali- ties dened in this method, from top to bottom: border, isolated, normal. (b)Voronoi diagram (thin lines) and Delaunay triangulation (bold lines) of the data (circles). (c)Gabriel graph (bold lines) of the data. (G2-G5)Number of nodes x ( edges y) in (between) corresponding connected components of G2 and G4, are indicatd beside nodes (edges) of G3 and G5. The class graph G3 shows the topology of the classes and the way they are connected and the den- sity of the connections between the dierent components, where two classes are drawn with their densities and topology; the class and quality graph G5 provides the same of information as in G3 in addition to the visualization of border and isolated components. . . . . . . . . . . . . . . . . . . . . . 23 2.8 Pie chart visualization . . . . . . . . . . . . . . . . . . . . . . . . . . 24 3.1 SOM Toolbox overview An illustration of some features of the SOM Toolbox with banksearch data (A)The SOM lattice with units represented as pie charts (B)Zoomed details view with labels and pie charts (C)Pie chart representing the unit position and the class contribution (D)Smoothed data histogram vi- sualization as an example visualization (E)DMatrix visualization as another example (F)There are other visualizations that can be selected from the visu- alization menu . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27 3.2 Base conguration of the SOM Class Coloring (A)The SOM units located on the grid with their class contributions, that is given by function f. (B)Representation of A as it is in SOM Toolbox, where f is represented with pie chart. . . . . . 29 3.3 Coloring by Simple Flooding assuming one class per unit. (A)Colored points representing the dominant classes of the SOM units, (B to D)The process of color ooding animated in 3 steps. . . . . . . . . . . . . . . . . . . . . 30 3.4 Unit substitution.(A) Multi class units represented by pie charts,(B)Substitution points that can be used by color ooding. . . . . . . . . . . . . . . . . . 30 3.5 Instability eect in the color ooding. (A)While growing, the two blue points close the way and prevent the red point from growing to the left side. (B)After making a slight change in the position of one of the blue points, the red point can grow to the left side, which changes the end result dramatically. . . . . 31 vii 3.6 Areas in Voronoi diagram (A) are similar to these produced by color ooding (B). . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32 3.7 Relation between Voronoi diagram and Delaunay triangulation(DT). (A)a set of point (sites) in a 2-D space(B). (B)Voronoi faces (thin lines) and Delaunay triangulation (thick lines) of the sites in A. (C)The circumsphere of every triangle in DT contains no sites inside. . . . . . . . . . . . . . . . . . . 33 3.8 Sweepline algorithm. The slides 1-9 animate the algorithm while nding the Voronoi diagram of the three sites in slide 1. In slide 2 a parabola for the upper site arises because the sweepline passes this site. In the next two slides the parabola grows while the sweepline moves. In slide 5 two parabolas for the two lower points arise because the sweepline passes these points. In the rest slides all parabolas grow while the line sweeping. The intersections of the parabolas form the edges of the Voronoi diagram as in slide 9 (red lines). . . 35 3.9 Bowyer-Watson algorithm when adding new site. (A)The faces to be deleted are all faces whose circumsphere or itself contains the new added site. (B)Faces to delete are colored, and new faces are marked in dashed lines. (C)The resulting Delaunay triangulation after insertion and update. . . . . . . . . 36 3.10 Region substitution. . . . . . . . . . . . . . . . . . . . . . . . . . . . 38 3.11 Sector partitioning. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39 3.12 (A)Notations used int this section(Note that we use the colors as class names for simplifying). (B)Connection lines between regions that have same classes. 40 3.13 The work of attractor function: In region r1, at rst the 34 grids, that are most close to L1, are colored in red then the next nearest 45 grids are colored in yellow. The 41 grids most nearest to L5 are colored in blue, also 14 grids from r4 that are nearest to L5 from the other side, are colored in blue. that gives the eect of an area extending over two regions In regions r2 and r3 the same thing is done with L2 and L3, because L2 and L3 have a common point the resulting colored area appears as a thin area, that extends over two regions In r4 the line L4 has the length 0 (is equivalent to a point), thus the resulting colored area has a circular shape. . . . . . . . . . . . . . . . . 41 3.14 Distance between a point and a line segment[Sof]. . . . . . . . . . . . . . 43 viii 3.15 Finding the lines segments (A)At rst each region is colored with the dominant class color. The class red is isolated because no neighbor region have this class;</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>4 each</head><label>4</label><figDesc>neuron represents an n-dimensional column vector , where n depends on the data space dimensionality (input vectors dimensionality). The reason for using one-and two-dimensional grids is that space structures of higher dimensionality cause problems with data display. Neurons are usually located in the nodes of the two-dimensional grid with rectangular or hexagonal cells. The number of neurons in the lattice determines the resulting resolution and the granularity of data presentation. During the training not only the winning neuron is modied but its neighbors as well, although the strength of the adaption may depend on their distance from the winner neuron. This allows considering the SOM to be a method of projecting the data nonlinearly onto a lower-dimensional display. When using this algorithm the vectors similar in the initial space get close to each other on the nal map SOMs [Koh97].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>1 shows how this works with a 2D vector. The weight modication is dened by the following formula:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Where σ(t) is a diminishing function of time. This value is often called the radius of the neighborhood. At the beginning of the learning procedure it is fairly large, but it is made to gradually shrink during learning. Towards the end a single winning neuron is trained. Most frequently the linear decreasing function of time is used. Let us proceed to the learning rate function α(t). This is also a decreasing function of time. A variant of this function are linear and inversely proportional of time with the formula:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2.2: Agglomerative algorithm: (A) steps, (B) corresponding dendrogram</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>derstanding the data by giving answers to questions like what and where are the clusters, which shape has the data cloud, etc. Projection methods are very useful in answering such questions. Projecting data on to its underlying subspace can detect its real structures, facilitate functional analysis, and help making a judg- ment. Principally there are two types of projection methods: linear methods such as principal component analysis (PCA), and nonlinear such as multidimensional scaling (MDS) and SOM.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>Figure 2.3: U-Matrix, PMatrix, and U*Matrix [Ult05]. (A)Dataset consisting of a mixture of two Gaussians with 2048 points each. (B)The U-Matrix of the dataset, darker colors correspond to large distances. (C)The P-Matrix of the dataset, darker colors correspond to larger dinsities. (D)The U*-Matrix of the dataset, which shows clearly the two Gaussians.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2.3 (b) shows the U-Matrix of the dataset, darker colors correspond to large distances. Figure 2.3 (c) shows the P-Matrix of the dataset, darker colors correspond to larger densities. Figure 2.3 (d) shows the U*-Matrix which shows clearly the two Gaussians.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2.4: Hit Histogram visualization methods [Ves99] (a)The size of the black hexagon is proportional to the value of the histogram in the corresponding unit. (b)The height of the bar tells the same thing as in (a).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2.5: SDH visualization with dierent values of the smoothing parameter s[PRM02]. The Data set consists of 5000 samples, that are randomly drawn from a probability distribution, that is a mixture of 5 Gaussians. The SOM is of 10X10 units arranged on a rectangular grid.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2.6: Gradient visualization with parameters:(a) σ = 1, (b) σ = 5, (c) σ = 15 [PDR]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2.6 shows an example that illustrates this visualization. The example is taken from [PDR] and shows vector eld visualizations with dierent values of σ of a 30X40 SOM that is trained with the Phonetic data set. Low values of σ lead to very granular visualizations, where only direct neighbors are taken into account for the computation of each arrow and thus only local gradients can be observed, as visualized in Figure 2.6-(a) with σ = 1. By increasing this value, the clustering structure revealed shifts gradually from local towards global. Figure 2.6-(b) provides a far better overview on the clustering structure,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>A</head><label></label><figDesc>dierent visualization method is proposed in [Aup03], which is based on graph analysis. This method assumes a labeled data set; that is the class label for each datum is known. This means in case of a trained SOM the class for each data vector is known and the problem is to extract information about the topology of the classes by making use of graph analysis, in particular Gabriel graph. Gabriel graph GG is a sub graph of Delaunay triangulation [For97] in which every edge has a forbidden region of a diameter with length equal to the edge length, Figure 2.7 - c; that is for every edge eij in GG the open ball with diameter |eij| contains no other edges other than eij. This method denes three qualities of data, that can occur in the visualization, Figure 2.7 -a, which are:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head></head><label></label><figDesc>After applying the algorithm described in [Aup03] visualizations like this in Fig- ure 2.7 -G3 and G5 are obtained. Pie-chart class visualization In this section we will illustrate the pie chart visualization described in [RPM03].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2.8 shows an example of the pie chart visualization. In this visualization</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2.7: Gabriel Graph Visualization [Aup03] (a)Notation for the three data qualities dened in this method, from top to bottom: border, isolated, normal. (b)Voronoi diagram (thin lines) and Delaunay triangulation (bold lines) of the data (circles). (c)Gabriel graph (bold lines) of the data. (G2-G5)Number of nodes x ( edges y) in (between) corresponding connected components of G2 and G4, are indicatd beside nodes (edges) of G3 and G5. The class graph G3 shows the topology of the classes and the way they are connected and the density of the connections between the dierent components, where two classes are drawn with their densities and topology; the class and quality graph G5 provides the same of information as in G3 in addition to the visualization of border and isolated components.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2.8: Pie chart visualization</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head></head><label></label><figDesc>In this chapter the SOM Toolbox described in [RPM03] will be illustrated having the focus on the visualization aspects. It comprises visualization tools which use the SOM as a visualization start point, it provides then the possibility to make various visualizations on top of the SOM lattice. As input the tool takes a vector le with the raw data, a class info le with the involved class names, and template le with information about the data such as the dimensionality. The tool performs the SOM training process and provides a ground visualization of the trained SOM as a SOM grid with the units represented as pie charts, that show the class contribution. Depending on the details level set by the user, labels can also be displayed on the grid, see Figure 3.1. As we can see the SOM Toolbox provides a basis on which we can build various visualizations. In the next sections we will propose SOM Class Coloring visualization and then describe how to extend the SOM Toolbox with it.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3.1: SOM Toolbox overview An illustration of some features of the SOM Toolbox with banksearch data (A)The SOM lattice with units represented as pie charts (B)Zoomed details view with labels and pie charts (C)Pie chart representing the unit position and the class contribution (D)Smoothed data histogram visualization as an example visualization (E)DMatrix visualization as another example (F)There are other visualizations that can be selected from the visualization menu</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3.2: Base conguration of the SOM Class Coloring (A)The SOM units located on the grid with their class contributions, that is given by function f. (B)Representation of A as it is in SOM Toolbox, where f is represented with pie chart.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3.3 illustrate the process of color ooding. As we can see, a reasonable coloring was produced by simple color ooding.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_21"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3.3: Coloring by Simple Flooding assuming one class per unit. (A)Colored points representing the dominant classes of the SOM units, (B to D)The process of color ooding animated in 3 steps.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_22"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3.4: Unit substitution.(A) Multi class units represented by pie charts,(B)Substitution points that can be used by color ooding.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_23"><head>Figure 3 . 5 :</head><label>35</label><figDesc>Figure 3.5: Instability eect in the color ooding. (A)While growing, the two blue points close the way and prevent the red point from growing to the left side. (B)After making a slight change in the position of one of the blue points, the red point can grow to the left side, which changes the end result dramatically.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_24"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3.6: Areas in Voronoi diagram (A) are similar to these produced by color ooding (B).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_25"><head>This similarity is clear in Figure 3 . 6 .</head><label>36</label><figDesc>Voronoi diagrams are discussed in details in subsection 3.4.1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_26"><head></head><label></label><figDesc>Figure 3.7: Relation between Voronoi diagram and Delaunay triangulation(DT). (A)a set of point (sites) in a 2-D space(B). (B)Voronoi faces (thin lines) and Delaunay triangulation (thick lines) of the sites in A. (C)The circumsphere of every triangle in DT contains no sites inside.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_27"><head>Figure 3 . 7 .</head><label>37</label><figDesc>More formally: R(s i ), R(s j ) ∈ V (S) and have a common edge e ⇔ s i s j is an edge in DT (S) There are many algorithms for computing the voronoi diagram; there are two main approaches: algorithms make use of the duality and compute the Delaunay triangulation such as the randomized-incremental algorithm [AS92] and Bowyer- Watson algorithm [Dev98]; other algorithms compute the voronoi diagram di- rectly such as the sweepline algorithm [For86]. There are other algorithms that use the divide-and-conquer algorithm such as the gift-wrapping algorithm [For97].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_28"><head>tained</head><label></label><figDesc>by a balanced search structure called the sweeping line. It is used in many application for solving problems such as nding Delaunay triangulation, voronoi diagrams and other problems like sorting, tree management, etc. In this algorithm a sweep line crosses the space and produces events when passing the underlying sites. These events control the computing see Figure 3.8. Actually the sweep line splits the problem domain into two regions, an explored region and an unexplored region. It applies an ordering to the problem because we reason about the explored area based on what we have seen so far and ignore the unexplored area, that is only the points crossing the sweep line aect the current computation since all other points are too far away. In the problem of nding the voronoi diagram see Figure 3.8, the sweepline moves vertically from the top of the plane to its bottom, a parabola is maintained for every site: the parabola arises when the sweepline passes the site and grows while the line sweeping down, so that the base line denes the parabola for the corresponding site. The voronoi diagram is then given by the intersection points of the parabolas. Actually, not all the parabolas are computing while the beach line sweeps, because this would be very costly. Instead, only instances are calculated when the beach line changes topology for example when the sweepline passes a new site. More Information about this algorithm are available in [For86].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_29"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3.8: Sweepline algorithm. The slides 1-9 animate the algorithm while nding the Voronoi diagram of the three sites in slide 1. In slide 2 a parabola for the upper site arises because the sweepline passes this site. In the next two slides the parabola grows while the sweepline moves. In slide 5 two parabolas for the two lower points arise because the sweepline passes these points. In the rest slides all parabolas grow while the line sweeping. The intersections of the parabolas form the edges of the Voronoi diagram as in slide 9 (red lines).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_30"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3.9: Bowyer-Watson algorithm when adding new site. (A)The faces to be deleted are all faces whose circumsphere or itself contains the new added site. (B)Faces to delete are colored, and new faces are marked in dashed lines. (C)The resulting Delaunay triangulation after insertion and update.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_31"><head>3 .</head><label>3</label><figDesc>Figure 3.10: Region substitution.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_32"><head>(</head><label></label><figDesc>pixel) to a class separately with the help of a distance algorithm, that takes into account the sharing value and the neighborhood and the second method is by coloring the dierent classes on a chessboard like grid with the areas according to their relative frequencies. Both of the two methods are described in details in the following subsections.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_33"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3.12: (A)Notations used int this section(Note that we use the colors as class names for simplifying). (B)Connection lines between regions that have same classes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_34"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3.12 (B)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_35"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3.13: The work of attractor function: In region r1, at rst the 34 grids, that are most close to L1, are colored in red then the next nearest 45 grids are colored in yellow. The 41 grids most nearest to L5 are colored in blue, also 14 grids from r4 that are nearest to L5 from the other side, are colored in blue. that gives the eect of an area extending over two regions In regions r2 and r3 the same thing is done with L2 and L3, because L2 and L3 have a common point the resulting colored area appears as a thin area, that extends over two regions In r4 the line L4 has the length 0 (is equivalent to a point), thus the resulting colored area has a circular shape.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_36"><head></head><label></label><figDesc>Line segment connecting the site with the middle of an edge: This attractor is useful to produce a colored area that spreads over many Voronoi cells. See L2 and L3 in Figure 3.13 • Line segment inside the Voronoi cell: produced a pseudo rectangular areas inside the cell. See L1 in Figure 3.13 • Point attractor: It is a line of length 0. It produces a circular area inside the region and is suitable for coloring isolated classes. See L4 in Figure 3.13.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_37"><head></head><label></label><figDesc>units labeled with classes and and their contribution fractions. A voronoi dia- gram is rst found with the units as sites. Then the resulting voronoi regions are colored each separately taking into account all of the neighbor regions. The coloring consists of the following steps:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_38"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3.15-D Border smoothing by weighting the line segments There is an undesired eect, that occurs if two neighboring areas in two neigh</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_39"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3.16-A, where the contribution fractions for the class c are f 1 for region r 1 and f 2 for region r 2 , then we assign the points of the line segments connecting the two regions weights as follows: the face points take the contribution value as a weight w1 = f 1 , w 2 = f 2 , and the common point is assigned a weight of the value w 1 +w 2 2 . See Figure 3.16-B. This implies that the pixel concentration on both sides of the common edge is the same for both regions regardless of the values of the contribution fractions. This weighted distance produces a smoothed bordering like in 3.16-C. The minimum visible class parameter There are Nodes that have classes assigned with a low contribution. Some of these are outliers. If one is interested to have a visualization of a certain abstraction level for example if only the main class clusters are important, the such classes could make an undesired eect, because the are unimportant details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_40"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3.16: Smoothing of the border by using weighted line segments. (A)The border at the common edge has a zigzag form because the areas are not equal. (B)The line segments are weighted by using the contribution value as weights for the face points and the average for the common point. (C) Smooth partitioning as a result of the weighted sorting.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_41"><head></head><label></label><figDesc>Figure 3.17: The minimum visible class parameter. (A) Visualization with parameter set to 0. (B) The same data visualized with the parameter set to 50% and (C) set the parameter set to 100%.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_42"><head>segment.</head><label></label><figDesc>Figure 3.18: Chessboard Visualization vs. smooth partition visualization. (A)the smooth partition visualization, (B)Chessboard visualization. In each case with and without voronoi borders.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_43"><head></head><label></label><figDesc>Figure 4.1-B the class coloring visualization. The iris data is a simple data set in relation of class distribution: as it is clear from the visualization there are no SOM nodes with multiple classes, each node has a single class assigned. Even so we can see the benet of the SOM coloring when it is compared with the pure pie chart visualization: with the help of the</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_44"><head>Figure 4 .</head><label>4</label><figDesc>2 shows an overview of the class visualization of the trained SOM: (A) is the pie chart visualization and (B)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_45"><head>K</head><label></label><figDesc>Sport Sport by comparing the pie chart visualization with the smoothed coloring in Fig- ure 4.2 we see that the class clusters are easier to recognize in the smooth coloring, especially those clusters formed by nodes with multiple classes. With pure pie chart visualization, it is dicult to recognize such class clusters. Smooth coloring provides sharp boundaries between class clusters. These boundaries are helpful in the process of assigning new data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_46"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4.3shows the class coloring of the same data with dierent values of the parameter minimum visible class. In Figure 4.3-(A) only the dominant classes are visualized; This is achieved by sitting the parameter minimum visible class to 100%. Recall that the coloring algorithm nd the dominant class for each Voronoi cell and paint the class color as a background for the corresponding cell. Then the algorithm paints the rest of the classes. By setting a large value for the parameter minimum visible class, we prevent the algorithm from painting classes other than the dominant class. The dominant class visualization shows the general class topology by visualizing the main classes and ignoring the details and outliers. In Figure 4.3-(B) the parameter is set to 50%, only few details are seen. In Figure 4.3-(C) the parameter is set to 0% and this causes the no classes are ltered and thus all details are visualized. In Figure 4.4 the the border of the voronoi cells are visible to help analyze the</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_47"><head>Voronoi cells.</head><label></label><figDesc>The cluster borders are smoothed without zigzag boundaries. Iso- lated classes are also painted as expected. If there are more than one isolated class in one Voronoi cell, they are painted as rings in the middle of the Voronoi cell. In fact this good quality holds for most of the cases. The sample in Figure 4.4 -(E) shows a typical example of isolated classes. Isolated classes are painted as a circle in the middle of the voronoi cell. If there are more than one isolated class, then the they are painted as rings about the circle. The order of painting (which class to which rings) is randomly selected. The sample in Figure 4.4 -(F) shows the eect of the weighting to produce smoothed borders without zigzag boundaries. The yellow area marked with a circle has smoothed boundaries, i.e. at the points, that join the yellow areas in the three cells cells.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_48"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4.5 shows the chessboard class coloring of the banksearch data set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_49"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4.6 shows the class coloring of the the data set. There are 8 classes, which represent the radio stations. The coloring partitions the SOM-lattice into 8 partitions, which are painted on top the SOM lattice. The nodes are organized according the similarity between feature vectors, which represent the genre of</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_50"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4.7 shows the class coloring of the data set with dierent values of the parameter minimum visible class. As we see in the gure the visible details decrease as the parameter value increase. After comparing the visualizations Figure 4.7-(A) an Figure 4.7-(D) we can see a serious problem: if we look at the areas marked with circles in (A), we see a dominant colors which are brown in the upper area and red in lower one. After setting the parameter to 100% the elementary classes should be eliminated and only the dominant class should be visible. If we look at the visualization in (D) we see that the corresponding areas (areas marked in circles) have a dierent classes than these dominant classes we see with our eyes; namely blue/cyan in the upper area and green in the lower one. This means that there is an error in the coloring algorithm. We analyzed the problem and found the error; it was namely in the algorithm, that nd the dominant class: the algorithm nds the dominant class as follows: for each class in the actual unit, it counts the regions in the neighborhood which contain this class, then it selects the class with the maximum number of such regions. The algorithm does not take into account the contributions for these classes. This means that if most of the surrounding regions have a class then it is considered the dominant class regardless the contribution fraction of this class. So the marked area in (A)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_51"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4.10 shows the chessboard visualization of the radio search data set. In the visualization we can see the main clusters and the class topology and clouds of color mixture in the areas with interleaved class distribution. This coloring is meaningful because if you concentrates only at the full painted areas you see the dominant classes and thus the general class topology. Whereas if you concentrate at the color mixture you see the detailed class distribution.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_52"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4.1: Class coloring of the iris data. The areas marked with circles are examples of regions, where it is dicult to assign new data without using the coloring.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_53"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4.2: Class visualization overview of the banksearch data set. (A)Pie chart visualization. (B)Smoothed class coloring of the same data set. For the categories and themes see Table 4.1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_54"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4.3: Class Coloring with dierent values of the parameter minimum visible class. (A)the parameter is set to 100%. (B)the parameter is set to 50%. (C)the parameter is set to 0%.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_55"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4.4: Class coloring with Voronoi cell border. (A) to (F) are signicant samples showing some strengths and drawbacks of the visualization.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_56"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4.5: Chessboard visualization of the banksearch dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_57"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4.6: Radio station data set. (A) Pie chart visualization. (B) Class coloring of the data set. The text labels describe the main categories of the genres. See [LR06].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_58"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4.7: Class coloring of the radio search data set. The parameter minimum visible class is set to dierent values (A) 0% (B) 30% (C) 60% (D)100%. The dominant classes in (C) do not agree with these seen by the human eye in (A).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_59"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4.8: Class coloring of the radio search data set after the correction of the algorithm. The parameter minimum visible class is set to dierent values (A) 0% (B) 30% (C) 60% (D)100%. The dominant classes in (C) agree with these seen by the human eye in (A).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_60"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4.9: Class coloring of the banksearch data set after the correction of the algorithm. Some of the drawbacks that was found in the least section have disappeared after the correction. For example the area marked with a circle is to be compared with Figure 4.4-(C).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_61"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4.10: Chessboard visualization of the radio search data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="true"><head>Table 4 .1: Categories and their associated themes in the banksearch dataset.</head><label>4</label><figDesc></figDesc><table>Dataset Id 
Dataset Category 
Associated Theme 

A 
Commercial Banks 
Banking &amp; Finance 

B 
Building Societies 
Banking &amp; Finance 

C 
Insurance Agencies 
Banking &amp; Finance 

D 
Java 
Programming languages 

E 
C/C++ 
Programming Languages 

F 
Visual Basic 
Programming languages 

G 
Astronomy 
Science 

H 
Biology 
Science 

I 
Soccer 
Sport 

J 
Motor Sport 
Sport 

</table></figure>
		</body>
		<back>
			<div type="annex">
			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Data clustering: a review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Flynn Anil Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">N</forename><surname>Murty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">264323</biblScope>
			<date type="published" when="1999-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A simple on-line randomized incremental algorithm for computing higher order voronoi diagrams</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franz</forename><surname>Aurenhammer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Otfried</forename><surname>Schwarzkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Symposium on Computational Geometry, Proceedings of the seventh annual symposium on Computational geometry</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="1992" />
			<biblScope unit="page">363381</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">High-dimensional labeled data analysis with gabriel graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Aupetit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings Intl. European Symp. on Articial Neural Networks (ESANN&apos;03)</title>
		<meeting>Intl. European Symp. on Articial Neural Networks (ESANN&apos;03)<address><addrLine>Bruges, Belgium</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
	<note>Dside publications</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Improved incremental randomized delaunay triangulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Devillers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings 14th Annu. ACM Sympos. Computer Geometry</title>
		<meeting>14th Annu. ACM Sympos. Computer Geometry</meeting>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="page">106115</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A sweepline algorithm for voronoi diagrams</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Fortune</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SCG 86: Proceedings of the second annual symposium on Computational geometry</title>
		<meeting><address><addrLine>New York, NY, USA; New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="1986" />
			<biblScope unit="page">313322</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Voronoi diagrams and delaunay triangulations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Fortune</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="page">377388</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Data exploration using self-organizing maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Kaski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Acta Polytechnica Scandinavica, Mathematics, Computing and Management in Engineering Series No</title>
		<imprint>
			<biblScope unit="volume">82</biblScope>
			<date type="published" when="1997-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Exploration of very large databases by self-organizing maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Teuvo</forename><surname>Kohonen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICNN&apos;97, International Conference on Neural Networks</title>
		<meeting>ICNN&apos;97, International Conference on Neural Networks<address><addrLine>Piscataway, NJ</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Visually proling radio stations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Lidy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Rauber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th International Conference on Music Information Retrieval (ISMIR 2006)</title>
		<meeting>the 7th International Conference on Music Information Retrieval (ISMIR 2006)</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Advanced visualization of self-organizingmaps with vector elds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Pölzlbauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Dittenbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Rauber</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Using smoothed data histograms for cluster visualization in self-organizing maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elias</forename><surname>Pampalk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Rauber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dieter</forename><surname>Merkl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Artical Neural Networks (ICANN&apos;02)</title>
		<meeting>the International Conference on Artical Neural Networks (ICANN&apos;02)<address><addrLine>Madrid, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The som-enhanced jukebox organization and visualization of music collections based on perceptual models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Rauber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elias</forename><surname>Pampalk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dieter</forename><surname>Merkl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">New Music Research</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">193210</biblScope>
			<date type="published" when="2003-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A nonlinear mapping for data structure analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Sammon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In IEEE Trans. Computer</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page">401409</biblScope>
			<date type="published" when="1969-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">A large benchmark dataset for web</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Sinka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Corne</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Pareto density estimation: Probability density estimation for knowledge discovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alfred</forename><surname>Ultsch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings Conf. Soc. for Information and Classication</title>
		<meeting>Conf. Soc. for Information and Classication<address><addrLine>Cottbus, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Esom-maps: tools for clustering, visualization, and classication with emergent som</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alfred</forename><surname>Ultsch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Technical Report</title>
		<meeting><address><addrLine>Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005-03" />
			<biblScope unit="volume">46</biblScope>
		</imprint>
		<respStmt>
			<orgName>Dept. of Mathematics and Computer Science, D-35032 Marburg</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Clustering of the self-organizing map</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juha</forename><surname>Vesanto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Alhoniemi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE-NN</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">586</biblScope>
			<date type="published" when="2000-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">SOM-based data visualization methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juha</forename><surname>Vesanto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">11126</biblScope>
		</imprint>
	</monogr>
<note type="report_type">IntelligentData-Analysis</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Nonlinear multidimensional data projection and visualisation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hujun</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Lecture Notes in Computer Science 2690</title>
		<meeting><address><addrLine>Manchester</addrLine></address></meeting>
		<imprint>
			<biblScope unit="page">60</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><forename type="middle">K</forename><surname>1qd</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
