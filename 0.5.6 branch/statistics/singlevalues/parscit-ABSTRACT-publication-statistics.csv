"path";"PDF";"original";"extracted";"original value";"extracted value";"Precision";"Recall";"F1"
"TUW-137078";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\groundtruth\TUW-137078.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\result\result-TUW-137078-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\extracted\parscit\parscit-TUW-137078-xstream.xml"")";"The aim of the ECIC project is the dissemination of participative methods of organizational development originating from Scandinavian countries to other European countries. To support this process we developed a hypertextual learning system. Because of the heterogeneous target group and the ill-structured domains of ECIC detailed usability testing is necessary. First tentative results indicate that graphical overview maps play an important role and interactive examples are very motivating.";;"none extracted value";"0,00";"0,00"
"TUW-138011";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\groundtruth\TUW-138011.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\result\result-TUW-138011-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\extracted\parscit\parscit-TUW-138011-xstream.xml"")";"The role of medical guidelines is becoming more and more important in the medical field. Within the Protocure project it has been shown that the quality of medical guidelines can be improved by formalisation. However formal-isation turns out to be a very time-consuming task, resulting in a formal guideline that is much more complex than the original version and the relation with this original guideline is often unclear. This paper presents a case study where the relation between the informal medical guideline and its formal counterpart is investigated. This has been used to determine the gaps between the formal and informal guidelines and the cause of the size explosion of the formal guidelines.";;"none extracted value";"0,00";"0,00"
"TUW-138447";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\groundtruth\TUW-138447.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\result\result-TUW-138447-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\extracted\parscit\parscit-TUW-138447-xstream.xml"")";"A key problem in multimedia systems is the faithful reproduction of color. One of the main reasons why this is a complicated issue are the different color reproduction technologies used by the various devices; displays use easily modeled additive color mixing, while printers use a subtractive process, the characterization of which is much more complex than that of self-luminous displays. In order to resolve these problems several processing steps are necessary, one of which is accurate device characterization. Our study examines different learning algorithms for one particular neural network technique which already has been found to be useful in related contexts – namely radial basis function network models – and proposes a modified learning algorithm which improves the colorimetric characterization process of printers. In particular our results show that is possible to obtain good performance by using a learning algorithm that is trained on only small sets of color samples, and use it to generate a larger look–up table (LUT) through use of multiple polynomial regression or an interpolation algorithm. We deem our findings to be a good start point for further studies on learning algorithms used in conjunction with this problem.";;"none extracted value";"0,00";"0,00"
"TUW-138544";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\groundtruth\TUW-138544.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\result\result-TUW-138544-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\extracted\parscit\parscit-TUW-138544-xstream.xml"")";"If visual information retrieval should make further progress, it will be necessary to identify new ways to derive visual properties from higher levels of understanding than the pixel level (e.g. from low-level features). The paper outlines the implementation of modelling of feature hierarchies in the visual information retrieval framework VizIR (free under GPL). The approach allows for the derivation of high-level features from low-level features by aggregation and localisa-tion as well as semantic enrichment with additional knowledge. The technical implementation is based on the MPEG-7 structures for aggregation and specialisation.";;"none extracted value";"0,00";"0,00"
"TUW-138547";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\groundtruth\TUW-138547.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\result\result-TUW-138547-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\extracted\parscit\parscit-TUW-138547-xstream.xml"")";"This paper describes solutions for parallel video processing based on LAN-connected PC-like workstations. We outline application scenarios for the processing of video with broadcast TV resolution and promising areas for future research. Additionally, we describe a prototype system implemented in our workgroup. This Network-of-Workstations is based on Gigabit Ethernet, free Java software and standard network protocols. Video data is streamed to and from processing hosts using IP multicast and the Real-time Transfer Protocol. Control mechanisms are based on network file sharing. In comparison to related approaches, our system makes use of high-performance network hardware and open source software. In consequence, our system is easier to implement, cheaper and more flexible than, for example, highly integrated commercial solutions.";;"none extracted value";"0,00";"0,00"
"TUW-139299";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\groundtruth\TUW-139299.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\result\result-TUW-139299-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\extracted\parscit\parscit-TUW-139299-xstream.xml"")";"This thesis presents methods to support protocol-based care in medicine. Time-oriented treatment plans and patient data are represented visually providing various interaction possibilities to aid execution and analysis of medical therapy plans formulated in the representation language Asbru. We introduce a two-view approach consisting of a Logical View and a Temporal View. The Logical View depicts therapy plans using a flow-chart like representation based on ""clinical algorithm maps"". The Temporal View on the other hand depicts plans as well as patient data in form of parameters and variables over time. The plan visualization method within the Temporal View is based on the idea of LifeLines. For being able to depict hierarchical structures and temporal uncertainties, we extended this concept and a novel glyph called PlanningLine has been developed. The development is embedded into a 3-step evaluation process including a user study with eight domain experts (physicians) at the beginning to acquire users' needs, a design evaluation, and an evaluation of our software prototype at the end of the thesis project.";;"none extracted value";"0,00";"0,00"
"TUW-139761";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\groundtruth\TUW-139761.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\result\result-TUW-139761-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\extracted\parscit\parscit-TUW-139761-xstream.xml"")";"This master thesis describes how to price options by means of Genetic Programming. The underlying model is the Generalized Autoregressive Conditional Heteroskedastic (GARCH) asset return process. The goal of this master thesis is to nd a closed-form solution for the price of European call options where the underlying securities follow a GARCH process. The data are simulated over a wide range to cover a lot of existing options in one single equation.
Genetic Programming is used to generate the pricing function from the data. Genetic Programming is a method of producing programs just by defining a problemdependent fitness function. The resulting equation is found via a heuristic algorithm inspired by natural evolution. Three different methods of bloat control are used. Additionally Automatic Defined Functions (ADFs) and a hybrid approach are tested, too. To ensure that a good configuration setting is used, preliminary testing of many different settings has been done, suggesting that simpler configurations are more successful in this environment.
The resulting equation can be used to calculate the price of an option in the given range with minimal errors. This equation is well behaved and can be used in standard spread sheet programs. It offers a wider range of utilization or a higher accuracy, respectively than other existing approaches.";;"none extracted value";"0,00";"0,00"
"TUW-139769";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\groundtruth\TUW-139769.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\result\result-TUW-139769-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\extracted\parscit\parscit-TUW-139769-xstream.xml"")";"This thesis deals with local branching, a local search algorithm applied on top of a Branch and Cut algorithm for mixed integer programming problems. Local branching defines custom sized neighborhoods around given feasible solutions and solves them partially or completely before exploring the rest of the search space. Its goal is to improve the heuristic behavior of a given exact integer programming solver, i.e. to focus on finding good solutions early in the computation. Local branching is implemented as an extension to the open source Branch and Cut solver COIN/BCP. The framework's main goal is to provide a generic implementation of local branching for integer programming problems. IP problems are optimization problems where some or all variables are integer values and must satisfy one or more (linear) constraints. Several extensions to the standard local branching algorithm were added to the framework. Pseudo-concurrent exploration of multiple local trees, aborting local trees and a variable fixing heuristic allow the user to implement sophisticated search metaheuristics that adjust the local branching parameters adaptively during the computation. A major design goal was to provide a clean encapsulation of the local branching algorithm to facilitate embedding of the framework in other, higher-level search algorithms, for example in evolutionary algorithms. As an example application, a solver for the multidimensional knapsack problem is implemented. A custom local branching metaheuristic imposes node limits on local subtrees and adaptively tightens the search space by fixing variables and reducing the size of the neighborhood. Test results show that local branching can offer significant advantages to standard Branch and Cut algorithms and eventually proves optimality in shorter time. Especially for large, complex test instances exploring the local neighborhood of a good feasible solution often yields better short-term results than the unguided standard Branch and Cut algorithm. Improving the solutions found early in the computation also helps to remove additional parts of the search tree, potentially leading to better solutions in longer runs. ";;"none extracted value";"0,00";"0,00"
"TUW-139781";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\groundtruth\TUW-139781.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\result\result-TUW-139781-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\extracted\parscit\parscit-TUW-139781-xstream.xml"")";;;"none extracted value";"none expected";"NA"
"TUW-139785";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\groundtruth\TUW-139785.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\result\result-TUW-139785-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\extracted\parscit\parscit-TUW-139785-xstream.xml"")";"In this master thesis a generic libray of efficient metaheuristics for combinatorial optimization is presented. In the version at hand classes that feature local search, simulated annealing, tabu search, guided local search and greedy randomized adaptive search procedure were implemeted. Most notably a generic implementation features the advantage that the problem dependent classes and methods only need to be realized once without targeting a specific algorithm because these parts of the sourcecode are shared among all present algorithms contained in EAlib. This main advantage is then exemplarily demonstrated with the quadratic assignment problem. The sourcecode of the QAP example can also be used as an commented reference for future problems. Concluding the experimental results of the individual metaheuristics reached with the presented implementation are presented.";;"none extracted value";"0,00";"0,00"
"TUW-140047";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\groundtruth\TUW-140047.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\result\result-TUW-140047-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\extracted\parscit\parscit-TUW-140047-xstream.xml"")";"This paper describes our submission to the MIREX 2006 Audio Music Similarity and Retrieval task. The task was to submit an audio feature extraction algorithm, to compute music similarity measures and to return a distance matrix from an audio collection consisting of 5000 pieces, which was subsequently evaluated through human listening tests as well as objective statistics. We submitted a new implementation of the Statistical Spectrum Descriptor (SSD) audio feature extractor and computed the distance matrix directly from feature space. Results from the human evaluation show that our approach is among the top 5 algorithms which furthermore show no statistically significant performance differences. The evaluation of a number of objective statistics ranked our algorithm 3rd in most of the cases. Our submission was one of the two fastest in terms of total runtime, having the shortest distance computation time. The approach has also been evaluated on Audio Cover Song Identification, where it was the bestperforming ""Audio Music Similarity and Retrieval"" submission, outperformed, however, by 4 submissions which were specifically designed for the cover identification task.";;"none extracted value";"0,00";"0,00"
"TUW-140048";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\groundtruth\TUW-140048.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\result\result-TUW-140048-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\extracted\parscit\parscit-TUW-140048-xstream.xml"")";"UN/CEFACT's Modeling Methodology (UMM) is a well accepted and formal notation to analyze and design B2B business processes. In a service oriented architecture (SOA) environment process specification languages like the Business Process Specification Schema (BPSS) are used to configure B2B information systems. However , mappings from UMM models to BPSS process specifications currently exist just on a conceptual level. This results in a gap between defined B2B processes and BPSS configurable e-commerce systems. Thus, a model driven code generation of BPSS descriptions is required. In this paper we present a technical implementation of a transformation engine that generates BPSS process specifications from a UMM model represented in the XML Metadata Interchange (XMI) language. This implementation bridges the gap mentioned above. It has been used in the EU project GILDAnet to generate BPSS descriptions from logistic processes modeled in UMM.";;"none extracted value";"0,00";"0,00"
"TUW-140229";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\groundtruth\TUW-140229.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\result\result-TUW-140229-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\extracted\parscit\parscit-TUW-140229-xstream.xml"")";"Detecting the needs of learners is a challenging but essential task to be able to provide adaptivity. In this paper we present a tool that enables learning management systems (LMS) to detect learning styles based on the behavior of learners during an online course. By calculating the learning styles and filling the student model of LMS with such personal data, a basis for adaptivity is provided.";;"none extracted value";"0,00";"0,00"
"TUW-140253";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\groundtruth\TUW-140253.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\result\result-TUW-140253-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\extracted\parscit\parscit-TUW-140253-xstream.xml"")";"Process types – a kind of behavioral types – specify constraints on message acceptance for the purpose of synchronization and to determine object usage and component behavior in object-oriented languages. So far process types have been regarded as a purely static concept for Actor languages incompatible with inherently dynamic programming techniques. We propose solutions of related problems causing the approach to become useable in more conventional dynamic and concurrent languagues. The proposed approach can ensure message acceptability and support local and static checking of race-free programs.";;"none extracted value";"0,00";"0,00"
"TUW-140308";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\groundtruth\TUW-140308.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\result\result-TUW-140308-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\extracted\parscit\parscit-TUW-140308-xstream.xml"")";"We show that algorithmic thinking is a key ability in informatics that can be developed independently from learning programming. For this purpose we use problems that are not easy to solve but have an easily understandable problem definition. A proper visualization of these problems can help to understand the basic concepts connected with algorithms: correctness, termination , efficiency, determinism, parallelism, etc. The presented examples were used by the author in a pre-university course, they may also be used in secondary schools to help understanding some concepts of computer science.";;"none extracted value";"0,00";"0,00"
"TUW-140533";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\groundtruth\TUW-140533.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\result\result-TUW-140533-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\extracted\parscit\parscit-TUW-140533-xstream.xml"")";"Formal specification and verification of software have made small but continuous advances throughout its long history, and have reached a point where commercial tools became available for verifying programs semi-automatically or automatically. The aim of the master thesis is to evaluate commercial and academic verification tools with respect to their usability in developing software and in teaching formal methods. The thesis will explain the theoretical foundation and compare the capabilities and characteristics of selected commercial and academic tools on concrete examples. The theoretical foundations deal on the one hand with the general ideas and principles of formal software verification, on the other hand present some internals of the selected tools to give a comprehensive understanding. The discussed tools are the Frege Program Prover, KeY, Perfect Developer, and the Prototype Verification System. The examples encompass simple standard computer science problems. The evaluation of these tools concentrates on the whole development process of specification and verification, not just on the verification results.";;"none extracted value";"0,00";"0,00"
"TUW-140867";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\groundtruth\TUW-140867.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\result\result-TUW-140867-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\extracted\parscit\parscit-TUW-140867-xstream.xml"")";"We present the status of formal methods at our university, and describe our course on formal software verification in more detail. We report our experiences in using Perfect Developer for the course assignments.";;"none extracted value";"0,00";"0,00"
"TUW-140895";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\groundtruth\TUW-140895.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\result\result-TUW-140895-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\extracted\parscit\parscit-TUW-140895-xstream.xml"")";"This master thesis presents an ant colony optimisation algorithm for the bounded diameter minimum spanning tree problem, a N P-hard combinatorial optimisation problem with various application fields, e.g. when considering certain aspects of quality in communication network design. The algorithm is extended with local optimisation in terms of a variable neighbourhood descent algorithm based on four different neighbourhood structures. These neighbourhood structures have been designed in a way to enable a fast identification of the best neighbouring solution. The proposed algorithm is empirically compared to various evolutionary algorithms and a variable neighbourhood search implementation on Euclidean instances based on complete graphs with up to 1000 nodes considering either solution quality as well as computation time. It turns out that the ant colony optimisation algorithm performs best among these heuristics with respect to quality of solution, but cannot reach the results of the variable neighbourhood search implementation concerning computation time.";;"none extracted value";"0,00";"0,00"
"TUW-140983";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\groundtruth\TUW-140983.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\result\result-TUW-140983-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\extracted\parscit\parscit-TUW-140983-xstream.xml"")";"In this paper we construct adaptive user profiles from tagging data. We present and evaluate an algorithm for creating such profiles, characterizing its behavior through statistical analysis.";;"none extracted value";"0,00";"0,00"
"TUW-141024";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\groundtruth\TUW-141024.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\result\result-TUW-141024-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\extracted\parscit\parscit-TUW-141024-xstream.xml"")";"Clinical practice guidelines are widely used to support medical staff in treatment planning and decision-making, whereas, the classification of different recommendations in the CPGs are one of the most important information sources to use. However, there is a lack of consensus amongst guideline developers, regarding those classification schemes. To address this problem, we mapped the different graded and ungraded evidence information used by different guideline developing organizations into our meta schema. In this paper we describe how guideline representation languages, such as Asbru and PROforma can be extended to model our meta schema.";;"none extracted value";"0,00";"0,00"
"TUW-141065";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\groundtruth\TUW-141065.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\result\result-TUW-141065-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\extracted\parscit\parscit-TUW-141065-xstream.xml"")";"The need to provide more holistic adaptivity to students has brought us to investigate the relationship between learning styles and working memory capacity (WMC). The aim of this investigation is to study the relationship between learning styles and WMC in order to get additional information about the students. This information can be used to make more holistic adaptivity possible by improving the student modelling process of both learning styles and WMC. An experiment with 297 participants was conducted. Findings suggest that relationships from WMC to the active/reflective, the sensing/intuitive, and the visual/verbal learning styles exist, whereas the suggested relationship from WMC to sequential/global learning styles could not be found.";;"none extracted value";"0,00";"0,00"
"TUW-141121";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\groundtruth\TUW-141121.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\result\result-TUW-141121-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\extracted\parscit\parscit-TUW-141121-xstream.xml"")";"Multimedia content can be described in versatile ways as its essence is not limited to one view. For music data these multiple views could be a song's audio features as well as its lyrics. Both of these modalities have their advantages as text may be easier to search in and could cover more of the 'content semantics' of a song, while omitting other types of semantic categorisation. (Psycho)acoustic feature sets, on the other hand, provide the means to identify tracks that 'sound similar' while less supporting other kinds of semantic categorisation. Those discerning characteristics of different feature sets meet users' differing information needs. We will explain the nature of text and audio feature sets which describe the same audio tracks. Moreover, we will propose the use of textual data on top of low level audio features for music genre classification. Further, we will show the impact of different combinations of audio features and textual features based on content words.";;"none extracted value";"0,00";"0,00"
"TUW-141140";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\groundtruth\TUW-141140.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\result\result-TUW-141140-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\extracted\parscit\parscit-TUW-141140-xstream.xml"")";"Current model-driven Web Engineering approaches (such as OO-H, UWE or WebML) provide a set of methods and supporting tools for a systematic design and development of Web applications. Each method addresses different concerns using separate models (content, navigation, presentation, business logic, etc.), and provide model compilers that produce most of the logic and Web pages of the application from these models. However, these proposals also have some limitations, especially for exchanging models or representing further modeling concerns, such as architectural styles, technology independence , or distribution. A possible solution to these issues is provided by making model-driven Web Engineering proposals interoperate, being able to complement each other, and to exchange models between the different tools. MDWEnet is a recent initiative started by a small group of researchers working on model-driven Web Engineering (MDWE). Its goal is to improve current practices and tools for the model-driven development of Web applications for better interoperability. The proposal is based on the strengths of current model-driven Web Engineering methods, and the existing experience and knowledge in the field. This paper presents the background, motivation, scope, and objectives of MDWEnet. Furthermore, it reports on the MDWEnet results and achievements so far, and its future plan of actions.";;"none extracted value";"0,00";"0,00"
"TUW-141336";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\groundtruth\TUW-141336.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\result\result-TUW-141336-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\extracted\parscit\parscit-TUW-141336-xstream.xml"")";"The QXOR-SAT problem is the quantified version of the satisfiability problem XOR-SAT in which the connective exclusive-or is used instead of the usual or. We study the phase transition associated with random QXOR-SAT instances. We give a description of this phase transition in the case of one alternation of quantifiers, thus performing an advanced practical and theoretical study on the phase transition of a quantified problem.";;"none extracted value";"0,00";"0,00"
"TUW-141618";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\groundtruth\TUW-141618.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\result\result-TUW-141618-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\extracted\parscit\parscit-TUW-141618-xstream.xml"")";"On-line analytical processing (OLAP) is a powerful data analysis technology. However, current implementations bypass the 3-layer model of databases for performance reasons and create a redundant database
	
The clear separation between external views, conceptional model and internal physical representations in the three layer database architecture constitutes one of the key factors to the success of database technology for on-line transaction processing (OLTP). However, experience has shown that it is not feasible to construct an on-line analytical processing (OLAP) view onto the data within this model. As a compromise, materialized views were introduced. Yet, these limit the applicability of OLAP to fairly standard applications. Some application domains such as the analysis of stock market data or telecommunication systems with a diverse structure and unpredictable aggregation and query patterns can not be timely handled using pre-materialization. As a result, the benefits of OLAP are not available when building analysis and visualization tool for this kind of applications.

We envision OLAP-on-demand as a foundation for the re-unification of the transaction processing and analysis in databases. Instead of pre-materializing expected queries, analysts can integrate the available data sources on the fly where the analysis of information is not delayed by certain``certain``update windows''. Although current computing hardware does not provide the necessary data throughput, we expect future generation systems to cope with the high demands of this approach. In the meantime, parallel and distributed query processing can be used to achieve the required response times.

As an ultimate goal the realisation of Codd's formulaic model is envisioned.";;"none extracted value";"0,00";"0,00"
"TUW-141758";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\groundtruth\TUW-141758.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\result\result-TUW-141758-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\extracted\parscit\parscit-TUW-141758-xstream.xml"")";"Medical information is often stored in a narrative way, which makes the automated processing a difficult and time-consuming task. Persons responsible for the authoring of medical documents do not take care of a further processing with automated systems. So, information stored in medical writings is not directly usable for the processing with computers. Due to this, efforts have been made to transfer these narrative documents in a format easier processable with computers. This matter of fact also applies to clinical practice guidelines (CPGs). As many medical documents , CPGs are written in a narrative speech as well, without regards to a computer-assisted processing. For the implementation of CPGs in medical facilities an automated processing is therefore desirable. An important fact is that a lot of information in CPGs is provided in a negated form, expressing that certain circumstances in patients or treatments are not available, existing or applicable. Although negated, this information is nevertheless very useful, since it can express the absence of certain conditions or diseases in patients. Moreover, negations can describe which treatment options should not be taken into account for a given patient, helping a practising physician or nurse in his/her decision process for the assortment of a proper treatment. Thus, a proper Negation Detection in CPGs is an important task for the automated processing of this type of medical documents. It helps to accelerate the decision making process and can support medical staff in their care for patients. We developed algorithms capable of Negation Detection in CPGs. We use syntactical methods provided by the English language to achieve a precise detection of occuring negations. According to our results we are convinced that the involvement of syntactical methods can improve Negation Detection, not only in medical writings but also in arbitrary narrative texts.";;"none extracted value";"0,00";"0,00"
"TUW-168222";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\groundtruth\TUW-168222.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\result\result-TUW-168222-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\extracted\parscit\parscit-TUW-168222-xstream.xml"")";"Recent research results on communication science mention a relation between the inter-locutor's perceptional preferences, his or her mental representations and his or her choice of expressions. However, there is not any tool for analyzing such. In my research I combined lexical with computational linguistic methods to develop a software prototype that is able to analyze text on the usage of perceptional expressions. This analyzing tool can help to identify the interlocutor's perceptional preference for easier meeting his or her way of thinking and thereby facilitate the understanding.";;"none extracted value";"0,00";"0,00"
"TUW-168482";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\groundtruth\TUW-168482.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\result\result-TUW-168482-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\extracted\parscit\parscit-TUW-168482-xstream.xml"")";"Formal dialogue games are a traditional approach to characterize the semantics of logics. In the 1970s Robin Giles attempted to provide an operational foundation for formal reasoning in physical theories by dialogue games based on atomic experiments that may show dispersion. This thesis motivates, describes and analyzes his approach and the connection to t-norm based fuzzy logics. We give a short introduction into t-norms and many-valued logics based on t-norms. In particular we focus on three fundamental t-norm based fuzzy logics: Lukasiewicz Logic, Gödel Logic, and Product Logic. We present and discuss several approaches for extending the game rules of Giles's Game in order to make it adequate for Gödel Logic and Product Logic. Moreover, we give hints at a strong correspondence between winning strategies in the game and derivations in an analytic proof system based on relational hypersequents. Another type of dialogue games are truth comparison games. This type is suitable for Gödel Logic and relates more to the degree based semantics of that logic than Giles's Game. We present the game and discuss winning strategies for both players indicating the validity or refutability of a formula. Additionally, several utilities implemented in the context of this thesis are presented. Amongst these is a web-based application which allows for the interactive exploration of Giles's Game and its extensions.";;"none extracted value";"0,00";"0,00"
"TUW-169511";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\groundtruth\TUW-169511.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\result\result-TUW-169511-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\extracted\parscit\parscit-TUW-169511-xstream.xml"")";;;"none extracted value";"none expected";"NA"
"TUW-172697";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\groundtruth\TUW-172697.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\result\result-TUW-172697-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\extracted\parscit\parscit-TUW-172697-xstream.xml"")";"Creating a concrete plan for preserving an institution's collection of digital objects requires the evaluation of available solutions against clearly defined and measurable criteria. Preservation planning aids in this decision making process to find the best preservation strategy considering the institution's requirements, the planning context and possible actions applicable to the objects contained in the repository. Performed manually, this evaluation of possible solutions against requirements takes a good deal of time and effort. In this demonstration, we present Plato, an interactive software tool aimed at creating preservation plans.";;"none extracted value";"0,00";"0,00"
"TUW-174216";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\groundtruth\TUW-174216.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\result\result-TUW-174216-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\extracted\parscit\parscit-TUW-174216-xstream.xml"")";"During the last decades, digital objects have become the primary medium to create, shape, and exchange information. However, in contrast to analog objects such as books that directly represent their content, digital objects are not usable without a corresponding technical environment. The fast changes in these environments and in formats and technologies mean that digital documents have a short lifespan before they become obsolete. Digital preservation, i.e. actions to ensure longevity of digital information, thus has become a pressing challenge. The dominant strategies prevailing today are migration and emulation; for each strategy, different tools are available. When converting an object to a different representation, a validation of the content is needed to verify that the transformed objects are still authentically representing the same intellectual content. This validation so far is largely done manually, which is infeasible for large collections.
Preservation planning supports decision makers in reaching accountable decisions by evaluating potential strategies against well-defined requirements. Especially the evaluation of different migration tools for digital preservation has to rely on validating the converted objects and thus on an analysis of the logical structure and the content of documents. Existing approaches for characterising and describing objects do not attempt to fully extract the informational content of digital objects and thus are not suffficient for an in-depth validation of transformed content.
This paper describes the eXtensible Characterisation Languages (XCL) that support the automatic validation of document conversions and the evaluation of migration quality by hierarchically decomposing a document and representing documents from different sources in an abstract XML language. The description language XCDL provides an abstract representation of digital content in XML, while the extraction language XCEL allows an extraction engine to create such an abstract description by mapping file format structures to XCDL concepts. We present the context of the development of these languages and tools and describe the overall concept and features of the languages. We further give examples and show how the languages can be applied to the evaluation of digital preservation solutions in the context of preservation planning.";;"none extracted value";"0,00";"0,00"
"TUW-175428";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\groundtruth\TUW-175428.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\result\result-TUW-175428-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\extracted\parscit\parscit-TUW-175428-xstream.xml"")";"Within this thesis a real-world problem related to a warehouse for spare parts is considered. Regarding several constraints typically stated by spare parts suppliers the time needed to collect articles should be minimized. Based on continuously arriving orders by customers predefined delivery times and capacity constraints have to be met. To accomplish this task efficient pickup tours need to be determined which is the main issue covered by this work which comes to an end with experimental results of a concrete implementation of the proposed approach.
The algorithm presented embeds a specifically developed dynamic program for computing optimal walks through the warehouse into a general variable neighborhood search (VNS) scheme. Several stages are used for first splitting up all orders, then creating tours out of the results and finally assigning them to available workers. The VNS uses a variant of the variable neighborhood descent (VND) as local improvement procedure. While the neighborhood structures defined are intended to produce candidate solutions, a dynamic program specially designed to compute optimal order picking tours is used to evaluate them. For this purpose properties specific to warehouses are exploited such to compute optimal routes within polynomial time. The final assignment of workers to tours is realized by another VNS. The task is then to find an allocation such that the last article to be picked up will be collected as early as possible.
Evaluations of experimental results of a concrete implementation indicate that the presented approach provides valuable pickup plans and computation times can be kept low. Moreover the performed test runs have been compared to a reference solution which was computed based on an approach found in relevant literature. A detailed analysis of the obtained results showed that the application of the proposed approach to real-world instances is promising whereas the savings with respect to working time can be kept high. Overall an efficient as well as effective approach is introduced to solve this real-world problem.";;"none extracted value";"0,00";"0,00"
"TUW-176087";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\groundtruth\TUW-176087.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\result\result-TUW-176087-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\extracted\parscit\parscit-TUW-176087-xstream.xml"")";"Reviews and few non-controlled studies showed the effectiveness of several specific designed computer video-games as an additional form of treatment in several areas. However, there is a lack in the literature of specially designed serious-games for treating mental disorders. Playmancer (ICT European initiative) aims to develop and assess a serious videogame that may help to treat underlying processes (e.g. lack of self-control strategies) in Eating and Impulse control disorders. Preliminary data will be shown.";;"none extracted value";"0,00";"0,00"
"TUW-177140";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\groundtruth\TUW-177140.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\result\result-TUW-177140-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\extracted\parscit\parscit-TUW-177140-xstream.xml"")";"The system cc? is a tool for testing correspondence between propo-sitional logic programs under the answer-set semantics with respect to different refined notions of program correspondence. The underlying methodology of cc? is to reduce a given correspondence problem to the satisfiability problem of quantified propositional logic and to employ extant solvers for the latter language as back-end inference engines. In a previous version of cc?, the system was designed to test correspondence between programs based on relativised strong equivalence under answer-set projection. Such a setting generalises the standard notion of strong equivalence by taking the alphabet of the context programs as well as the projection of the compared answer sets to a set of designated output atoms into account. This paper outlines a newly added component of cc? for testing similarly parameterised correspondence problems based on uniform equivalence.";;"none extracted value";"0,00";"0,00"
"TUW-179146";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\groundtruth\TUW-179146.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\result\result-TUW-179146-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\extracted\parscit\parscit-TUW-179146-xstream.xml"")";"The publish/subscribe paradigm is a common concept for delivering events from information producers to consumers in a decoupled manner. Some approaches allow durable subscriptions or the transportation of events even to mobile subscribers in a dynamic network infrastructure. However, in the safety-critical telematics durable delivery of events is not sufficient enough. Short network connectiv-ity time and small bandwidth limit the number and size of events to be transmitted hence relevant information needed for safety-critical decision making may not be timely delivered.
In this paper we propose the integration of publish/ subscribe systems and Aspect-oriented Space Containers (ASC) distributed via Distributed Hash Tables (DHT) in the network. The approach allows storage, manipulation, pre-processing, and prioritization of messages sent to mobile peers during bursts of connectivity.
The benefits of the proposed approach are a) less complex application logic due to the processing capabilities of Space Containers, and b) increased efficiency due to delivery of essential messages only aggregated and processed while mobile peers are not connected. We describe the architecture of the proposed approach and explain its benefits by means of an industry use case.";;"none extracted value";"0,00";"0,00"
"TUW-180162";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\groundtruth\TUW-180162.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\result\result-TUW-180162-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\extracted\parscit\parscit-TUW-180162-xstream.xml"")";"Web spam denotes the manipulation of web pages with the sole intent to raise their position in search engine rankings. Since a better position in the rankings directly and positively affects the number of visits to a site, attackers use different techniques to boost their pages to higher ranks. In the best case, web spam pages are a nuisance that provide undeserved advertisement revenues to the page owners. In the worst case, these pages pose a threat to Internet users by hosting malicious content and launching drive-by attacks against unsuspecting victims. When successful, these drive-by attacks then install malware on the victims' machines. In this paper, we introduce an approach to detect web spam pages in the list of results that are returned by a search engine. In a first step, we determine the importance of different page features to the ranking in search engine results. Based on this information, we develop a classification technique that uses important features to successfully distinguish spam sites from legitimate entries. By removing spam sites from the results, more slots are available to links that point to pages with useful content. Additionally, and more importantly, the threat posed by malicious web sites can be mitigated, reducing the risk for users to get infected by malicious code that spreads via drive-by attacks.";;"none extracted value";"0,00";"0,00"
"TUW-181199";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\groundtruth\TUW-181199.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\result\result-TUW-181199-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\extracted\parscit\parscit-TUW-181199-xstream.xml"")";"Answer-set programming (ASP) is an emerging logic-programming paradigm that strictly separates the description of a problem from its solving methods. Despite its semantic elegance, ASP suffers from a lack of support for program developers. In particular, tools are needed that help engineers in detecting erroneous parts of their programs. Unlike in other areas of logic programming , applying tracing techniques for debugging logic programs under the answer-set semantics seems rather unnatural, since employing imperative solving algorithms would undermine the declarative flavour of ASP. In this paper, we present the system spock, a debugging support tool for answer-set programs making use of ASP itself. The implemented techniques maintain the declarative nature of ASP within the debugging process and are independent of the actual computation of answer sets.";;"none extracted value";"0,00";"0,00"
"TUW-182414";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\groundtruth\TUW-182414.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\result\result-TUW-182414-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\extracted\parscit\parscit-TUW-182414-xstream.xml"")";"In den letzten Jahren haben Bibliotheken und Archive zunehmend die Aufgabe übernommen, neben konventionellen Publikationen auch Inhalte aus dem World Wide Web zu sammeln, um so diesen wertvollen Teil unseres kulturellen Erbes zu bewahren und wichtige Informationen langfristig verfügbar zu halten. Diese massiven Datensammlungen bieten faszinierende Möglichkeiten, rasch Zugriff auf wichtige Informationen zu bekommen, die im Live-Web bereits verloren gegangen sind. Sie sind eine unentbehrliche Quelle für Wissenschafter, die in der Zukunft die gesellschaftliche und technologische Entwicklung unserer Zeit nachvollziehen wollen.
Auf der anderen Seite stellt eine derartige Datensammlung aber einen völlig neuen Datenbestand dar, der nicht nur rechtliche, sondern auch zahlreiche ethische Fragen betreffend seine Nutzung aufwirft. Diese werden in dem Ausmaß zunehmen, in dem die technischen Möglichkeiten zur automatischen Analyse und Interpretation dieser Daten leistungsfähiger werden. Da sich die meisten Web-Archivierungsinitiativen dieser Problematik bewusst sind, bleibt die Nutzung der Daten derzeit meist stark eingeschränkt, oder es wird eine Art von ""Opt-Out""-Möglichkeit vorgesehen, wodurch Webseiteninhaber die Aufnahme ihrer Seiten in ein Webarchiv ausschließen können. Mit beiden Ansätzen können Webarchive ihr volles Nutzungspotential nicht ausschöpfen.
Das World Wide Web hat sich zu einem integralen Bestandteil unserer Publikations-und Kommunikationskultur entwickelt. Als solches bietet es uns einen reichhaltigen Schatz an wertvollen Informationen, die teilweise ausschließlich in elektronischer Form verfügbar sind, wie z.B. Informationsportale, Informationen zu zahlreichen Projekten und Bürgerinitiativen, Diskussionsforen, soziale Netze und Ähnliches. Weiters beeinflussen die technischen Möglichkeiten sowohl die Art der Gestaltung von Webseiten, als auch die Art, wie wir mit Information umgehen, wie unsere Gesellschaft vernetzt ist, wie sich Information ausbreitet bzw. wie sie genutzt wird. All dies stellt einen immens wertvollen Datenbestand dar, dessen Bedeutung uns teilweise erst bewusst werden mag, wenn dieser nicht mehr verfügbar ist. Dieser Artikel beschreibt einleitend kurz die Technologien, die zur Sammlung von Webinhalten zu Archivierungszwecken verwendet werden. Er hinterfragt Annahmen betreffend die freie Verfügbarkeit der Daten und unterschiedliche Nutzungsarten. Darauf aufbauend identifiziert er eine Reihe von offenen Fragen, deren Lösung einen breiteren Zugriff und Nutzung von Webarchiven erlauben könnte.";;"none extracted value";"0,00";"0,00"
"TUW-182899";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\groundtruth\TUW-182899.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\result\result-TUW-182899-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\extracted\parscit\parscit-TUW-182899-xstream.xml"")";"As photographic technologies continue to develop, so too do the social practices surrounding their use. The focus of this paper is on the social practices surrounding images captured from a new photographic device – SenseCam – which, rather than capturing individual images when triggered by the user, automatically captures a series of images. This paper is concerned with the use of SenseCam digital images in social contexts where there is a professional purpose: supporting the collaborative reflective practices of school teachers and university tutors as part of their professional development. Analysis of video data collected from 16 in-situ case studies of reflective discussions show evidence that reflection took place as defined in the literature. Further, the phototalk around SenseCam images was found to benefit reflection in these social situations through promoting a rich shared understanding of the lesson context: supporting return to the experience, sharing of background context, grounding conversations, illustrating and providing evidence, and allowing people to see more. The paper concludes with a discussion on how different features of SenseCam images, such as variable quality, lack of audio, incompleteness, helped in this reflection or not. Finally implications from this work and participants comments are used to suggest ways in which SenseCam may be used in the future in teacher and tutors social reflection.";;"none extracted value";"0,00";"0,00"
"TUW-185321";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\groundtruth\TUW-185321.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\result\result-TUW-185321-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\extracted\parscit\parscit-TUW-185321-xstream.xml"")";"The semantic heterogeneity of Open Source Software (OSS) development projects comes from the using of different tools and models by the various stakeholders. These differences make the process of integration become difficult, since the project managers should recognize the different structure of the tools and models for analyzing the state of the projects. This manual analysis is costly and error prone. In this work we propose a semantic web technology approach to bridge these semantic heterogeneities, by using engineering knowledge base (EKB). The EKB enables mapping between local and domain ontology layers to allow querying the local tool knowledge using the domain-level knowledge and syntax. We empirically evaluate the feasibility of an EKB-based project monitoring system based on real-world data.";;"none extracted value";"0,00";"0,00"
"TUW-185441";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\groundtruth\TUW-185441.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\result\result-TUW-185441-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\extracted\parscit\parscit-TUW-185441-xstream.xml"")";"Shape analysis is a static program analysis technique for discovering properties of heap-allocated data structures. It is crucial to finding software bugs or to verify high-level correctness properties. Various analyses have been introduced but their relation in terms of precision often remains unclear as different analyses use different abstractions of the heap. The aim of our work is to compare the precision of shape analyses. We propose a novel algorithm based on three-valued logic that extracts alias sets from shape graphs. Smaller sets are more precise and indicate a more precise underlying shape analysis. Using this metric, we experimentally compare – for the first time – the relative quality of the state-of-the-art graph-based shape analyses and make recommendations concerning the combination of analysis parameters.";;"none extracted value";"0,00";"0,00"
"TUW-186227";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\groundtruth\TUW-186227.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\result\result-TUW-186227-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\extracted\parscit\parscit-TUW-186227-xstream.xml"")";"One indispensable precondition for designing a functional software product for the modeling and execution of a computerized clinical practice guideline (CPG) is the comprehensive investigation of the different user groups involved and the issues they encounter. This led us to conduct a comprehensive literature study about the tasks involved in modeling a CPG into a formal representation as well as about the information needs of caregivers, i.e., physicians and nurses, and last but not least the information needs of patients. We have assessed and categorized the above mentioned information in order to create a reliable starting point for the development of a functional software tool.";;"none extracted value";"0,00";"0,00"
"TUW-189842";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\groundtruth\TUW-189842.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\result\result-TUW-189842-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\extracted\parscit\parscit-TUW-189842-xstream.xml"")";"New technologies open up possibilities for designing interactive experiences that can engage and motivate post-stroke survivors to undertake what would otherwise be boring repetitive movements. In this paper we outline a few of the challenges we met as part of the cross-disciplinary Motivating Mobility project. These are: the extended 'user'; autonomy and motivation; and early prototype studies.";;"none extracted value";"0,00";"0,00"
"TUW-191715";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\groundtruth\TUW-191715.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\result\result-TUW-191715-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\extracted\parscit\parscit-TUW-191715-xstream.xml"")";"In this paper we give an overview of the current research trends and explore the challenges in several subfields of the scientific discipline of computer graphics: interactive and photorealistic rendering, scientific and information visualization, and visual analytics. Five challenges are extracted that play a role in each of these areas: scalability, semantics, fusion, interaction, acquisition. Of course, not all of these issues are disjunct to each other, however the chosen structure allows for a easy to follow overview of the concrete future challenges.";;"none extracted value";"0,00";"0,00"
"TUW-191977";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\groundtruth\TUW-191977.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\result\result-TUW-191977-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\extracted\parscit\parscit-TUW-191977-xstream.xml"")";"Digital Library (DL) interoperability requires addressing a variety of issues associated with functionality. We report on the analysis and solutions identified by the Functionality Working Group of the DL.org project during its deliberations on DL interoperability. Ultimately, we hope that work based on our perspective will lead to improved architectures and software, as well as to greater interoperability, for next-generation DL systems.";;"none extracted value";"0,00";"0,00"
"TUW-192724";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\groundtruth\TUW-192724.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\result\result-TUW-192724-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\extracted\parscit\parscit-TUW-192724-xstream.xml"")";"Human resource strategy can emerge within a decentralized decision structure that gives managers autonomy to take responsive actions while overall strategic direction is considered within a strategic planning process. This study defines the concept of employee selection (especially strategic employee selection) and hypothesizes on the positive correlation between innovation characteristic in SME and value of strategic employee (the so called personnel usefulness function). An empirical study illustrates the importance of both elements in an integrative human resource strategy formation process particularly for firms operating in the international environments.";;"none extracted value";"0,00";"0,00"
"TUW-194085";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\groundtruth\TUW-194085.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\result\result-TUW-194085-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\extracted\parscit\parscit-TUW-194085-xstream.xml"")";"Topology is the general mathematical theory of convergence. Distributed computing is the formal investigation of communicating concurrent processes. We explore applications of topology to distributed computing in two directions: (1) Point-set topology and (2) algebraic topology.
We use the former to study the topological structure of infinite execution trees. This enables us to unify a number of impossibility proofs, in particular, the impossibility of distributed consensus — the task of all processes in a system agreeing on a single value — in various (close to) asynchronous systems with crash failures.
The latter is used to look into the combinatorial structure of configurations, i.e., the collection of current process states in the system. Configurations are regarded as simplices in a simplicial complex, and topological incompatibility of such complexes is utilized to prove the impossibility of a generalization of distributed consensus in certain systems. The particular problem considered is k-set agreement, which is the task of letting all processes agree to values within a set of at most k elements.";;"none extracted value";"0,00";"0,00"
"TUW-194561";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\groundtruth\TUW-194561.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\result\result-TUW-194561-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\extracted\parscit\parscit-TUW-194561-xstream.xml"")";"We propose a generalized version of context-sensitivity in term rewriting based on the notion of ""forbidden patterns"". The basic idea is that a rewrite step should be forbidden if the redex to be contracted has a certain shape and appears in a certain context. This shape and context is expressed through forbidden patterns. In particular we analyze the relationships among this novel approach and the commonly used notion of context-sensitivity in term rewriting, as well as the feasibility of rewriting with forbidden patterns from a computational point of view. The latter feasibility is characterized by demanding that restricting a rewrite relation yields an improved termination behaviour while still being powerful enough to compute meaningful results. Sufficient criteria for both kinds of properties in certain classes of rewrite systems with forbidden patterns are presented.";;"none extracted value";"0,00";"0,00"
"TUW-194660";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\groundtruth\TUW-194660.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\result\result-TUW-194660-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\extracted\parscit\parscit-TUW-194660-xstream.xml"")";"Interaction Nets are a novel model of computation based on graph rewriting. Their main properties are parallel evaluation and sharing of computation, which leads to efficient programs. However, Interaction Nets lack several features that allow for their convenient use as a programming language. In this paper , we describe the implementation of an extension for pattern matching of interaction rules. Furthermore, we show the cor-rectness of the implementation and discuss its complexity.";;"none extracted value";"0,00";"0,00"
"TUW-197422";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\groundtruth\TUW-197422.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\result\result-TUW-197422-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\extracted\parscit\parscit-TUW-197422-xstream.xml"")";"Visual Analytics strongly emphasizes the importance of interaction. However, until now, interaction is only sparingly treated as subject matter on its own. How and why interactivity is beneficial to gain insight and make decisions is mostly left in the dark. Due to this lack of initial direction, it seems important to make further attempts in facilitating a deeper understanding of the concept of interactivity. Therefore, different perspectives towards interactivity are discussed and cognitive theories and models are investigated. The main aim of this paper is to broaden the view on interaction and spark further discussion towards a sound theoretical grounding for the field.";;"none extracted value";"0,00";"0,00"
"TUW-197852";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\groundtruth\TUW-197852.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\result\result-TUW-197852-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\extracted\parscit\parscit-TUW-197852-xstream.xml"")";"To describe the structure of a system, the UML Class Diagram yields the means-of-choice. Therefor, the Class Diagram provides concepts like class, attribute, operation, association, generalization, aggregation, enumeration, etc. When students are introduced to this diagram, they often have to solve exercises where texts in natural language are given and they have to model the described systems. When analyzing such exercises, it becomes evident that certain kinds of phrases describing a particular concept appear again and again contextualized to the described domain.
In this paper, we present an approach which allows the automatic generation of tex-tual specifications from a given Class Diagram based on standard phrases in natural language. Besides supporting teachers in preparing exercises, such an approach is also valuable for various e-learning scenarios.";;"none extracted value";"0,00";"0,00"
"TUW-198400";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\groundtruth\TUW-198400.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\result\result-TUW-198400-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\extracted\parscit\parscit-TUW-198400-xstream.xml"")";"Although our society is critically dependent on software systems, these systems are mainly secured by protection mechanisms during operation instead of considering security issues during software design. Deficiencies in software design are the main reasons for security incidents, resulting in severe economic consequences for (i) the organizations using the software and (ii) the development companies. Lately, model-driven development has been proposed in order to increase the quality and thereby the security of software systems. This paper evaluates current efforts that position security as a fundamental element in model-driven development, highlights their deficiencies and identifies current research challenges. The evaluation shows that applying special-purpose methods to particular aspects of the problem is more suitable than applying generic ones, since (i) the problem can be represented on the proper abstraction level, (ii) the user can build on the knowledge of experts, and (iii) the available tools are more efficient and powerful.";;"none extracted value";"0,00";"0,00"
"TUW-198401";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\groundtruth\TUW-198401.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\result\result-TUW-198401-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\extracted\parscit\parscit-TUW-198401-xstream.xml"")";"IT security incidents pose a major threat to the efficient execution of corporate strategies and business processes. Although companies generally spend a lot of money on security companies are often not aware of their spending on security and even more important if these investments into security are effective. This paper provides decision makers with an overview of decision support techniques, describes pros and cons of these methodologies.";;"none extracted value";"0,00";"0,00"
"TUW-198405";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\groundtruth\TUW-198405.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\result\result-TUW-198405-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\extracted\parscit\parscit-TUW-198405-xstream.xml"")";"As business processes gain more importance in todays business environment, their unimpeded execution is crucial for a company's success. Corporate decision makers are faced with a wide spectrum of potential risks on the one hand and a plenitude of security safeguards on the other hand. This paper gives an overview of a new approach for the elicitation of security requirements of business processes, for the analysis of threats and vulnerabilities and for the interactive selection of an optimal security level according to the given business processes as well as multiple objectives. It provides decision makers with an instrument for interactively defining Secure Business Processes that are economically and technically efficient.";;"none extracted value";"0,00";"0,00"
"TUW-198408";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\groundtruth\TUW-198408.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\result\result-TUW-198408-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\extracted\parscit\parscit-TUW-198408-xstream.xml"")";"In der betrieblichen Praxis kommt der komponentenbasierten Software-Entwicklung hoher Stellenwert zu. Angesichts mehrfacher Zielsetzungen und vielfältiger Nebenbedingungen ist dabei insbesondere die Auswahl der ""besten"" Kombination von Komponenten ein nicht-triviales Entscheidungsproblem. Bislang wurden hierfür vor allem die Nutzwertanalyse bzw. der Analytic Hierarchy Process zur Entscheidungsunterstützung vorgeschlagen, wobei aber beide eine Reihe von Unzulänglichkeiten aufweisen. Diese Arbeit will dazu nunmehr eine Alternative anbieten. Darin werden in einem ersten Schritt zunächst (zulässige) Pareto-effiziente Kombinationen von Software-Komponenten berechnet und die Entscheidungsträger dann im zweiten Schritt interaktiv bei der Suche nach jener Variante unterstützt, die einen Ziele-Mix in Aussicht stellt, der den jeweiligen individuellen Präferenzen am besten entspricht. Das neue Verfahren zeichnet sich im Vergleich zu herkömmlichen Ansätzen insbesondere durch den Verzicht auf umfangreiche a priori Präferenzinformationen (wie z.B. Zielgewichtungen) aus. Darüber hinaus kann es ohne großen Anpassungsaufwand in bestehende Vorgehensmodelle zur Auswahl von Software-Komponenten integriert werden.";;"none extracted value";"0,00";"0,00"
"TUW-200745";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\groundtruth\TUW-200745.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\result\result-TUW-200745-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\extracted\parscit\parscit-TUW-200745-xstream.xml"")";"In this work, an algorithm for the generalized minimum spanning tree problem (GMST) is developed. Given is a complete graph where the nodes are partitioned into clusters. A solution is a spanning tree which contains exactly one node of each cluster and its costs are minimal. This problem is NP-hard. In this work, a heuristic is developed for this problem.
	
In this method, an evolutionary algorithm (EA) is used with two different solution archives. Using a solution archive, it is possible to store solutions generated by the EA in order to detect duplicates and converts duplicate solutions into new solutions. One solution archive based on an encoding in which the spanned nodes of each cluster in the solution are stored. The other archive is based on an encoding which characterizes the connections between the clusters.

These archives are extended by a bounding strategy based on the branch-and-bound technique. They try to calculate appropriate bounds at a convenient positions which give information about how good the solutions in the respective area of the archive can be in the best case. If a bound was found which is worse than the best known solution, the solutions are unattractive in the course of the algorithm and will not be considered. Therefore inferior solutions can be detected at an early stage and only promising solutions that can bring improvements will be pursued.

In addition to the bounding strategy a nearest neighbor approach is implemented in which a cluster attached to the spanning tree is preferred among the the n nearest neighboring clusters.

Tests were carried out in which the bounding strategy was used in the different variants. These tests led to the conclusion that the bounding strategy leads to an improvement in comparison to the ""normal"" archives. The comparison between the archives shows that the pop version lead to better results than the gosh version. When both archives are used simultaneously, the results are better than the results of the other two variants.";;"none extracted value";"0,00";"0,00"
"TUW-200748";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\groundtruth\TUW-200748.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\result\result-TUW-200748-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\extracted\parscit\parscit-TUW-200748-xstream.xml"")";"The Rooted Delay-Constrained Steiner Tree Problem (RDCSTP) is a variant of the well-known Steiner Tree Problem on a graph in which the paths to all terminal nodes are restricted by a certain maximum delay. The problem mostly appears in the context of network routing for multicasts, i.e., sending packages from a fixed source to a subset of other participants in the network. Since the RDCSTP belongs to the class of N P-hard problems it is in general not possible to solve large instances exactly in a reasonable amount of time. Therefore, the focus mostly lies on developing good heuristics that can still solve large instances comparatively fast to near optimality.
In this thesis a Multilevel Refinement heuristic – which has already been successfully applied to other problems like the Graph Partitioning Problem – is implemented as an improvement heuristic for the RDCSTP. In the general approach of this metaheuristic the problem's complexity is first iteratively reduced while still maintaining its general characteristics. The problem is thereby simplified and can at the top level finally easily be solved. Then, the solution on this highest level is refined until a solution for the original problem is obtained.
The algorithm introduced here implements the Multilevel Refinement approach as an improvement heuristic, iteratively changing an existing solution. However, it is designed in a way that also allows it to be used to construct an initial solution. Another distinctiveness is that, due to the additional delay constraints, supplementary data structures have to be used to avoid creating invalid solutions on higher levels as much as possible. In the refinement phase an additional improvement algorithm, the Key Path Improvement, is executed on each level, drastically increasing result quality.
Experimental tests are carried out, evaluating the performance of the algorithm on large instances and comparing it to other algorithms in the literature. The obtained results are promising and indicate that the Multilevel Refinement metaheuristic is indeed a competitive approach for the RDCSTP.";;"none extracted value";"0,00";"0,00"
"TUW-200948";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\groundtruth\TUW-200948.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\result\result-TUW-200948-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\extracted\parscit\parscit-TUW-200948-xstream.xml"")";"We introduce a new approach for establishing fixed-parameter tractability of problems parameterized above tight lower bounds or below tight upper bounds. To illustrate the approach we consider two problems of this type of unknown complexity that were introduced by Mahajan, Raman and Sikdar (J. Comput. Syst. Sci. 75, 2009). We show that a generalization of one of the problems and three nontrivial special cases of the other problem admit kernels of quadratic size.";;"none extracted value";"0,00";"0,00"
"TUW-200950";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\groundtruth\TUW-200950.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\result\result-TUW-200950-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\extracted\parscit\parscit-TUW-200950-xstream.xml"")";"We study the complexity of several coloring problems on graphs, pa-rameterized by the treewidth t of the graph:
(1) The list chromatic number ?l(G) of a graph G is defined to be the smallest positive integer r, such that for every assignment to the vertices v of G, of a list Lv of colors, where each list has length at least r, there is a choice of one color from each vertex list Lv yielding a proper coloring of G. We show that the problem of determining whether ?l(G) ? r, the LIST CHROMATIC NUMBER problem, is solvable in linear time for every fixed treewidth bound t. The method by which this is shown is new and of general applicability.
(2) The LIST COLORING problem takes as input a graph G, together with an assignment to each vertex v of a set of colors Cv. The problem is to determine whether it is possible to choose a color for vertex v from the set of permitted colors Cv, for each vertex, so that the obtained coloring of G is proper. We show that this problem is W [1]-hard, parameterized by the treewidth of G. The closely related PRECOLORING EXTENSION problem is also shown to be W [1]-hard, pa-rameterized by treewidth.
(3) An equitable coloring of a graph G is a proper coloring of the vertices where the numbers of vertices having any two distinct colors differs by at most one. We show that the problem is hard for W [1], parameterized by (t, r). We also show that a list-based variation, LIST EQUITABLE COLORING is W [1]-hard for trees, parameterized by the number of colors on the lists.";;"none extracted value";"0,00";"0,00"
"TUW-200959";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\groundtruth\TUW-200959.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\result\result-TUW-200959-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\extracted\parscit\parscit-TUW-200959-xstream.xml"")";"We consider monotonicity problems for graph searching games. Variants of these games – defined by the type of moves allowed for the players – have been found to be closely connected to graph decompositions and associated width measures such as path-or tree-width.
Of particular interest is the question whether these games are monotone, i.e. whether the cops can catch a robber without ever allowing the robber to reach positions that have been cleared before. The monotonicity problem for graph searching games has intensely been studied in the literature, but for two types of games the problem was left unresolved. These are the games on digraphs where the robber is invisible and lazy or visible and fast. In this paper, we solve the problems by giving examples showing that both types of games are non-monotone.
Graph searching games on digraphs are closely related to recent proposals for digraph decom-positions generalising tree-width to directed graphs. These proposals have partly been motivated by attempts to develop a structure theory for digraphs similar to the graph minor theory developed by Robertson and Seymour for undirected graphs, and partly by the immense number of algorith-mic results using tree-width of undirected graphs and the hope that part of this success might be reproducible on digraphs using a ""directed tree-width"". For problems such as disjoint paths and Hamiltonicity, it has indeed been shown that they are tractable on graphs of small directed tree-width. However, the number of such examples is still small.
We therefore explore the limits of the algorithmic applicability of digraph decompositions. In particular, we show that various natural candidates for problems that might benefit from digraphs having small ""directed tree-width"" remain NP-complete even on almost acyclic graphs.";;"none extracted value";"0,00";"0,00"
"TUW-201066";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\groundtruth\TUW-201066.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\result\result-TUW-201066-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\extracted\parscit\parscit-TUW-201066-xstream.xml"")";;;"none extracted value";"none expected";"NA"
"TUW-201160";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\groundtruth\TUW-201160.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\result\result-TUW-201160-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\extracted\parscit\parscit-TUW-201160-xstream.xml"")";;;"none extracted value";"none expected";"NA"
"TUW-201167";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\groundtruth\TUW-201167.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\result\result-TUW-201167-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\extracted\parscit\parscit-TUW-201167-xstream.xml"")";;;"none extracted value";"none expected";"NA"
"TUW-201821";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\groundtruth\TUW-201821.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\result\result-TUW-201821-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\extracted\parscit\parscit-TUW-201821-xstream.xml"")";"Multi-Context Systems are an expressive formalism to model (possibly) non-monotonic information exchange between heterogeneous knowledge bases. Such information exchange, however, often comes with unforseen side-effects leading to violation of constraints, making the system inconsistent, and thus unusable. Although there are many approaches to assess and repair a single inconsistent knowledge base, the heterogeneous nature of Multi-Context Systems poses problems which have not yet been addressed in a satisfying way: How to identify and explain a inconsistency that spreads over multiple knowledge bases with different logical formalisms (e.g., logic programs and ontologies)? What are the causes of inconsistency if inference/information exchange is non-monotonic (e.g., absent information as cause)? How to deal with inconsistency if access to knowledge bases is restricted (e.g., companies exchange information, but do not allow arbitrary modifications to their knowledge bases)? Many traditional approaches solely aim for a consistent system, but automatic removal of inconsistency is not always desireable. Therefore a human operator has to be supported in finding the erroneous parts contributing to the inconsistency. In my thesis those issues will be adressed mainly from a foundational perspective, while our research project also provides algorithms and prototype implementations.";;"none extracted value";"0,00";"0,00"
"TUW-202034";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\groundtruth\TUW-202034.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\result\result-TUW-202034-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\extracted\parscit\parscit-TUW-202034-xstream.xml"")";"In this thesis the application of clustering algorithms for solving the Hierarchical Ring Network Problem (HRNP) is investigated.
When the network is represented as a graph, an informal problem definition for this NP-complete problem is: Given a set of network sites (nodes) assigned to one of three layers and the costs for establishing connections between sites (i.e., edge costs) the objective is to find a minimum cost connected network under certain constraints that are explained in detail in the thesis. The most important constraint is that the nodes have to be assigned to rings of bounded size that connect the layers hierarchically.
The ring structure is a good compromise between the robustness of a network and the cost for establishing it. It is guaranteed, that the network can continue to provide its service if one network node per ring fails.
The basic idea in this thesis for solving this network design problem was to cluster the sites with hierarchical clustering heuristics and to use the resulting hierarchy as support for the ring-finding heuristics. Previous apporaches for related network design problems did not use the inherent network structure in such a way. Usual approaches are based on greedy heuristics.
Three clustering heuristics were implemented: Girvan-Newman, K-means and Kernighan-Lin. Especially the first algorithm is interesting, because it was successfully applied analyzing large network structures, also in the context of internet communities.
For finding rings three heuristics were implemented too. Strategic variation of the maximum allowed ring size helps the first heuristic to find rings using the cluster hierarchy. The second heuristic finds rings by searching for paths that are connected to previously found rings. Third a repair heuristic was implemented that tries to add remaining nodes to existing rings. Local search heuristics are applied last to improve the solution quality.
To check how the clustering approach performs for solving the problem of this thesis two test instance generators were implemented. One generates instances randomly and the second generates instances based on the popular TSPLIB archive.
The evaluation of the random test instances has shown, that all three clustering heuristics were able to solve those test instances, while Girvan-Newman and Kernighan-Lin found valid solutions in each test run this was not possible for K-means. When Kernighan-Lin was used as clustering algorithm solutions could be found faster on average, but the resulting costs where slightly higher. For the TSPLIB based instances the clustering algorithms had more problems to find valid solutions, but for each test instance at least one type of clustering was successful.";;"none extracted value";"0,00";"0,00"
"TUW-202824";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\groundtruth\TUW-202824.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\result\result-TUW-202824-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\extracted\parscit\parscit-TUW-202824-xstream.xml"")";"Cloud computing is introducing the next big shift in the IT-industry. It fundamentally changes the IT-strategy of organizations. Cloud computing promises many advantages such as reduced capital expense, support of brief surges in capacity and a better economies of scale [1]. Cloud computing is not only a useful technology for the private sector rather it also can benefit the public sector in many ways [2]. It makes e-government systems faster and cheaper and accelerates the adaptation of use of IT by citizens [3]. Cloud computing is high on the agenda of Obama administration and is being used by federal and local governments with significant benefits [4]. In the European Union the potentials of the Cloud computing have been recognized and the Cloud agenda is being pushed forward, not only because of its cost saving potentials but also because of its impact on the environment.
In this work we have conducted a case study for integration of Cloud computing in the Austrian Public sector. The contribution of this work is identification of the requirements of the public sector and obstacles for integration of cloud computing in the Austrian public sector. In this case study eight ministries and the office of chancellor have been interviewed.";;"none extracted value";"0,00";"0,00"
"TUW-203409";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\groundtruth\TUW-203409.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\result\result-TUW-203409-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\extracted\parscit\parscit-TUW-203409-xstream.xml"")";"Our work focuses on investigating novel ways of efficient processor simulation using just-in-time compilation techniques. We can automatically generate a cycle-accurate simulator from a processor description that captures hardware structure and instruction set. The simulator employs an adaptive two-level just-in-time compilation scheme based on LLVM to attain high simulation speeds.
As the main source of slowdown during simulation we identified the LLVM code generator. We reduced compilation time in our own experimental code generator by an order of magnitude compared to LLVM's original backend. Current work aims at leveraging instruction descriptions already available in LLVM to extend the coverage of our fast JIT code generator.";;"none extracted value";"0,00";"0,00"
"TUW-203924";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\groundtruth\TUW-203924.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\result\result-TUW-203924-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\extracted\parscit\parscit-TUW-203924-xstream.xml"")";"Genetic dispositions play a major role in individual disease risk and treatment response. Genomic medicine, in which medical decisions are refined by genetic information of particular patients, is becoming increasingly important. Here we describe our work and future visions around the creation of a distributed infrastructure for pharmacogenetic data and medical decision support, based on industry standards such as the Web Ontology Language (OWL) and the Arden Syntax.";;"none extracted value";"0,00";"0,00"
"TUW-204724";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\groundtruth\TUW-204724.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\result\result-TUW-204724-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\extracted\parscit\parscit-TUW-204724-xstream.xml"")";"The strategic management of intellectual capital involves rethinking how the companies creates value from a knowledge -centric perspective and redesigning and orchestrating the role of staff knowledge in the firm's strategy. This paper presents the author's software system for facilitating decision making at a strategic level in terms of the profitability of investment in staff knowledge. This software is a computer implementation of the method for planning and selection of personnel in SME.";;"none extracted value";"0,00";"0,00"
"TUW-205557";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\groundtruth\TUW-205557.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\result\result-TUW-205557-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\extracted\parscit\parscit-TUW-205557-xstream.xml"")";"The spread of safety critical real time systems results in an increased necessity for worst case execution time (WCET) analysis of these systems: finding the time limit within which the software system responds in all possible scenarios. Computing the WCET for programs with loops or recursion is, in general, un-decidable. We present an automatic method for computing tight upper bounds on the iteration number of special classes of program loops. These upper bounds are further used in the WCET analysis of programs. The technique deploys pattern-based recurrence solving in conjunction with program flow refinement using SMT reasoning. To do so, we refine program flows using SMT reasoning and rewrite certain multi-path loops into single-path ones, possibly over-approximating the loop-bound. The multi-path loops we consider are I) abruptly-terminating loops that might terminate early due to break statements and II) loops with additional monotonic updates, that conditionally modify the loop counter. For those, the minimum increase of the loop counter is computed and used as loop step expression. Single-path loops are further translated into a set of recurrence relations over program variables. For solving recurrences we deploy a pattern-based recurrence solving algorithm, computing closed forms for a restricted class of recurrence equations. Finally, iteration bounds are derived for program loops from the computed closed forms. We only compute closed forms for a restricted class of loops, however, in practice, these recurrences describe the behavior of a large set of program loops that are relevant to WCET analysis. Our technique is implemented in the r-TuBound tool and was successfully tried out on a number of challenging WCET benchmarks: we evaluate the symbolic loop bound generation technique and present an experimental evaluation of the method carried out with the r-TuBound software tool. We evaluate our method against various academic and industrial WCET benchmarks, and compare the results to the original TuBound tool.";;"none extracted value";"0,00";"0,00"
"TUW-205933";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\groundtruth\TUW-205933.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\result\result-TUW-205933-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\extracted\parscit\parscit-TUW-205933-xstream.xml"")";"Philosophy-of-information considerations can analyse information concepts according to four ways of thinking. A Unified Theory of Information (UTI) requires the fourth way of thinking – integration. This integration can be performed, if a complex systems view is informed by the heuristics of a historical and logical account. In particular, the terms of ""difference"" or ""variety"", negentropy and semiosis are used for integration. Reference is made to Gregory Bateson, Arkady D. Ursul, Edgar Morin, and Charles Sanders Peirce. An integrated information definition is presented. Information is defined as relation such that an Evolutionary System se (signator; the signmaker) reflects (1) some perturbation P (signandum/signatum; (to-be-)signified (2) by the order O it builds up spontaneously (signans; the sign) (3) for the sake of negentropy. The process of information-generation coincides with the process of sign-production and both coincide with the process of self-organisation; so do their respective results: information, sign, and self-organised order. The concepts of self-organisation and information (sign) turn out to be co-extensive. The notion ""emergent information"" is applied to characterise the complexity of information processes that proceed between determinacy and indeterminacy. Since information generation is a process that allows novelty to emerge, it is worth noting that it is not a mechanical process that can be formalised, expressed by a mathematical function, or carried out by a computer.";;"none extracted value";"0,00";"0,00"
"TUW-213513";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\groundtruth\TUW-213513.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\result\result-TUW-213513-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\extracted\parscit\parscit-TUW-213513-xstream.xml"")";;;"none extracted value";"none expected";"NA"
"TUW-216744";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\groundtruth\TUW-216744.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\result\result-TUW-216744-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\extracted\parscit\parscit-TUW-216744-xstream.xml"")";"Medical practitioners often have unmet information needs that impact patient care. However, currently available web-based search engines are not suitable for routine use. Finding relevant information takes too long, assessing the trustworthiness of found information is difficult, and support for the heterogeneity of languages and nomenclature across European countries is lacking. In this paper, we analyze the current barriers to web-based searching by medical practitioners and introduce the European Khresmoi project, which aims to dismantle these barriers.";;"none extracted value";"0,00";"0,00"
"TUW-217690";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\groundtruth\TUW-217690.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\result\result-TUW-217690-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\extracted\parscit\parscit-TUW-217690-xstream.xml"")";"Precise localization for mobile Augmented Reality in large indoor environments without specific tracking infrastructure is challenging. This is especially true for rooms with changing properties, like lighting, seating and carpeting. With these constraints a map for a vision based tracking approach has to be continuously updated. The Parallel Tracking and Mapping (PTAM) algorithm is capable of generating and extending a map while tracking the camera pose in an unknown environment. However, it has originally been designed for small workspace environments and has therefore certain limitations. We have extended and modified the original implementation in order to ensure efficient and robust map generation and tracking in large rooms. Furthermore, we have tested a mobile setup with the system in Festsaal in Vienna’s Hofburg, which is close to thousand square meters in size. The user’s position and path was tracked while the environment was augmented with virtual objects and the system was successfully tested for robustness and occlusions.";;"none extracted value";"0,00";"0,00"
"TUW-217971";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\groundtruth\TUW-217971.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\result\result-TUW-217971-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\extracted\parscit\parscit-TUW-217971-xstream.xml"")";"Many real-world Visual Analytics applications involve time-oriented data. I am working in a research project related to this challenge where I am responsible for the interactive visualization part. My goal are interactive visualizations to explore such time-oriented data according to the user tasks while considering the structure of time. Time is composed of many granularities that are likely to have crucial influence on the formation of the data. The challenge is to integrate the granularities into a detailed compound view on the data, like the compound eye of insects integrates many images into one view. Other members of our team are experts in temporal data mining and user centered design. The goal is to combine our research topics to an integrated system that helps domain experts to get more insight from their time-oriented data.";;"none extracted value";"0,00";"0,00"
"TUW-221215";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\groundtruth\TUW-221215.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\result\result-TUW-221215-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\extracted\parscit\parscit-TUW-221215-xstream.xml"")";"The Selective Graph Coloring Problem (SGCP) is about finding a subgraph of a particular structure whose chromatic number is as low as possible. The original graph is divided into several clusters, and from each cluster the subgraph has to contain exactly one node. This problem is NP-hard and therefore it is usually solved by means of heuristics.
	
I implemented several variants of an algorithm making use of Variable Neighborhood Search (VNS) to search the space of solution candidates and then evaluating the solution using heuristic or exact methods. Furthermore, each variant can be used with or without a solution archive, i.e. a data structure in which previously found solutions are stored so that duplicates need not be re-evaluated but can be efficiently converted into new solutions instead. For exact computation of the chromatic number integer linear programming was used. To obtain an upper bound a variant of greedy coloring was used. Another variant of the algorithm also counts the number of conflicts that would appear if one color less were used. Finally, two methods were implemented to obtain a lower bound: maximum clique and linear programming using column generation.

The program was tested with various instances from the literature. My algorithm often finished computation within a very short time, but in general it led to slightly worse results.";;"none extracted value";"0,00";"0,00"
"TUW-223906";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\groundtruth\TUW-223906.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\result\result-TUW-223906-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\extracted\parscit\parscit-TUW-223906-xstream.xml"")";"This paper describes our contribution to the social event detection (SED) task of the MediaEval Benchmark 2013. We present a robust unsupervised approach for the clustering of tagged photos and videos into social events. Results on the SED datasets show that the proposed approach yields an excellent generalization ability and state-of-the-art clustering performance.";;"none extracted value";"0,00";"0,00"
"TUW-223973";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\groundtruth\TUW-223973.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\result\result-TUW-223973-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\extracted\parscit\parscit-TUW-223973-xstream.xml"")";"Translating clinical practice guidelines into a computer-inter-pretable format is a challenging and laborious task. In this project we focus on supporting the early steps of the modeling process by automatically identifying conditional activities in guideline documents in order to model them automatically in further consequence. Therefore, we developed a rule-based, heuristic method that combines domain-independent information extraction rules and semantic pattern rules. The classification also uses a weighting coefficient to verify the relevance of the sentence in the context of other information aspects, such as effects, intentions , etc. Our evaluation results show that even with a small set of training data, we achieved a recall of 75 % and a precision of 88 %. This outcome shows that this method supports the modeling task and eases the translation of CPGs into a semi-formal model.";;"none extracted value";"0,00";"0,00"
"TUW-225252";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\groundtruth\TUW-225252.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\result\result-TUW-225252-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\extracted\parscit\parscit-TUW-225252-xstream.xml"")";;;"none extracted value";"none expected";"NA"
"TUW-226000";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\groundtruth\TUW-226000.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\result\result-TUW-226000-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\extracted\parscit\parscit-TUW-226000-xstream.xml"")";"The user interface is the most important feature of interaction between users and (AAL) services. Explicitly defined user interfaces are bound to a specific toolkit and programming language or markup language. Thus a separate user interface definition has to be created manually for different classes of I/O devices to be supported. Compared to manual user interface creation, the automatic or semi-automatic generation of user interfaces based on interaction descriptions considerably reduces the manual effort necessary for integrating a large number of devices and therefore automatically increases the number of supported devices. The main goal of this paper is to provide an overview of selected existing solutions for the definition of generic user interactions and the generation of user interfaces. The comparison shows that the aspect of adaptability is partly covered by the presented User Interaction Description Languages. Nevertheless it is important to analyze them with respect to additional criteria, like accessibility, context-and use-case awareness, to receive a meaningful overview of advantages and drawbacks of the different approaches leading to a good basis for choosing one of the presented approaches.";;"none extracted value";"0,00";"0,00"
"TUW-226016";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\groundtruth\TUW-226016.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\result\result-TUW-226016-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\extracted\parscit\parscit-TUW-226016-xstream.xml"")";"Invariant genereation is a critical problem in proving different properties for programs with loops, properties including correctnes. The problem becomes harder with the incresing numbers of quantifiers in the property to be proven. In this paper we study and combine different methods of invariant generation in order to obtain stronger properties.";;"none extracted value";"0,00";"0,00"
"TUW-228620";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\groundtruth\TUW-228620.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\result\result-TUW-228620-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\extracted\parscit\parscit-TUW-228620-xstream.xml"")";"Business networking relies on application-specific quantity and quality of information in order to support social infrastructures in, e.g., energy allocation coordinated by smart grids, healthcare services with electronic health records, traffic management with personal sensors, RFID in retail and logistics, or integration of individuals' social network information into good, services, and rescue operations. Due to the increasing reliance of networking applications on sharing ICT services, dependencies threaten privacy, security, and reliability of information and, thus, innovative business applications in smart societies. Resilience is becoming a new security approach, since it takes dependencies into account and aims at achieving equilibriums in case of opposite requirements. This special issue on 'Security and Privacy in Business Networking' contributes to the journal 'Electronic Markets' by introducing a different view on achieving acceptable secure business networking applications in spite of threats due to covert channels. This view is on adapting resilience to enforcement of IT security in business networking applications. Our analysis shows that privacy is an evidence to measure and improve trustworthy relationships and reliable interactions between participants of business processes and their IT systems. The articles of this special issue, which have been accepted after a double-blind peer review, contribute to this view on interdis-ciplinary security engineering in regard to the stages of security and privacy requirements analysis, enforcement of resulting security requirements for an information exchange, testing with a privacy-preserving detection of policy violations, and knowledge management for the purpose of keeping business processes resilient.";;"none extracted value";"0,00";"0,00"
"TUW-231707";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\groundtruth\TUW-231707.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\result\result-TUW-231707-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\extracted\parscit\parscit-TUW-231707-xstream.xml"")";"The VRVis Research Center in Vienna is the largest technology transfer institution in the area of Visual Computing in Austria. The requirements of the funding body FFG include the publication of scientific research results in first class peer reviewed media, and the active cooperation with co-funding companies. As a consequence the requirements on the staff of VRVis are manifold: they have to communicate with real users, use real data, know about software and hardware, understand the market, do professional documentation, initiate new projects and write funding proposals for these, be part of the scientific community and publish and review papers, manage several projects in parallel and obey strict deadlines for their projects and some more. Such staff is barely available and must be trained on the job.";;"none extracted value";"0,00";"0,00"
"TUW-233317";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\groundtruth\TUW-233317.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\result\result-TUW-233317-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\extracted\parscit\parscit-TUW-233317-xstream.xml"")";"Visual Analytics prototypes increasingly support human sensemak-ing through providing Provenance information. For data analysts the challenge of knowledge generation starts with assessing the quality of a data set, but Provenance is not yet utilized to aid this task. This position paper aims at characterizing the complexity of Visual Analytics methods introducing Provenance in Data Quality by highlighting the challenges of (1) generating Provenance from Data Quality Control and (2) sensemaking based on Data Quality Provenance.";;"none extracted value";"0,00";"0,00"
"TUW-233657";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\groundtruth\TUW-233657.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\result\result-TUW-233657-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\extracted\parscit\parscit-TUW-233657-xstream.xml"")";"Recent research in Visualization has focused mostly on data analysis systems for domain experts, but also considered presentation to external people in the form of storytelling. The established directions assume that the target audience has in inherent interest in the facts to be discovered, sometimes even to the point of them being willing to learn how to operate a complex visualization system and spend considerable time and effort. In reality, sometimes the opposite is true: people unwilling to face an inconvenient truth actively avert their eyes. As a solution, we propose the presentation of facts by experts who manage to gain a limited amount of attention by means of rapid and expressive visualization. Using conventional desktop systems, this method is hard to implement, but new visual channels will open up new possibilities.";;"none extracted value";"0,00";"0,00"
"TUW-236063";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\groundtruth\TUW-236063.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\result\result-TUW-236063-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\extracted\parscit\parscit-TUW-236063-xstream.xml"")";"The purpose of the project VALCRI is to develop a new system prototype for information exploitation by intelligence analysts working in law enforcement agencies. Information visu-alisation will be a core element of the prototype. Such systems have to be designed to support the sensemaking and reasoning processes of the analysts. One of the goals of the project is, therefore, to get a more thorough understanding of sensemaking processes and to develop a set of recommendations for the design of intelligence analysis systems to help analysts in their work.";;"none extracted value";"0,00";"0,00"
"TUW-236120";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\groundtruth\TUW-236120.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\result\result-TUW-236120-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\extracted\parscit\parscit-TUW-236120-xstream.xml"")";"As part of the project ""Educational standards in vocational schools"" the responsible Federal Ministry compiled competence models, descriptors and didactic examples for Informatics in technical high schools (HTL). The implementation of this project implies a paradigm shift for the vocational school system , which requires a new pedagogic foundation. This paper presents a didacti-cal concept, that meets the requirements of the educational standards in general and competence orientation in particular. A proposal for the implementation of these educational standards in the competence area industrial information technology (INIT) is presented. The specification is based on selected teaching sessions for micro controller technique. Learner-oriented teaching methods are applied , along with procedures to promote the learners' intrinsic motivation and creativity in general. It became apparent that the competence area INIT conveys remarkably demanding learning outcomes and its implementation proved challenging on multiple levels.";;"none extracted value";"0,00";"0,00"
"TUW-237297";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\groundtruth\TUW-237297.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\result\result-TUW-237297-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\extracted\parscit\parscit-TUW-237297-xstream.xml"")";"This paper is about the role of e-learning environments to support first year students of computer science. Our goal is to make their transition easier from high school to university learning as well as to introduce our students self-regulated learning step by step. Our results are based on our qualitative study with our students. We analyzed the interviews we carried out with them to identify challenges in the first semester program. In this paper we discussed some possible solutions with support of our e-learning environment we have already established. We know that we need to adapt our system to meet these challenges. We conclude our paper with our future work.";;"none extracted value";"0,00";"0,00"
"TUW-240858";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\groundtruth\TUW-240858.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\result\result-TUW-240858-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\extracted\parscit\parscit-TUW-240858-xstream.xml"")";"The chase procedure is considered as one of the most fundamental algorithmic tools in database theory. It has been successfully applied to different database problems such as data exchange, and query answering and containment under constraints, to name a few. One of the central problems regarding the chase procedure is all-instance termination, that is, given a set of tuple-generating dependencies (TGDs) (a.k.a. existential rules), decide whether the chase under that set terminates, for every input database. It is well-known that this problem is un-decidable, no matter which version of the chase we consider. The crucial question that comes up is whether existing restricted classes of TGDs, proposed in different contexts such as ontological reasoning, make the above problem decidable. In this work, we focus our attention on the oblivious and the semi-oblivious versions of the chase procedure, and we give a positive answer for classes of TGDs that are based on the notion of guardedness.";;"none extracted value";"0,00";"0,00"
"TUW-245336";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\groundtruth\TUW-245336.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\result\result-TUW-245336-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\extracted\parscit\parscit-TUW-245336-xstream.xml"")";"As technology designers, we often inadvertently inscribe values and concepts in systems beyond what we intended. Further, while we aim to work from a user-and use-centred perspective, we often miss the perspectives of other critical stakeholders and the broader context in which technology systems are to be used. Reflecting on issues of responsible innovation from the bottom up, this paper explores two exemplar cases: designing for older people and health care agendas, and designing for eco-behaviour change agendas. While very different in focus, both highlight the importance that different conceptualisations have on shaping possible solutions, and the challenge of identifying and negotiating competing agendas. This draws attention to the limits of both top-down and bottom-up perspectives and suggests instead a middle out approach to responsible design that considers both policy and practice aspects in the process. It also calls for us as responsible designers to be reflective practitioners aware of our power in inscribing possible futures.";;"none extracted value";"0,00";"0,00"
"TUW-245799";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\groundtruth\TUW-245799.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\result\result-TUW-245799-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\extracted\parscit\parscit-TUW-245799-xstream.xml"")";"The diagnosis of failures in train traffic installations can be done in several ways: direct observations and measurements, software assisted diagnosis using specific software packages, process variable monitoring for electronically centralized installations. This work presents basic concepts for Model Based Diagnosis (MBD) that uses fuzzy logic to analyse the integrity of Centralized Traffic Control Installations (CTC) in train traffic. We define the diagnosis relations to be used and show how to apply them to train traffic security installations. Implementing these concepts into an expert system assists maintenance operators in quick failure diagnosis of the train traffic security installations.";;"none extracted value";"0,00";"0,00"
"TUW-247301";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\groundtruth\TUW-247301.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\result\result-TUW-247301-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\extracted\parscit\parscit-TUW-247301-xstream.xml"")";;;"none extracted value";"none expected";"NA"
"TUW-247741";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\groundtruth\TUW-247741.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\result\result-TUW-247741-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\extracted\parscit\parscit-TUW-247741-xstream.xml"")";"Evaluating the quality of segmentations is an important process in image processing, especially in the medical domain. Many evaluation metrics have been used in evaluating segmentation. There exists no formal way to choose the most suitable metric(s) for a particular segmentation task and/or particular data. In this paper we propose a formal method for choosing the most suitable metrics for evaluating the quality of segmenta-tions with respect to ground truth segmentations. The proposed method depends on measuring the bias of metrics towards/against the properties of the the seg-mentations being evaluated. We firstly demonstrate how metrics can have bias towards/against particular properties and then we propose a general method for ranking metrics according to their overall bias. We finally demonstrate for 3D medical image segmentations that ranking produced using metrics with low overall bias strongly correlate with manual rankings done by an expert.";;"none extracted value";"0,00";"0,00"
"TUW-247743";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\groundtruth\TUW-247743.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\result\result-TUW-247743-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\extracted\parscit\parscit-TUW-247743-xstream.xml"")";"The Self-Organizing Map (SOM) is a useful and strong tool for data analysis, especially for large data sets or data sets of high dimensionality. SOM visualizations map the data model dimensions to visual dimensions like color and position, thus they help exploring the SOM. Visualization can also involve the data itself so that it helps accessing information that are not available in the trained SOM, thereby enabling a deeper look inside the data. If the data comes with supervised class labels, these labels can be also involved in the visualization, thus enabling the user to have a clearer idea about the data and the structures learned by the SOM. In this work we propose a novel SOM visualization method, namely the SOM class coloring, which is based on the data class labels. This method finds a colored partitioning of the SOM lattice, that reflects the class distribution. SOM class coloring helps discovering class information such as class topology, class clusters, and class distribution. Furthermore class labels can be assigned to new data items by estimating the point on the lattice, that best represents the data item and then assigning the class of the partition that includes this point to the data item.";;"none extracted value";"0,00";"0,00"
"TUW-251544";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\groundtruth\TUW-251544.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\result\result-TUW-251544-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\extracted\parscit\parscit-TUW-251544-xstream.xml"")";"Die Wiener Zauberschule der Informatik (WIZIK) führt Kinder der Primarstufe an die Denkweise der Informatik heran und vermittelt ihnen erste informatische Kompetenzen. Die Kinder lernen spielerisch verschiedene Problemlösungsstrategien kennen und erhalten einen ersten Einblick in die Grundlagen logischen und prozessorientierten Denkens.";;"none extracted value";"0,00";"0,00"
"TUW-252847";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\groundtruth\TUW-252847.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\result\result-TUW-252847-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\extracted\parscit\parscit-TUW-252847-xstream.xml"")";"This paper provides an overview of the Retrieving Diverse Social Images task that is organized as part of the MediaEval 2016 Benchmarking Initiative for Multimedia Evaluation. The task addresses the problem of result diversification in the context of social photo retrieval where images, meta-data, text information, user tagging profiles and content and text models are available for processing. We present the task challenges, the proposed data set and ground truth, the required participant runs and the evaluation metrics.";;"none extracted value";"0,00";"0,00"
"TUW-255712";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\groundtruth\TUW-255712.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\result\result-TUW-255712-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\extracted\parscit\parscit-TUW-255712-xstream.xml"")";"Abstract argumentation is a rich research subfield of AI and till today, numerous frameworks for it have been proposed. It is thus natural to ask whether one can translate between these structures, and what are the price and consequences of undergoing this process. Although every study explains how a given structure relates to the cornerstone of abstract argu-mentation-Dung's framework-there are less results available concerning the connections between more advanced formalisms. Moreover, the existing research is not particularly systematized or classified in a way that would clearly show us the properties of a given transformation. In our work, we address these issues by creating an in-depth compendium on the intertranslatability of argumentation frameworks, describing approximately eighty translations. Furthermore, we provide a system for analyzing a given transformation in terms of its functional, syntactical, semantical and computational properties and the underlying methodology.";;"none extracted value";"0,00";"0,00"
"TUW-256654";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\groundtruth\TUW-256654.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\result\result-TUW-256654-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\extracted\parscit\parscit-TUW-256654-xstream.xml"")";"A section is a contiguous region of memory, to which data or code can be appended (like the Forth dictionary). Assembly languages and linkers have supported multiple sections for a long time. This paper describes the benefits of supporting multiple sections in Forth, interfaces and implementation techniques.";;"none extracted value";"0,00";"0,00"
"TUW-257397";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\groundtruth\TUW-257397.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\result\result-TUW-257397-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\extracted\parscit\parscit-TUW-257397-xstream.xml"")";"Configuration files, command-line arguments and environment variables are the dominant tools for local configuration management today. When accessing such program execution environments, however, most applications do not take context, e.g. the system they run on, into account. The aim of this paper is to integrate unmodified applications into a coherent and context-aware system by instrumenting the getenv API. We propose a global database stored in configuration files that includes specifications for contextual interpretations and a novel matching algorithm. In a case study we analyze a complete Debian operating system where every getenv API call is intercepted. We evaluate usage patterns of 16 real-world applications and systems and report on limitations of unforeseen context changes. The results show that getenv is used extensively for variability. The tool has acceptable overhead and improves context-awareness of many applications.";;"none extracted value";"0,00";"0,00"
"TUW-257870";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\groundtruth\TUW-257870.pdf"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\result\result-TUW-257870-xstream.xml"")";"=HYPERLINK(""D:\Java\git\MethodDemosGit-0.5.6\MethodDemos\output\extracted\parscit\parscit-TUW-257870-xstream.xml"")";"This position paper describes a critical incident from an early AAL project related to the design decisions made about which features to include. In order to give the older users of a sensor-based telecare monitoring system more tangible value, a number of non-sensor-based interactive services were incorporated into the system which was installed in a residential facility. These services were chosen based on recommendations and input from older people. In the end though, many services were not used and actually contributed to the system being removed from residences.";;"none extracted value";"0,00";"0,00"
